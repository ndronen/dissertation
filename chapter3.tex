\chapter{Studies}
\label{sec:studies}

Here we explain the designs of the studies intented to answer the research
questions described in the prevoius section.  The studies are outlined
in Table \ref{tab:studies}.

%Table 7 gives an outline of each study, aligned with the corresponding research question.

\begin{table}
\centering
\begin{tabular}{c|c|c|c} 
\hline
Research question & Study & Status & Citation \\
\hline
\parbox{7cm}{Spell checking} & 1 & To do& - \\
\hline
\parbox{7cm}{Isolated non-word error correction} & 2 & To do & - \\
\hline
\parbox{7cm}{Contextual non-word error correction} & 3 & To do & - \\
\hline
\parbox{7cm}{Contextual real-word error correction}& 4 & To do & - \\
\hline
%\parbox{7cm}{How well does a CNN LM predict grammaticality?} & 5 & To do & - \\
%\hline
\end{tabular}
\caption{Completed and proposed studies.}
\label{tab:studies}
\end{table}

\subsection{Study 1}
\label{sec:study1}
\subsubsection{Objective}

The objective of this study is to determine how best to train a ConvNet to function as a dictionary -- that is, to distinguish unknown from known words.  This study will allow us to understand and perhaps rectify the strengths and limitations of ConvNets on the task of binary classification of known words.

Our choice of a baseline and competitor models will also shed light on the ability of a ConvNet to learn a character-level boundary between words and non-words.  By using Knesser-Ney smoothed language models as a baseline, we will determine whether the soft boundary learned by a ConvNet has any surprising properties.  By comparing ConvNet behavior to that of powerful recurrent neural networks, we will determine whether the whole-sequence approach of recurrent networks differs substantially from the n-gram approach of ConvNets.

%The objective of this study is to determine whether it is possible to train a convolutional neural network as a language model of sentences using an unlabeled corpus.

\subsubsection{Background} 

The standard bag-of-words model use in natural language processing
discards the order of a text, which eliminates all of the information
provided by the sequential nature of language.  Broadly speaking, there
are two kinds of neural networks that do not completely discard the
order of words.  Recurrent neural networks are one.  When trained 
at time $t$ to predict the next word $w_{t+1}$ given the previous
word $w_t$, and RNN yields a very high quality probabilistic language
model.  They process input sequentially, so they can be trained with
variable-width inputs.  They have seen wide use as language models 
in recent years.  Since convolutional neural networks are also sensitive
to order, it would be of scientific interest to be able to train a CNN
as a language model in order to understand its properties.

\subsubsection{Methodology}

\begin{itemize}
    \item input: vector of character indices representing a token
    \item output: binary (0,1)
    \item data
    \begin{itemize}
        \item Lists of pseudowords
        \item Roger Mitton corpora
        \item Model 1
        \begin{itemize}
            \item Train
            \begin{itemize}
                \item Positive examples: half of Aspell vocabulary (without proper names)
                \item Negative examples: pseudowords or errors generated by applying to half of the Aspell vocabulary the edits learned from another corpus of student writing errors 
            \end{itemize}
            \item Test
            \begin{itemize}
                \item Positive examples: other half of Aspell vocabulary (without proper names)
                \item Negative examples: pseudowords, errors generated as in the training set, non-word errors from Mitton corpora 
            \end{itemize}
        \end{itemize}
        \item Model 2
        \begin{itemize}
            \item Train: all Mitton corpora except one (with errors that occur in the test set removed from training, as there may be some overlap)
            \item Test: the held-out corpus
        \end{itemize}
    \end{itemize}
    \item baseline: two character n-gram language models, one trained using positive examples, the other with negative examples.
    \item competitor models: RNN, Bi-RNN, LSTM, Bi-LSTM
    \item hyperparameters:
    \begin{itemize}
        \item sample weights (training only)
        \begin{itemize}
            \item Positive examples: proportional to the edit distance from the word's stem to the nearest stem in Aspell suggestions
            \item Negative examples: proportional to edit distance to nearest word in Aspell dictionary (pseudowords), the number of edit operations required to transform Aspell vocabulary word into negative example (generated errors).
        \end{itemize}
        \item filters: with and without nonce characters, with varying number of filters and hidden layers, filter width in 2,3,4,5.
        \item training set size: train baseline language models and ConvNets with training sets of increasing sizes. 
    \item analysis
    \begin{itemize}
        \item Accuracy, F1
        \item precision-recall curves
        \item three-way confusion matrix using baseline binary classifier language model
        \item distributions of edit distances by cell in confusion matrix
        \item examples of model errors
        \item characteristics of n-grams in words the model gets wrong
    \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Status}

\todo[inline]{TODO.}

%This study is almost complete.  We have developed a method for training a CNN as a model of sentences using a CNN and have verified that the method performs well for the size of the training set we have used thus far.  The high-level architecture of the CNN itself is shown in Figure \ref{fig:cnnarch}.

\subsubsection{Anticipated outcome}

\todo[inline]{TODO.}

%The CNN language model described above classifies positive and negative with reasonably high accuracy.  On binary classification with a balanced number of positive and negative examples, the network achieves a misclassification error rate from 8-18\%.  The variance in the error rate depends on the size of the training set and whether words that not in the Word2Vec dictionary are assigned their own random vector or are assigned the zero vector.  In our experience, increasing the size of the training set while keeping all other model parameters the same results in a more accurate model.  Since there a millions of sentences in a Wikipedia dump and we have been using a relatively small sample of 100,000 sentences, we expect to achieve even greater model accuracy by training on a larger number of sentences.

%We are preparing a paper on this method for the 3rd Workshop on Continuous Vector Space Models and their Compositionality to be held at ACL in Beijing, China, in the summer of 2015.  See Appendix \ref{appendix:cnnlm1} for an early draft.

\subsection{Study 2}
\label{sec:study2}
\subsubsection{Objective}

\todo[inline]{TODO.}

%The objective of this study is to determine which CNN architecture yields the most accurate model.  

\subsubsection{Status}

\todo[inline]{TODO.}

\subsubsection{Background} 

\todo[inline]{TODO.}

%While we expect our current model to perform better as we increase the training set size, there are other ways the performance might be improved. There is, for instance, more than one reference architecture for CNNs applied to natural language processing tasks.  Some of the design choices for these architectures are shown in Table \ref{tab:refarch}, along with the choices we've made for our architecture.

\iffalse
\begin{table}
\centering
\begin{tabular}{c|c|c|c|c} 
\hline
 & \parbox{2cm}{Collobert and Weston (2008)} & \parbox{2cm}{Kalchbrenner et al (2014)} & \parbox{2cm}{Kim (2014)} & \parbox{2cm}{Our model} \\
\hline
\parbox{5cm}{Convolutional layers?} & 1 & $> 1$ & 1 & 1\\
\hline
\parbox{5cm}{Do kernels span word representations?} & Yes & No & Yes  & Yes \\
\hline
\parbox{5cm}{Are word representations pretrained?} & No & No & Yes & Yes \\
\hline
\parbox{5cm}{Type of pooling?} & Max & Dymamic $k$-max & Max & Max \\
\end{tabular}
\caption{Characteristics of different reference architectures.}
\label{tab:refarch}
\end{table}
\fi

%While the Kalchbrenner (DCNN) and Kim networks perform nearly identically on a number of supervised tasks, there may be a reason to favor a hierarchical CNN over our current architecture\footnote{That said, the fact that the two networks exhibit such similar performance is grounds for some skepticism about the DCNN model and the purported value of dynamic $k$-max pooling.}.  Our current architecture has a single convolutional and pooling layer, and the kernels span from 3-5 words.  The prediction of our model is thus based on the output of several hundred $n$-gram vector space feature detectors.  Connecting those outputs in a hierarchy may result in the network being able to identify peculiar relationships between non-adjacent words.  The ability to make inferences based on distant words in a sentence may be crucial for optimal results in our Study \ref{sec:study5}.  A successful conclusion of our dissertation work therefore requires a thorough exploration of the space of network architectures and parameter configurations.

\subsubsection{Methodology}

\begin{itemize}
    \item input/output
        \begin{itemize}
            \item Binary classification model
                \begin{itemize}
                    \item input: a vector of character indices representing a non-word error; a vector of character indices representing a proposed correction
                    \item output: [0,1]
                \end{itemize}
            \item Multiclass classification model
                \begin{itemize}
                    \item input: a vector of character indices representing a non-word error
                    \item output: softmax over the vocabulary
                \end{itemize}
        \end{itemize}
    \item data
        \begin{itemize}
            \item Train
                \begin{itemize}
                    \item Positive examples: known words from Aspell (excluding proper nouns and real words in Mitton corpora) 
                    \item Negative examples: hand-curated lists of non-word spelling errors; non-word spelling errors generated by modifying real words using edits learned from a corpus; the length of the edit can vary, as can its probability; more than one edit can be applied to a real word 
                \end{itemize}
            \item Test
                \begin{itemize}
                    \item Positive examples: real words in Roger Mitton's corpora
                    \item Negative examples: non-words in Roger Mitton's corpora
                \end{itemize}
        \end{itemize}
    \item baselines:
        \begin{itemize}
        \item Existing dictionary implementations with correction feature 
            \begin{itemize}
                \item Aspell implementations
                    \begin{itemize}
                    \item Vanilla Aspell 
                    \item Aspell with Google unigram language model for ranking 
                \item Norvig implementations (with Aspell vocabulary)
                    \begin{itemize}
                    \item Norvig with Google unigram language model
                    \item Norvig with Google unigram language model and phonetic hashing for candidate retrieval 
                    \end{itemize}
                \item  The noisy channel model \cite{kernighan1990spelling, church1991probability}.  This models the probability of the correct word given the error. 
            \end{itemize}
        \end{itemize}
    \item competitor models
        \begin{itemize}
            \item Random forest and logistic regression models trained with the following features.
                \begin{itemize}
                    \item The edit distance between the suggestion and non-word.
                    \item The edit distance between the SOUNDEX (Metaphone) of the suggestion and the non-word.
                    \item The keyboard distance between the suggestion and the non-word.
                    \item The number of suggestions made by the dictionary for the non-word.
                    \item Whether the suggestion (non-word) contains a space.
                    \item Whether the suggestion and non-word begin with the same character.
                    \item The probability of the suggestion according to the unigram probabilities in the Google N-Gram corpus.
                    \item The character uni-, bi-, and tri-gram bag-of-words vector of the suggestion (non-word).
                    \item The numbers of characters, vowels, consonants, and capitals in the suggestion (non-word).
                \end{itemize}
        \end{itemize}
    \item hyperparameters
    \item analysis
\end{itemize}


    

\subsubsection{Anticipated outcome}

\todo[inline]{TODO.}

%The most promising architecture for future work will be identified. We will use that architecture for all subsequent studies.

\subsection{Study 3}
\label{sec:study3}

\subsubsection{Objective}

\todo[inline]{TODO.}

%The objective of this study is to determine the effects of different training curriculua on the kernels learned by a discrminative convolutional neural network language model.  The purpose is primarily phenomenological: we wish to catalogue the characteristics of kernels learned under different training regimes. 

\subsubsection{Status} 

\todo[inline]{TODO.}

\subsubsection{Background} 

\todo[inline]{TODO.}

%Different kinds of negative examples cause a network's kernels to learn different features.  Negative examples generated by a probabilistic language model, for instance, are mostly plausible locally and implausible globally.  Negative examples created by permuting entire sentences are implausible both locally and globally.  The features learned by the kernels that specialize in detecting negative examples should thus vary depending on the examples used to train the network. A thorough understanding of how the kernels \textit{vary} assumes the existence of some method for identifying how they \textit{behave}. We describe our proposed method for doing so in Section \ref{sec:method1}.

\subsubsection{Proposed methodology} 
\label{sec:method1}

\todo[inline]{TODO.}

%This study will use the same training data as used in the previous study. Two models will be trained, one for each way of generating negative examples.  The validation sets, test sets, and method of early stopping will likewise be unchanged from the previous study.

%We will characterize kernels in several ways.  

%\subsubsection{Results} 
%\subsubsection{Publication} 
\subsubsection{Anticipated outcome}

\todo[inline]{TODO.}

\subsection{Study 4}
\label{sec:study4}

\subsubsection{Objective} 

\todo[inline]{TODO.}

\subsubsection{Status} 

\todo[inline]{TODO.}

\subsubsection{Background} 

\todo[inline]{TODO.}

\subsubsection{Proposed methodology} 

\todo[inline]{TODO.}

\subsubsection{Background} 

\todo[inline]{TODO.}

%\subsubsection{Results} 
%\subsubsection{Publication} 
\subsubsection{Anticipated outcome}

\todo[inline]{TODO.}

%\subsection{Study 5}
%\label{sec:study5}

%\subsubsection{Objective} 

%\todo[inline]{TODO.}

%\subsubsection{Status} 

%\todo[inline]{TODO.}

%\subsubsection{Background} 

%\todo[inline]{TODO.}

%\subsubsection{Proposed methodology} 

%\todo[inline]{TODO.}

%\subsubsection{Results} 
%\subsubsection{Publication} 

%\subsubsection{Anticipated outcome}

%\todo[inline]{TODO.}

\section{Timeline}
\label{sec:timeline}

\begin{table}
\centering
\begin{tabular}{c|c|c|c} 
\hline
To do & Task description & Semester(s) \\
\hline
Study 1 & \parbox{6cm}{How might CNNs be trained as language models?} & Spring 2015 \\
\hline
Study 2 & \parbox{6cm}{What CNN architecture is best for achieving high accuracy?} & Spring 2015 - Summer 2015 \\
\hline
Study 3 & \parbox{6cm}{What are the characteristics of kernels learned by a CNN LM?} & Fall 2015 \\
\hline
Study 4 & \parbox{6cm}{How well do the features of a CNN LM perform when classifying sentences?} & Fall 2015-Spring 2016 \\
\hline
Study 5 & \parbox{6cm}{How well does a CNN LM predict grammaticality?} & Fall 2015-Spring 2016 \\
\hline
Wrap-up & Dissertation submission and defense. & Spring 2016 \\
\hline
\end{tabular}
\caption{Timeline.}
\label{tab:studies}
\end{table}

\section{Risks and exposures}
\label{sec:risks}

Our experiments thus far have involved training sets of a relatively small
sample of sentences from Wikipedia consisting of up to 100,000 sentences.
Convolutional neural networks are computationally costly to train.
Our model can process sentences at a rate of 500 per minute, although
that does depend on the network's capacity (e.g. the number of kernels
in a feature map).  Training models using much larger number of sentences
will result in per-epoch training times shown in Table \ref{tab:perepoch}.
This can slow down our progress, particularly when we evaluate a fairly
large number of model configurations for Study \ref{sec:study2}.  We hope
to avoid having to spend much time profiling our code to identify bottlenecks.

\begin{table}
\centering
\begin{tabular}{c|c} 
\hline
N & Time per epoch \\
\hline
100,000 & 3.3 hours \\
1,000,000 & 1.4 days \\
6,000,000 & 8.4 days \\
\hline
\end{tabular}
\caption{Expected training time per epoch}
\label{tab:perepoch}
\end{table}

The most speculative study proposed here is Study \ref{sec:study5}.
We chose to make it our last because we believe that successfully
completing it requires a greater knowledge of the workings of CNNs
than we currently have and that we will certainly have obtained by the
time we complete Studies \ref{sec:study1}-\ref{sec:study4}.
