\begin{table}
\scriptsize
    \centering
    \begin{tabular}{c|c|lrlr}
& &     \multicolumn{4}{c}{\textsc{Model}} \\
\hline
& & \multicolumn{2}{c}{\textsc{Non-word}} & \multicolumn{2}{c}{\textsc{Real word}} \\ 
\hline
& & \textsc{Word} &   \textsc{P(Non-word)} &             \textsc{Word} &        \textsc{P(Real word)} \\
\hline
\multirow{20}{*}{\pbox{10cm}{\textsc{Ground}\\ \textsc{truth}}} & \multirow{10}{*}{\pbox{20cm}{\textsc{Non-}\\ \textsc{word}}} & t\underline{bu}'s &  1.0 &             \underline{xx}xi & 1.0 \\
%& &  hardtac\underline{'k}s &  1.000 &  accout\underline{re}ments's & 1.000 \\
%& &  parmi\underline{ig}ana &  1.000 &        FORTRAN\underline{s'} & 0.999 \\
& &   con\underline{uj}rer &  1.0 &     bodybui\underline{dl}ing & 1.0 \\
%& & syn\underline{no}ymous &  1.000 &              o\underline{pp} & 0.999 \\
& & pr\underline{te}ense's &  1.0 &   over\underline{ac}pacity's & 1.0 \\
%& &     b\underline{na}ter &  1.000 &    s\underline{ap}smodically & 0.997 \\
& &     Ber\underline{'t}s &  1.0 &          Eli\underline{as}'s & 1.0 \\
%& &   every\underline{ad}y &  1.000 &              \underline{aa}h & 0.996 \\
& &      pru\underline{en} &  1.0 &       jodhpur\underline{'s}s & 1.0 \\
%& &                      l &  0.506 &           ani\underline{um}s & 0.507 \\
& &   Juda\underline{ci}al &  0.5 &                            o & 0.5 \\
%& &     \underline{or}uses &  0.505 &         \underline{or}ttener & 0.506 \\
& &      lea\underline{hs} &  0.5 &                           GP & 0.5 \\
%& & granda\underline{nu}ts &  0.504 &                           ho & 0.506 \\
& &     \underline{ro}nate &  0.5 &                           AV & 0.5 \\
%& &         JV             &  0.501 &                            x & 0.504 \\
%& &         PD             &  0.500 &   th\underline{ro}oughfare's & 0.503 \\
%& & cent\underline{ar}list &  0.500 &  Faber\underline{eg} & 0.502 \\
& &         CE             &  0.5 &              o\underline{ra} & 0.5 \\
  \cline{2-6}
& \multirow{10}{*}{\pbox{20cm}{\textsc{Real}\\ \textsc{word}}} & PD             &  0.5 &                    Wodehouse & 0.5 \\
& &         JV             &  0.5 &                            x & 0.5 \\
%& &        tsp             &  0.501 &                    grandiose & 0.505 \\
& &         dB             &  0.5 &                           AV & 0.5 \\
%& &      rye's             &  0.502 &                       untrue & 0.506 \\
& &       sere             &  0.5 &                           ho & 0.5 \\
%& &     dulcet             &  0.505 &                           GP & 0.506 \\
& &          l             &  0.5 &                            o & 0.5 \\
%& &     subset             &  0.508 &                     fibrosis & 0.508 \\
%& &         lb             &  0.5 &                sonsofbitches & 0.5 \\
%& &      Bobbi             &  0.957 &                   flaccidity & 1.000 \\
& &     Oliver             &  1.0 &                     surgeons & 1.0 \\
%& &     honcho             &  0.967 &                  suspender's & 1.000 \\
& &  bobwhites             &  1.0 &                  musketeer's & 1.0 \\
%& &        wee             &  0.982 &                      acidify & 1.000 \\
& &    Bobbi's             &  1.0 &                      washing & 1.0 \\
%& &     Hamlin             &  0.986 &              impecuniousness & 1.000 \\
& &    torches             &  1.0 &                  afterword's & 1.0 \\
%& &       weer             &  0.997 &                  quadruped's & 1.000 \\
& &    porches             &  1.0 &                   appearance & 1.0 \\
\hline
\end{tabular}
    %\caption{Confusion matrix of model trained using artificial negative examples created via a single transpose.}
    %\label{tab:NWEDConfusionMatrixTranspose}
\end{table}