%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Automated assessment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@BOOK{Burstein1998-an,
  title    = "Computer analysis of essays",
  author   = "Burstein, J and Kukich, K and Wolff, S and Lu, C and Chodorow, M",
  abstract = "Abstract Research into the use of advanced computational
              linguistics techniques recently culminated in the implementation
              of a prototype automatic essay scoring system at Educational
              Testing Service (ETS). In an evaluation study using data sets
              from thirteen ...",
  year     =  1998,
  url      = "http://144.81.87.152/Media/Research/pdf/erater_ncmefinal.pdf",
  keywords = {AWA}
}

@ARTICLE{Hassaine2012-oz,
  title    = "Automated essay scoring using structural and grammatical features",
  author   = "Hassaine, Abdelaali",
  abstract = "Automated essay scoring is a research field which is continuously
              gaining popularity. Grading essays by hand is expensive and time
              consuming, automated scoring systems can yield fast, effective
              and affordable solutions that would make it possible to grade
              essays and other sophisticated testing tools. This study has been
              conducted on a dataset of thousands of English essay sets
              belonging to eight different categories provided by the Hewlett
              Foundation. Each category corresponds to the same question or
              problem statement. The score of each essay of the training set is
              provided in this dataset by human raters. Several features have
              been determined to predict the final grade. First, the number of
              occurrences of the 100 most frequent words in English is computed
              in each essay. Then, the list of average scores associated to
              each compounding word in the training set is determined. From
              this list several statistical values are considered as separate
              feature including the minimum, maximum, mean and median values,
              v...",
  day      =  19,
  month    =  oct,
  year     =  2012,
  url      = "http://dx.doi.org/10.5339/qfarf.2012.CSP33",
  doi      = "10.5339/qfarf.2012.CSP33",
  keywords = {AWA}
}

@ARTICLE{Tsai2012-ef,
  title    = "The Consistency Between Human Raters and an Automated Essay
              Scoring System in Grading High School Students' English Writing",
  author   = "Tsai, Min-Hsiu",
  abstract = "This study investigates the consistency between human raters and
              an automated essay scoring system in grading high school
              students' English compositions. A total of 923 essays from 23
              classes of 12 senior high schools in Taiwan (Republic of China)
              were obtained and scored manually and electronically. The results
              show that the consistency between human raters is significantly
              higher than the consistency between human raters and the
              automated essay scoring system. To discover whether the students
              were confident with the automated essay grading system, a
              questionnaire was also distributed. The results indicate the
              participants realize their vocabulary level was inadequate, and
              they wanted to know the scores the automated essay grading system
              gave them along with the generated comments regarding their
              compositions.",
  journal  = "Action in Teacher Education",
  volume   =  34,
  issue    =  4,
  pages    = "328--335",
  year     =  2012,
  url      = "http://dx.doi.org/10.1080/01626620.2012.717033",
  eprint   = "http://www.tandfonline.com/doi/pdf/10.1080/01626620.2012.717033",
  doi      = "10.1080/01626620.2012.717033",
  keywords = {AWA}
}

@ARTICLE{Zhang2012-ym,
  title    = "Comparison of e-rater® Automated Essay Scoring Model Calibration
              Methods Based on Distributional Targets",
  author   = "Zhang, Mo and Williamson, David M. and Breyer, F. Jay and
              Trapani, Catherine",
  abstract = "This article describes two separate, related studies that provide
              insight into the effectiveness of e-rater score calibration
              methods based on different distributional targets. In the first
              study, we developed and evaluated a new type of e-rater scoring
              model that was cost-effective and applicable under conditions of
              absent human rating and small candidate volume. This new model
              type, called the Scale Midpoint Model, outperformed an existing
              e-rater scoring model that is often adopted by certain e-rater
              system users without modification. In the second study, we
              examined the impact of three distributional score calibration
              approaches on existing models’ performance. These approaches
              included percentile calibrations on e-rater scores in accordance
              with a human rating distribution, normal distribution, and
              uniform distribution. Results indicated that these score
              calibration approaches did not have overall positive effects on
              the performance of existing e-rater scoring models.",
  journal  = "International Journal of Testing",
  volume   =  12,
  issue    =  4,
  pages    = "345--364",
  year     =  2012,
  url      = "http://dx.doi.org/10.1080/15305058.2011.645973",
  eprint   = "http://www.tandfonline.com/doi/pdf/10.1080/15305058.2011.645973",
  doi      = "10.1080/15305058.2011.645973",
  keywords = {AWA}
}

@ARTICLE{Islam2012-cq,
  title   = "Automated Essay Scoring Using Generalized Latent Semantic Analysis",
  author  = "Islam, Md. Monjurul and Hoque, A. S. M. Latiful",
  journal = "J. Coll. Physicians Surg. Pak.",
  volume  =  7,
  issue   =  3,
  day     =  1,
  month   =  mar,
  year    =  2012,
  url     = "http://dx.doi.org/10.4304/jcp.7.3.616-626",
  issn    = "1022-386X",
  doi     = "10.4304/jcp.7.3.616-626",
  keywords = {AWA}
}

@ARTICLE{Chen2010-ht,
  title    = "An Unsupervised Automated Essay Scoring System",
  author   = "Chen, Yen-Yu and Liu, Chien-Liang and Chang, Tao-Hsing and Lee,
              Chia-Hoang",
  abstract = "The proposed automated essay-scoring system uses an
              unsupervised-learning approach based on a voting algorithm.
              Experiments show that this approach works well compared to
              supervised-learning approaches.",
  journal  = "Intelligent Systems, IEEE",
  volume   =  25,
  issue    =  5,
  pages    = "61--67",
  year     =  2010,
  url      = "http://dx.doi.org/10.1109/MIS.2010.3",
  keywords = "natural language processing;unsupervised learning;supervised
              learning approaches;unsupervised automated essay scoring
              system;voting algorithm;Humans;Large-scale systems;Learning
              systems;Machine learning;Natural language processing;Supervised
              learning;Text analysis;Training data;Unsupervised
              learning;Writing;intelligent systems;machine learning;natural
              language processing",
  issn     = "1541-1672",
  doi      = "10.1109/MIS.2010.3",
  keywords = {AWA}
}

@INCOLLECTION{Shermis2010-tn,
  title     = "Automated Essay Scoring in a High Stakes Testing Environment",
  booktitle = "Innovative Assessment for the 21st Century",
  author    = "Shermis, Mark D.",
  abstract  = "This chapter discusses the use of automated essay scoring (AES)
               as a possible replacement for an annual statewide high-stakes
               writing test. The examples provided are drawn from development
               work in the state of Florida, but might apply to any state in
               the United States. In the first section, literature associated
               with the frequency, costs, and consequences of high-stakes
               testing is reviewed. In the second section, automated essay
               scoring is introduced and a description of how it works as an
               assessment tool is provided. In the third section, an example of
               how AES is used as an instructional tool is given and I argue
               for a tighter integration of assessment with instruction.
               Finally, I propose that AES actually replace the high-stakes
               testing program for accountability (and other) purposes, and
               provide a list of advantages for proceeding in this fashion.",
  publisher = "Springer US",
  pages     = "167--185",
  day       =  1,
  month     =  jan,
  year      =  2010,
  url       = "http://dx.doi.org/10.1007/978-1-4419-6530-1_10",
  doi       = "10.1007/978-1-4419-6530-1\_10",
  keywords = {AWA}
}

@INCOLLECTION{Castro-Castro2008-bn,
  title     = "A Multilingual Application for Automated Essay Scoring",
  booktitle = "Advances in Artificial Intelligence – {IBERAMIA} 2008",
  author    = "Castro-Castro, Daniel and Lannes-Losada, Roc\'{\i}o and
               Maritxalar, Montse and Niebla, Ianire and P\'{e}rez-Marqu\'{e}s,
               Celia and \'{A}lamo-Su\'{a}rez, Nancy C. and Pons-Porrata,
               Aurora",
  abstract  = "In this paper, we present a text evaluation system for students
               to improve Basque or Spanish writing skills. The system uses
               Natural Language Processing techniques to evaluate essays by
               detecting specific measures. The application uses a
               client-server architecture and both the interface and the
               application itself are multilingual. The article also explains
               how the system can be adapted to evaluate Spanish essays written
               in Cuban schools.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "243--251",
  series    = "Lecture Notes in Computer Science",
  day       =  1,
  month     =  jan,
  year      =  2008,
  url       = "http://dx.doi.org/10.1007/978-3-540-88309-8_25",
  doi       = "10.1007/978-3-540-88309-8\_25",
  keywords = {AWA}
}

@ARTICLE{Kukich2000-sq,
  title   = "Beyond automated essay scoring",
  author  = "Kukich, K",
  journal = "IEEE Intell. Syst.",
  year    =  2000,
  issn    = "1541-1672",
  keywords = {AWA}
}

@ARTICLE{Ben-Simon2007-hp,
  title    = "Toward More Substantively Meaningful Automated Essay Scoring",
  author   = "Ben-Simon, Anat and Bennett, Randy Elliot",
  abstract = "This study evaluated a “substantively driven” method for scoring
              NAEP writing assessments automatically. The study used variations
              of an existing commercial program, e-rater®, to compare the
              performance of three approaches to automated essay scoring: a
              brute-empirical approach in which variables are selected and
              weighted solely according to statistical criteria, a hybrid
              approach in which a fixed set of variables more closely tied to
              the characteristics of good writing was used but the weights were
              still statistically determined, and a substantively driven
              approach in which a fixed set of variables was weighted according
              to the judgments of two independent committees of writing
              experts. The research questions concerned (1) the reproducibility
              of weights across writing experts, (2) the comparison of scores
              generated by the three automated approaches, and (3) the extent
              to which models developed for scoring one NAEP prompt generalize
              to other NAEP prompts of the same genre. Data came from the 2002
              NAEP Writing Online study and from the main NAEP 2002 writing
              assessment. Results showed that, in carrying out the
              substantively driven approach, experts initially assigned weights
              to writing dimensions that were highly similar across committees
              but that diverged from one another after committee 1 was shown
              the empirical weights for possible use in its judgments and
              committee 2 was not shown those weights. The substantively driven
              approach based on the judgments of committee 1 generally did not
              operate in a markedly different way from the brute empirical or
              hybrid approaches in most of the analyses conducted. In contrast,
              many consistent differences with those approaches were observed
              for the substantively driven approach based on the judgments of
              committee 2. This study suggests that empirical weights might
              provide a useful starting point for expert committees, with the
              understanding that the weights be moderated only somewhat to
              bring them more into line with substantive considerations. Under
              such circumstances, the results may turn out to be reasonable,
              though not necessarily as highly related to human ratings as
              statistically optimal approaches would produce.",
  journal  = "The Journal of Technology, Learning and Assessment",
  volume   =  6,
  issue    =  1,
  day      =  1,
  month    =  aug,
  year     =  2007,
  url      = "http://dx.doi.org/1631",
  issn     = "1540-2525",
  doi      = "1631",
  keywords = {AWA}
}

@ARTICLE{Slotnick1974-bi,
  title    = "Computer Scoring of Formal Letters",
  author   = "Slotnick, Henry B.",
  abstract = "This study examines the accuracy with which the computer could
              simulate human judgments of the overall quality of formal letters
              written by adults aged 26-35. The initial correlation between
              machine and human scoring was .85, and a subsequent test of the
              generalizability of the findings yielded a correlationof .78.
              Measures of fluency, diction, and spelling were important
              predictors ofoverall quality.",
  journal  = "Journal of Business Communication",
  volume   =  11,
  issue    =  2,
  pages    = "11--19",
  month    =  jan,
  year     =  1974,
  url      = "http://dx.doi.org/10.1177/002194367401100202",
  doi      = "10.1177/002194367401100202",
  keywords = {AWA}
}

@ARTICLE{Slotnick1971-wu,
  title    = "{BITS}, {NYBBLES}, {BYTES}: A View of Electronic Grading",
  author   = "Slotnick, Henry B. and Knapp, John V. and Bussell, Rodney L.",
  abstract = "Commonly used essay grading procedures are subjective and suffer
              from inconsistency and instability. An objective procedure,
              described here, involvesregressing a computer's measurements on
              grades assigned by an expert judge.Results of using the procedure
              on college freshman themes indicate somesuccess for it.",
  journal  = "Journal of Business Communication",
  volume   =  8,
  issue    =  2,
  pages    = "35--52",
  month    =  jan,
  year     =  1971,
  url      = "http://dx.doi.org/10.1177/002194367100800204",
  doi      = "10.1177/002194367100800204",
  keywords = {DL}
}

@ARTICLE{Webster1990-ek,
  title     = "Computers and writing assessment: A preliminary view",
  author    = "Webster, W",
  journal   = "Computers and Composition",
  publisher = "JAI",
  year      =  1990,
  keywords = {AWA}
}

@ARTICLE{Knievel2009-dl,
  title    = "What is Humanistic about Computers and Writing? Historical
              Patterns and Contemporary Possibilities for the Field",
  author   = "Knievel, Michael",
  abstract = "Given the status of Ellen W. Nold's “Fear and Trembling: The
              Humanist Approaches the Computer” (1975) as one of the first
              articles published in computers and writing, it may be said that
              the relationship between computers and the humanities has
              organized the field since its inception. In this article, I trace
              ways in which scholars have described that relationship in
              answering the implicit question of “what is humanistic about
              computers and writing?” from 1975 to present. The rhetorical
              positioning of the field vis-\`{a}-vis the question has evolved
              as shifts toward postmodern and social epistemologies in English
              studies, coupled with social and cultural trends catalyzed by new
              technologies, have challenged traditional humanities parameters.
              The resulting new spaces for humanistic argument have emboldened
              scholars in computers and writing to claim a more significant
              role in an emerging production-driven model of the humanities.
              This model is organized around an emphasis on electronic
              literacy, which has (1) disrupted the printed book's status as
              the central object of inquiry within the academy and, (2)
              importantly and concurrently, gained social and economic currency
              outside of it. In combination, these changes in social and
              academic contexts offer computers and writing an opportunity to
              embrace a more central role in the humanities than at any time in
              its history.",
  journal  = "Computers and Composition",
  volume   =  26,
  issue    =  2,
  pages    = "92--106",
  month    =  jun,
  year     =  2009,
  url      = "http://dx.doi.org/10.1016/j.compcom.2009.02.002",
  keywords = "Humanities; Humanistic; Technology; History; Literacy; Politics;
              English studies",
  issn     = "8755-4615",
  doi      = "10.1016/j.compcom.2009.02.002",
  keywords = {AWA}
}

@ARTICLE{Williamson2012-hs,
  title     = "A Framework for Evaluation and Use of Automated Scoring",
  author    = "Williamson, David M. and Xi, Xiaoming and Breyer, F. Jay",
  abstract  = "A framework for evaluation and use of automated scoring of
               constructed-response tasks is provided that entails both
               evaluation of automated scoring as well as guidelines for
               implementation and maintenance in the context of constantly
               evolving technologies. Consideration of validity issues and
               challenges associated with automated scoring are discussed
               within the framework. The fit between the scoring capability and
               the assessment purpose, the agreement between human and
               automated scores, the consideration of associations with
               independent measures, the generalizability of automated scores
               as implemented in operational practice across different tasks
               and test forms, and the impact and consequences for the
               population and subgroups are proffered as integral evidence
               supporting use of automated scoring. Specific evaluation
               guidelines are provided for using automated scoring to
               complement human scoring for tests used for high-stakes
               purposes. These guidelines are intended to be generalizable to
               new automated scoring systems and as existing systems change
               over time.",
  journal   = "Educational Measurement: Issues and Practice",
  publisher = "Blackwell Publishing Inc",
  volume    =  31,
  issue     =  1,
  pages     = "2--13",
  year      =  2012,
  url       = "http://dx.doi.org/10.1111/j.1745-3992.2011.00223.x",
  issn      = "1745-3992",
  doi       = "10.1111/j.1745-3992.2011.00223.x",
  keywords = {AWA}
}

@BOOK{Williamson2006-ur,
  title     = "Automated Scoring of Complex Tasks in Computer-based Testing",
  author    = "Williamson, DM and Mislevy, RJ and Bejar, II",
  publisher = "Lawrence Erlbaum Associates",
  year      =  2006,
  url       = "http://books.google.com/books?id=LLeP7j4AZOIC",
  keywords = {AWA}
}

@ARTICLE{Hiller1969-zl,
  title     = "Opinionation, Vagueness, and {Specificity-Distinctions}: Essay
               Traits Measured by Computer",
  author    = "Hiller, Jack H. and Marcotte, Donald R. and Martin, Timothy",
  abstract  = "Three characteristics of writing quality were selected for study
               among the many discussed in English texts on the assumption that
               single words or discrete phrases reliably cue the presence of
               such characteristics in essays. A set of 256 graded essays was
               searched by computer for cues, and the measures thus obtained
               were correlated with the essay grades. All predicted
               correlations were significant at p ≤ .01.",
  journal   = "Am. Educ. Res. J.",
  publisher = "American Educational Research Association",
  volume    =  6,
  issue     =  2,
  pages     = "271--286",
  month     =  mar,
  year      =  1969,
  url       = "http://dx.doi.org/10.2307/1161898",
  doi       = "10.2307/1161898",
  keywords = {AWA}
}

@ARTICLE{Spandel1981-lx,
  title     = "Direct measures of writing skill: Issues and applications",
  author    = "Spandel, V and Stiggins, RJ",
  publisher = "ERIC",
  year      =  1981,
  keywords = {AWA}
}

@ARTICLE{Page1968-ix,
  title     = "The Analysis of Essays by Computer. Final Report",
  author    = "Page, EB and Paulus, DH",
  publisher = "ERIC",
  year      =  1968,
  keywords = {AWA}
}

@ARTICLE{Page1997-ea,
  title     = "Computer Analysis of Student Essays: Finding Trait Differences
               in Student Profile",
  author    = "Page, EB and Poggio, JP and Keith, TZ",
  publisher = "ERIC",
  year      =  1997,
  keywords = {AWA}
}

@ARTICLE{Page1968-lf,
  title     = "The use of the computer in analyzing student essays",
  author    = "Page, Ellis B.",
  journal   = "Int. Rev. Educ.",
  publisher = "Martinus Nijhoff, The Hague/Kluwer Academic Publishers",
  volume    =  14,
  issue     =  2,
  pages     = "210--225",
  day       =  1,
  month     =  jun,
  year      =  1968,
  url       = "http://dx.doi.org/10.1007/BF01419938",
  issn      = "0020-8566",
  doi       = "10.1007/BF01419938",
  keywords = {AWA}
}

@ARTICLE{Page1966-lh,
  title    = "{GRADING} {ESSAYS} {BY} {COMPUTER}: {PROGRESS} {REPORT}",
  author   = "Page, Ellis B.",
  journal  = "Proceedings of the Invitational Conference on Testing Problems",
  year     =  1966,
  url      = "http://psycnet.apa.org/psycinfo/1967-15820-001",
  keywords = {AWA}
}

@ARTICLE{Page2003-bs,
  title     = "Project essay grade: {PEG}",
  author    = "Page, EB",
  journal   = "Automated essay scoring: A cross-",
  publisher = "Mahwah, NJ: Lawrence Erlbaum …",
  year      =  2003,
  keywords = {AWA}
}

@ARTICLE{Page1995-cv,
  title     = "The Computer Moves into Essay Grading: Updating the Ancient Test",
  author    = "Page, E and Petersen, NS",
  journal   = "Phi Delta Kappan",
  publisher = "ERIC",
  year      =  1995,
  issn      = "0031-7217",
  keywords = {AWA}
}

@ARTICLE{Page1994-ow,
  title    = "Computer Grading of Student Prose, Using Modern Concepts and
              Software",
  author   = "Page, Ellis Batten",
  abstract = "Abstract In earlier work of Project Essay Grade (PEG) we used
              computers to evaluate prose of high school students. In major
              experiments, PEG successfully imitated single human ratings,
              despite the crude hardware and software of the late 1960s. Today,
              computers are common in home and school, and advanced software
              packages permit much more powerful analysis. In the present
              research we analyzed recent federal samples of 495 and 599 essays
              and simulated groups of human judges, reaching multiple Rs as
              high as .87, close to the apparent reliability of the targeted
              judge groups. We also generated weights from formative samples of
              two thirds each, which predicted well the other one-third samples
              (with simple rs higher than .84). Another cross-validation
              predicted across different years, students, and judge panels,
              with an r of .83. Thus, the computer surpassed two judges, which
              is the usual human panel. Results appear encouraging for further
              research and indeed for early application to large programs of
              essay evaluation and reporting.",
  journal  = "J. Exp. Educ.",
  volume   =  62,
  issue    =  2,
  pages    = "127--142",
  year     =  1994,
  url      = "http://dx.doi.org/10.1080/00220973.1994.9943835",
  eprint   = "http://www.tandfonline.com/doi/pdf/10.1080/00220973.1994.9943835",
  issn     = "0022-0973",
  doi      = "10.1080/00220973.1994.9943835",
  keywords = {AWA}
}

@ARTICLE{Page1966-dc,
  title     = "The Imminence of... Grading Essays by Computer",
  author    = "Page, Ellis B.",
  journal   = "Phi Delta Kappan",
  publisher = "Phi Delta Kappa International",
  volume    =  47,
  issue     =  5,
  pages     = "238--243",
  month     =  jan,
  year      =  1966,
  url       = "http://dx.doi.org/10.2307/20371545",
  doi       = "10.2307/20371545",
  keywords = {AWA}
}

@ARTICLE{Klobucar2012-dy,
  title     = "Automated essay scoring and the search for valid writing
               assessment",
  author    = "Klobucar, A and Deane, P and Elliot, N and others",
  abstract  = "In educational settings, assessment targets determine the need
               for local validation. Instructional improvement, for example, is
               validated by examining the relationship between curricular
               innovations and improvements in criterion measures such as
               course grades. In ...",
  journal   = "advances in writing …",
  publisher = "wac.colostate.edu",
  year      =  2012,
  url       = "http://wac.colostate.edu/books/wrab2011/chapter6.pdf",
  keywords = {AWA}
}

@ARTICLE{Matzen2004-yp,
  title   = "Basic Writing Placement with Holistically Scored Essays: Research
             Evidence",
  author  = "Matzen, Richard N. and Hoyt, Jeff E",
  journal = "Journal of Developmental Education",
  volume  =  28,
  issue   =  1,
  year    =  2004,
  url     = "http://www.elcentrocollege.edu/Campus_Info/TitleV/docs/Basic%20Writing%20Placement%20with%20Holistically%20Scored%20Essays.pdf",
  keywords = {AWA}
}

@ARTICLE{Blei2012-hn,
  title     = "Probabilistic topic models",
  author    = "Blei, David M.",
  journal   = "Communications of the ACM",
  publisher = "ACM",
  volume    =  55,
  issue     =  4,
  pages     = "77--84",
  day       =  1,
  month     =  apr,
  year      =  2012,
  url       = "http://dx.doi.org/10.1145/2133806.2133826",
  issn      = "0001-0782",
  doi       = "10.1145/2133806.2133826",
  keywords = {DL}
}

@INCOLLECTION{Steyvers2007-ei,
  title     = "Probabilistic Topic Models",
  booktitle = "Latent Semantic Analysis: A Road to Meaning",
  author    = "Steyvers, Mark and Griffiths, Tom",
  editor    = "{T. Landauer, D McNamara, S. Dennis, and W. Kintsch}",
  publisher = "Laurence Erlbaum",
  year      =  2007,
  url       = "http://faculty.washington.edu/jwilker/559/SteyversGriffiths.pdf",
  keywords = {DL}
}

@UNPUBLISHED{Hofmann2013-ui,
  title         = "Probabilistic Latent Semantic Analysis",
  author        = "Hofmann, Thomas",
  abstract      = "Probabilistic Latent Semantic Analysis is a novel
                   statistical technique for the analysis of two-mode and
                   co-occurrence data, which has applications in information
                   retrieval and filtering, natural language processing,
                   machine learning from text, and in related areas. Compared
                   to standard Latent Semantic Analysis which stems from linear
                   algebra and performs a Singular Value Decomposition of
                   co-occurrence tables, the proposed method is based on a
                   mixture decomposition derived from a latent class model.
                   This results in a more principled approach which has a solid
                   foundation in statistics. In order to avoid overfitting, we
                   propose a widely applicable generalization of maximum
                   likelihood model fitting by tempered EM. Our approach yields
                   substantial and consistent improvements over Latent Semantic
                   Analysis in a number of experiments.",
  journal       = "arXiv [cs.LG]",
  day           =  23,
  month         =  01,
  year          =  2013,
  url           = "http://arxiv.org/abs/1301.6705",
  archivePrefix = "arXiv",
  eprint        = "1301.6705",
  primaryClass  = "cs.LG",
  arxivid       = "1301.6705",
  keywords = {DL}
}


%@MISC{Socher_undated-we,
%  title  = "Deep Learning for {NLP} (without Magic) - {NAACL} 2013",
%  author = "Socher, Richard and Manning, Christopher",
%  url    = "http://nlp.stanford.edu/courses/NAACL2013/"
%}

%@MISC{Bengio_undated-bi,
%  title  = "Deep Learning of Representations - {AAAI} 2013 Tutorial",
%  author = "Bengio, Yoshua" %}

@UNPUBLISHED{Mnih2012-wp,
  title         = "A Fast and Simple Algorithm for Training Neural
                   Probabilistic Language Models",
  author        = "Mnih, Andriy and Teh, Yee Whye",
  abstract      = "In spite of their superior performance, neural probabilistic
                   language models (NPLMs) remain far less widely used than
                   n-gram models due to their notoriously long training times,
                   which are measured in weeks even for moderately-sized
                   datasets. Training NPLMs is computationally expensive
                   because they are explicitly normalized, which leads to
                   having to consider all words in the vocabulary when
                   computing the log-likelihood gradients. We propose a fast
                   and simple algorithm for training NPLMs based on
                   noise-contrastive estimation, a newly introduced procedure
                   for estimating unnormalized continuous distributions. We
                   investigate the behaviour of the algorithm on the Penn
                   Treebank corpus and show that it reduces the training times
                   by more than an order of magnitude without affecting the
                   quality of the resulting models. The algorithm is also more
                   efficient and much more stable than importance sampling
                   because it requires far fewer noise samples to perform well.
                   We demonstrate the scalability of the proposed approach by
                   training several neural language models on a 47M-word corpus
                   with a 80K-word vocabulary, obtaining state-of-the-art
                   results on the Microsoft Research Sentence Completion
                   Challenge dataset.",
  journal       = "arXiv [cs.CL]",
  day           =  27,
  month         =  06,
  year          =  2012,
  url           = "http://arxiv.org/abs/1206.6426",
  archivePrefix = "arXiv",
  eprint        = "1206.6426",
  primaryClass  = "cs.CL",
  arxivid       = "1206.6426",
  keywords = {DL}
}

@UNPUBLISHED{Goodfellow2013-sb,
  title         = "Pylearn2: a machine learning research library",
  author        = "Goodfellow, Ian J. and Warde-Farley, David and Lamblin,
                   Pascal and Dumoulin, Vincent and Mirza, Mehdi and Pascanu,
                   Razvan and Bergstra, James and Bastien, Fr\'{e}d\'{e}ric and
                   Bengio, Yoshua",
  abstract      = "Pylearn2 is a machine learning research library. This does
                   not just mean that it is a collection of machine learning
                   algorithms that share a common API; it means that it has
                   been designed for flexibility and extensibility in order to
                   facilitate research projects that involve new or unusual use
                   cases. In this paper we give a brief history of the library,
                   an overview of its basic philosophy, a summary of the
                   library's architecture, and a description of how the
                   Pylearn2 community functions socially.",
  journal       = "arXiv [stat.ML]",
  day           =  20,
  month         =  08,
  year          =  2013,
  url           = "http://arxiv.org/abs/1308.4214",
  archivePrefix = "arXiv",
  eprint        = "1308.4214",
  primaryClass  = "stat.ML",
  arxivid       = "1308.4214",
  keywords = {DL}
}

@UNPUBLISHED{Bengio2012-qg,
  title         = "Representation Learning: A Review and New Perspectives",
  author        = "Bengio, Yoshua and Courville, Aaron and Vincent, Pascal",
  abstract      = "The success of machine learning algorithms generally depends
                   on data representation, and we hypothesize that this is
                   because different representations can entangle and hide more
                   or less the different explanatory factors of variation
                   behind the data. Although specific domain knowledge can be
                   used to help design representations, learning with generic
                   priors can also be used, and the quest for AI is motivating
                   the design of more powerful representation-learning
                   algorithms implementing such priors. This paper reviews
                   recent work in the area of unsupervised feature learning and
                   joint training of deep learning, covering advances in
                   probabilistic models, auto-encoders, manifold learning, and
                   deep architectures. This motivates longer-term unanswered
                   questions about the appropriate objectives for learning good
                   representations, for computing representations (i.e.,
                   inference), and the geometrical connections between
                   representation learning, density estimation and manifold
                   learning.",
  journal       = "arXiv [cs.LG]",
  day           =  24,
  month         =  06,
  year          =  2012,
  url           = "http://arxiv.org/abs/1206.5538",
  archivePrefix = "arXiv",
  eprint        = "1206.5538",
  primaryClass  = "cs.LG",
  arxivid       = "1206.5538",
  keywords = {DL}
}

%@ARTICLE{Hahnloser1998-os,
%  title       = "On the piecewise analysis of networks of linear threshold
%                 neurons",
%  author      = "Hahnloser, R L.t.",
%  affiliation = "Institut f{\"{u}}r Neuroinformatik ETHZ/UNIZ, Z{\"{u}}rich,
%                 Switzerland",
%  abstract    = "The computational abilities of recurrent networks of neurons
%                 with a linear activation function above threshold are
%                 analyzed. These networks selectively realise a linear mapping
%                 of their input. Using this property, the dynamics as well as
%                 the number and the stability of stationary states can be
%                 investigated. The important property of the boundedness of
%                 neural activities can be guaranteed by global inhibition. If
%                 used together with self-excitation, the global inhibition
%                 gives rise to a multi stable winner-take-all (WTA) mechanism.
%                 A condition for a neuron to be a potential winner of the
%                 competing dynamics is derived. The network becomes a largest
%                 input selector when the self-excitation is marginal.Slowing
%                 down the global inhibition produces oscillations. The study of
%                 oscillations of random networks suggests that all cyclic
%                 trajectories of linear threshold networks are a result of the
%                 existence of partitions with undamped linear oscillations.
%                 Chaotic dynamics were never encountered in computer
%                 simulations and perhaps do not exist at all in small networks.",
%  journal     = "Neural Netw.",
%  volume      =  11,
%  issue       =  4,
%  pages       = "691--697",
%  day         =  29,
%  month       =  jun,
%  year        =  1998,
%  url         = "http://www.ncbi.nlm.nih.gov/pubmed/12662807",
%  kind        = "Research Article",
%  issn        = "0893-6080",
%  pmid        = "12662807",
%  keywords = {DL}
%}

@UNPUBLISHED{Glehre2013-xf,
  title         = "Knowledge Matters: Importance of Prior Information for
                   Optimization",
  author        = "G{\"{u}}l\c{c}ehre, \c{C}a\u{g}lar and Bengio, Yoshua",
  abstract      = "We explore the effect of introducing prior information into
                   the intermediate level of neural networks for a learning
                   task on which all the state-of-the-art machine learning
                   algorithms tested failed to learn. We motivate our work from
                   the hypothesis that humans learn such intermediate concepts
                   from other individuals via a form of supervision or guidance
                   using a curriculum. The experiments we have conducted
                   provide positive evidence in favor of this hypothesis. In
                   our experiments, a two-tiered MLP architecture is trained on
                   a dataset with 64x64 binary inputs images, each image with
                   three sprites. The final task is to decide whether all the
                   sprites are the same or one of them is different. Sprites
                   are pentomino tetris shapes and they are placed in an image
                   with different locations using scaling and rotation
                   transformations. The first part of the two-tiered MLP is
                   pre-trained with intermediate-level targets being the
                   presence of sprites at each location, while the second part
                   takes the output of the first part as input and predicts the
                   final task's target binary event. The two-tiered MLP
                   architecture, with a few tens of thousand examples, was able
                   to learn the task perfectly, whereas all other algorithms
                   (include unsupervised pre-training, but also traditional
                   algorithms like SVMs, decision trees and boosting) all
                   perform no better than chance. We hypothesize that the
                   optimization difficulty involved when the intermediate
                   pre-training is not performed is due to the \textbackslashem
                   composition of two highly non-linear tasks. Our findings are
                   also consistent with hypotheses on cultural learning
                   inspired by the observations of optimization problems with
                   deep learning, presumably because of effective local minima.",
  journal       = "arXiv [cs.LG]",
  day           =  17,
  month         =  01,
  year          =  2013,
  url           = "http://arxiv.org/abs/1301.4083",
  archivePrefix = "arXiv",
  eprint        = "1301.4083",
  primaryClass  = "cs.LG",
  arxivid       = "1301.4083",
  keywords = {DL}
}

@ARTICLE{Srivastava2013-yx,
  title     = "Improving neural networks with dropout",
  author    = "Srivastava, N",
  abstract  = "Neural networks are powerful computational models that are being
               used extensively for solving problems in vision, speech, natural
               language processing and many other areas. In spite of many
               successes, neural networks still suffer from a major weakness.
               The ...",
  publisher = "cs.toronto.edu",
  year      =  2013,
  url       = "http://www.cs.toronto.edu/~nitish/msc_thesis.pdf",
  keywords = {DL}
}

@UNPUBLISHED{Le2011-hy,
  title         = "Building high-level features using large scale unsupervised
                   learning",
  author        = "Le, Quoc V. and Ranzato, Marc'aurelio and Monga, Rajat and
                   Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean,
                   Jeff and Ng, Andrew Y.",
  abstract      = "We consider the problem of building high-level,
                   class-specific feature detectors from only unlabeled data.
                   For example, is it possible to learn a face detector using
                   only unlabeled images? To answer this, we train a 9-layered
                   locally connected sparse autoencoder with pooling and local
                   contrast normalization on a large dataset of images (the
                   model has 1 billion connections, the dataset has 10
                   million 200x200 pixel images downloaded from the Internet).
                   We train this network using model parallelism and
                   asynchronous SGD on a cluster with 1,000 machines (16,000
                   cores) for three days. Contrary to what appears to be a
                   widely-held intuition, our experimental results reveal
                   that it is possible to train a face detector without having
                   to label images as containing a face or not. Control
                   experiments show that this feature detector is robust not
                   only to translation but also to scaling and out-of-plane
                   rotation. We also find that the same network is sensitive to
                   other high-level concepts such as cat faces and human bod-
                   ies. Starting with these learned features, we trained our
                   network to obtain 15.8\% accuracy in recognizing 20,000
                   object categories from ImageNet, a leap of 70\% relative im-
                   provement over the previous state-of-the-art.",
  journal       = "arXiv [cs.LG]",
  publisher     = "arxiv.org",
  day           =  29,
  month         =  dec,
  year          =  2011,
  url           = "http://arxiv.org/abs/1112.6209",
  archivePrefix = "arXiv",
  eprint        = "1112.6209",
  primaryClass  = "cs.LG",
  arxivid       = "1112.6209",
  keywords = {DL}
}


@ARTICLE{journals/jmlr/RaikoVL12,
  title   = "Deep Learning Made Easier by Linear Transformations in Perceptrons",
  author  = "{Tapani Raiko, Harri Valpola, and Yann LeCun}",
  journal = "Journal of Machine Learning Research",
  url     = "http://jmlr.org/proceedings/papers/v22/raiko12/raiko12.pdf",
  keywords = {DL}
}

@BOOK{Shermis2003-om,
  title     = "Automated essay scoring: A cross-disciplinary perspective",
  author    = "Shermis, Mark D and Burstein, Jill C",
  publisher = "Psychology Press",
  year      =  2003,
  keywords = {AWA}
}

@TECHREPORT{Turney2010-qx,
  title    = "From Frequency to Meaning: Vector Space Models of Semantics",
  author   = "Turney, Peter D. and Pantel, Patrick",
  abstract = "Computers understand very little of the meaning of human
              language. This profoundly limits our ability to give instructions
              to computers, the ability of computers to explain their actions
              to us, and the ability of computers to analyse and process text.
              Vector space models (VSMs) of semantics are beginning to address
              these limits. This paper surveys the use of VSMs for semantic
              processing of text. We organize the literature on VSMs according
              to the structure of the matrix in a VSM. There are currently
              three broad classes of VSMs, based on term-document,
              word-context, and pair-pattern matrices, yielding three classes
              of applications. We survey a broad range of applications in these
              three categories and we take a detailed look at a specific open
              source project in each category. Our goal in this survey is to
              show the breadth of applications of VSMs for semantics, to
              provide a new perspective on VSMs for those who are already
              familiar with the area, and to provide pointers into the
              literature for those who are less familiar with the field.",
  month    =  mar,
  year     =  2010,
  url      = "http://arxiv.org/abs/1003.1141",
  keywords = "NLP",
  keywords = {DL}
}

@INPROCEEDINGS{Burstein1999-qv,
  title     = "Automated essay scoring for nonnative English speakers",
  booktitle = "Proceedings of a Symposium on Computer Mediated Language
               Assessment and Evaluation in Natural Language Processing",
  author    = "Burstein, Jill and Chodorow, Martin",
  publisher = "Association for Computational Linguistics",
  pages     = "68--75",
  day       =  1,
  month     =  jun,
  year      =  1999,
  url       = "http://dl.acm.org/citation.cfm?id=1598847",
  keywords = {AWA}
}

@ARTICLE{Bishop1995-ry,
  title    = "Training with Noise is Equivalent to Tikhonov Regularization",
  author   = "Bishop, Chris M.",
  abstract = "It is well known that the addition of noise to the input data of
              a neural network during training can, in some circumstances, lead
              to significant improvements in generalization performance.
              Previous work has shown that such training with noise is
              equivalent to a form of regularization in which an extra term is
              added to the error function. However, the regularization term,
              which involves second derivatives of the error function, is not
              bounded below, and so can lead to difficulties if used directly
              in a learning algorithm based on error minimization. In this
              paper we show that for the purposes of network training, the
              regularization term can be reduced to a positive semi-definite
              form that involves only first derivatives of the network mapping.
              For a sum-of-squares error function, the regularization term
              belongs to the class of generalized Tikhonov regularizers. Direct
              minimization of the regularized error function provides a
              practical alternative to training with noise.",
  journal  = "Neural Computation",
  volume   =  7,
  issue    =  1,
  pages    = "108--116",
  month    =  jan,
  year     =  1995,
  url      = "http://dx.doi.org/10.1162/neco.1995.7.1.108",
  issn     = "0899-7667",
  doi      = "10.1162/neco.1995.7.1.108",
  keywords = {DL}
}

@BOOK{Ericsson2006-bm,
  title     = "Machine scoring of student essays truth and consequences",
  author    = "Ericsson, Patricia Freitag and Haswell, Richard H",
  abstract  = "The current trend toward machine-scoring of student work,
               Ericsson and Haswell argue, has created an emerging issue with
               implications for higher education across the disciplines, but
               with particular importance for those in English departments and
               in administration. The academic community has been silent on the
               issue-some would say excluded from it-while the commercial
               entities who develop essay-scoring software have been very
               active. Machine Scoring of Student Essays is the first volume to
               seriously consider the educational mechanisms and consequences
               of this trend, and it offers important discussions from some of
               the leading scholars in writing assessment. Reading and
               evaluating student writing is a time-consuming process, yet it
               is a vital part of both student placement and coursework at
               post-secondary institutions. In recent years, commercial
               computer-evaluation programs have been developed to score
               student essays in both of these contexts. Two-year colleges have
               been especially drawn to these programs, but four-year
               institutions are moving to them as well, because of the
               cost-savings they promise. Unfortunately, to a large extent, the
               programs have been written, and institutions are installing
               them, without attention to their instructional validity or
               adequacy. Since the education software companies are moving so
               rapidly into what they perceive as a promising new market, a
               wider discussion of machine-scoring is vital if scholars hope to
               influence development and/or implementation of the programs
               being created. What is needed, then, is a critical resource to
               help teachers and administrators evaluate programs they might be
               considering, and to more fully envision the instructional
               consequences of adopting them. And this is the resource that
               Ericsson and Haswell are providing here.",
  publisher = "Utah State University Press",
  year      =  2006,
  url       = "http://site.ebrary.com/id/10328608",
  keywords = {AWA}
}

@INPROCEEDINGS{Tieleman2009-nl,
  title  = "Using fast weights to improve persistent contrastive divergence",
  author = "Tieleman, Tijmen and Hinton, Geoffrey",
  pages  = "1033--1040",
  year   =  2009,
  keywords = {DL}
}

@ARTICLE{Frasconi1998-fm,
  title    = "A general framework for adaptive processing of data structures",
  author   = "Frasconi, P and Gori, M and Sperduti, A",
  abstract = "A structured organization of information is typically required by
              symbolic processing. On the other hand, most connectionist models
              assume that data are organized according to relatively poor
              structures, like arrays or sequences. The framework described in
              this paper is an attempt to unify adaptive models like artificial
              neural nets and belief nets for the problem of processing
              structured information. In particular, relations between data
              variables are expressed by directed acyclic graphs, where both
              numerical and categorical values coexist. The general framework
              proposed in this paper can be regarded as an extension of both
              recurrent neural networks and hidden Markov models to the case of
              acyclic graphs. In particular we study the supervised learning
              problem as the problem of learning transductions from an input
              structured space to an output structured space, where
              transductions are assumed to admit a recursive hidden state-space
              representation. We introduce a graphical formalism for
              representing this class of adaptive transductions by means of
              recursive networks, i.e., cyclic graphs where nodes are labeled
              by variables and edges are labeled by generalized delay elements.
              This representation makes it possible to incorporate the symbolic
              and subsymbolic nature of data. Structures are processed by
              unfolding the recursive network into an acyclic graph called
              encoding network. In so doing, inference and learning algorithms
              can be easily inherited from the corresponding algorithms for
              artificial neural networks or probabilistic graphical model",
  journal  = "IEEE Trans. Neural Netw.",
  volume   =  9,
  issue    =  5,
  pages    = "768--786",
  year     =  1998,
  url      = "http://dx.doi.org/10.1109/72.712151",
  keywords = "acyclic graphs; adaptive systems; adaptive transductions;
              Artificial neural networks; belief nets; categorical values; data
              structures; data variables; directed acyclic graphs; encoding;
              generalized delay elements; graph theory; graphical formalism;
              Graphical models; hidden Markov models; HMM; inference
              algorithms; input structured space; learning (artificial
              intelligence); learning algorithms; Neural networks; numerical
              values; output structured space; probabilistic graphical model;
              Problem-solving; recurrent neural nets; Recurrent neural
              networks; recursive hidden state-space representation; recursive
              network unfolding; subsymbolic data; supervised learning; symbol
              manipulation; symbolic data; symbolic processing; transduction",
  issn     = "1045-9227",
  doi      = "10.1109/72.712151",
  keywords = {DL}
}

@BOOK{Socher_undated-kh,
  title    = "Parsing Natural Scenes and Natural Language with Recursive Neural
              Networks",
  author   = "Socher, Richard and Lin, Cliff Chiung-Yu and Ng, Andrew Y. and
              Manning, Christopher D.",
  abstract = "Recursive structure is commonly found in the inputs of different
              modalities such as natural scene images or natural language
              sentences. Discovering this recursive structure helps us to not
              only identify the units that an image or sentence contains but
              also how they interact to form a whole. We introduce a max-margin
              structure prediction architecture based on recursive neural
              networks that can successfully recover such structure both in
              complex scene images as well as sentences. The same algorithm can
              be used both to provide a competitive syntactic parser for
              natural language sentences from the Penn Treebank and to
              outperform alternative approaches for semantic scene
              segmentation, annotation and classification. For segmentation and
              annotation our algorithm obtains a new level of state-of-theart
              performance on the Stanford background dataset (78.1\%). The
              features from the image parse tree outperform Gist descriptors
              for scene classification by 4\%. 1.",
  keywords = {DL}
}

@ARTICLE{Rudner2006-qb,
  title   = "An evaluation of {IntelliMetric™} essay scoring system",
  author  = "Rudner, Lawrence M and Garcia, Veronica and Welch, Catherine",
  journal = "The Journal of Technology, Learning and Assessment",
  volume  =  4,
  issue   =  4,
  year    =  2006,
  keywords = {AWA}
}

@ARTICLE{Powers2002-kt,
  title    = "Stumping e-rater: challenging the validity of automated essay
              scoring",
  author   = "Powers, Donald E. and Burstein, Jill C. and Chodorow, Martin and
              Fowles, Mary E. and Kukich, Karen",
  abstract = "For this study, various parties were invited to “challenge”
              e-rater—an automated essay scorer that relies on natural language
              processing techniques—by composing essays in response to Graduate
              Record Examinations (GRE®) Writing Assessment prompts with the
              intention of undermining its scoring capability. Specifically,
              using detailed information about e-rater's approach to essay
              scoring, writers tried to “trick” the computer-based system into
              assigning scores that were higher or lower than deserved.
              E-rater's automated scores on these “problem essays” were
              compared with scores given by two trained, human readers, and the
              difference between the scores constituted the standard for
              judging the extent to which e-rater was fooled. Challengers were
              differentially successful in writing problematic essays. As a
              whole, they were more successful in tricking e-rater into
              assigning scores that were too high than in duping e-rater into
              awarding scores that were too low. The study provides information
              on ways in which e-rater, and perhaps other automated essay
              scoring systems, may fail to provide accurate evaluations, if
              used as the sole method of scoring in high-stakes assessments.
              The results suggest possible avenues for improving automated
              scoring methods.",
  journal  = "Comput. Human Behav.",
  volume   =  18,
  issue    =  2,
  pages    = "103--134",
  month    =  mar,
  year     =  2002,
  url      = "http://dx.doi.org/10.1016/S0747-5632(01)00052-8",
  keywords = "Automated scoring; Computer-assisted; Essay scoring; Graduate
              Record Examinations (GRE); Validity; Writing assessment",
  issn     = "0747-5632",
  doi      = "10.1016/S0747-5632(01)00052-8",
  keywords = {AWA}
}

@ARTICLE{Powers2000-ne,
  title   = "Comparing the validity of automated and human essay scoring",
  author  = "Powers, Donald E and Burstein, Jill C and Chodorow, Martin and
             Fowles, Mary E and Kukich, Karen and Board, Graduate Record
             Examinations",
  journal = "RESEARCH REPORT-EDUCATIONAL TESTING SERVICE PRINCETON RR",
  issue   =  10,
  year    =  2000,
  keywords = {AWA}
}

@ARTICLE{Chodorow2004-dl,
  title   = "Beyond Essay Length: Evaluating e-rater®'s Performance on {TOEFL®}
             Essays",
  author  = "Chodorow, Martin and Burstein, Jill",
  journal = "RESEARCH REPORT-EDUCATIONAL TESTING SERVICE PRINCETON RR",
  volume  =  4,
  year    =  2004,
  keywords = {AWA}
}

@ARTICLE{Tenenbaum2000-ok,
  title    = "A Global Geometric Framework for Nonlinear Dimensionality
              Reduction",
  author   = "Tenenbaum, Joshua and de Silva, Vin and Langford, John",
  abstract = "Scientists working with large volumes of high-dimensional data,
              such as global climate patterns, stellar spectra, or human gene
              distributions, regularly confront the problem of dimensionality
              reduction: finding meaningful low-dimensional structures hidden
              in their high-dimensional observations. The human brain confronts
              the same problem in everyday perception, extracting from its
              high-dimensional sensory inputs—30,000 auditory nerve fibers or
              106 optic nerve fibers—a manageably small number of perceptually
              relevant features. Here we describe an approach to solving
              dimensionality reduction problems that uses easily measured local
              metric information to learn the underlying global geometry of a
              data set. Unlike classical techniques such as principal component
              analysis (PCA) and multidimensional scaling (MDS), our approach
              is capable of discovering the nonlinear degrees of freedom that
              underlie complex natural observations, such as human handwriting
              or images of a face under different viewing conditions. In
              contrast to previous algorithms for nonlinear dimensionality
              reduction, ours efficiently computes a globally optimal solution,
              and, for an important class of data manifolds, is guaranteed to
              converge asymptotically to the true structure.",
  journal  = "Science",
  volume   =  290,
  issue    =  5500,
  pages    = "2319--2323",
  month    =  dec,
  year     =  2000,
  url      = "http://dx.doi.org/10.1126/science.290.5500.2319",
  keywords = "dimensionality-reduction; machine-learning",
  issn     = "0036-8075",
  doi      = "10.1126/science.290.5500.2319",
  keywords = {DL}
}

@ARTICLE{Bengio2009-pj,
    title   = "Learning Deep Architectures for {AI}",
    author  = "Bengio, Yoshua",
    journal = "Foundations and Trends in Machine Learning",
    volume  =  2,
    number  =  1,
    pages   = "1--127",
    month   =  jan,
    year    =  2009,
    url     = "http://dx.doi.org/10.1561/2200000006",
    issn    = "1935-8237",
    doi     = "10.1561/2200000006"
}

@INPROCEEDINGS{Larkey1998-ty,
  title    = "Automatic Essay Grading Using Text Categorization Techniques",
  author   = "Larkey, Leah",
  abstract = "The commas are the most useful and usable of all the stops. It is
              highly important to put them in place as you go along. If you try
              to come back after doing a paragraph and stick them in the
              various spots that tempt you you will discover that they tend to
              swarm like minnows into all sorts of crevices whose existence you
              hadnt realized and before you know it the whole long sentence
              becomes immobilized and lashed up squirming in commas. Better to
              use them sparingly, and with affection precisely when the need
              for one arises, nicely, by itself.",
  pages    = "90--95",
  year     =  1998,
  keywords = {AWA}
}

@inproceedings{Vincent2008-yj,
    title={Extracting and composing robust features with denoising autoencoders},
    author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
    booktitle={Proceedings of the 25th international conference on Machine learning},
    pages={1096--1103},
    year={2008},
    organization={ACM}
}

@ARTICLE{Bordes2012-sb,
  title    = "Joint Learning of Words and Meaning Representations for
              {Open-Text} Semantic Parsing",
  author   = "Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio,
              Yoshua",
  journal  = "Journal of Machine Learning Research - Proceedings Track",
  volume   =  22,
  pages    = "127--135",
  year     =  2012,
  keywords = {DL}
}

@ARTICLE{Ramineni2013-zq,
  title    = "Automated essay scoring: Psychometric guidelines and practices",
  author   = "Ramineni, Chaitanya and Williamson, David M.",
  abstract = "In this paper, we provide an overview of psychometric procedures
              and guidelines Educational Testing Service (ETS) uses to evaluate
              automated essay scoring for operational use. We briefly describe
              the e-rater system, the procedures and criteria used to evaluate
              e-rater, implications for a range of potential uses of e-rater,
              and directions for future research. The description of e-rater
              includes a summary of characteristics of writing covered by
              e-rater, variations in modeling techniques available, and the
              regression-based model building procedure. The evaluation
              procedures cover multiple criteria, including association with
              human scores, distributional differences, subgroup differences
              and association with external variables of interest. Expected
              levels of performance for each evaluation are provided. We
              conclude that the a priori establishment of performance
              expectations and the evaluation of performance of e-rater against
              these expectations help to ensure that automated scoring provides
              a positive contribution to the large-scale assessment of writing.
              We call for continuing transparency in the design of automated
              scoring systems and clear and consistent expectations of
              performance of automated scoring before using such systems
              operationally.",
  journal  = "Assessing Writing",
  volume   =  18,
  issue    =  1,
  pages    = "25--39",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1016/j.asw.2012.10.004",
  keywords = "Automated Essay Scoring; Constructed response items; Large-scale
              writing assessments",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2012.10.004",
  keywords = {AWA}
}

@ARTICLE{Rudner2002-xe,
  title    = "Automated Essay Scoring Using Bayes' Theorem",
  author   = "Rudner, Lawrence M. and Liang, Tahung",
  abstract = "Automated Essay Scoring Using Bayes' Theorem",
  journal  = "The Journal of Technology, Learning and Assessment",
  volume   =  1,
  issue    =  2,
  month    =  jan,
  year     =  2002,
  url      = "http://ejournals.bc.edu/ojs/index.php/jtla/article/view/1668",
  issn     = "1540-2525",
  keywords = {AWA}
}

@ARTICLE{Vincent2010-yf,
  title    = "Stacked Denoising Autoencoders: Learning Useful Representations
              in a Deep Network with a Local Denoising Criterion",
  author   = "Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and
              Bengio, Yoshua and Manzagol, Pierre-Antoine",
  abstract = "We explore an original strategy for building deep networks, based
              on stacking layers of denoising autoencoders which are trained
              locally to denoise corrupted versions of their inputs. The
              resulting algorithm is a straightforward variation on the
              stacking of ordinary autoencoders. It is however shown on a
              benchmark of classification problems to yield significantly lower
              classification error, thus bridging the performance gap with deep
              belief networks (DBN), and in several cases surpassing it. Higher
              level representations learnt in this purely unsupervised fashion
              also help boost the performance of subsequent SVM classifiers.
              Qualitative experiments show that, contrary to ordinary
              autoencoders, denoising autoencoders are able to learn Gabor-like
              edge detectors from natural image patches and larger stroke
              detectors from digit images. This work clearly establishes the
              value of using a denoising criterion as a tractable unsupervised
              objective to guide the learning of useful higher level
              representations.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  11,
  pages    = "3371--3408",
  month    =  dec,
  year     =  2010,
  url      = "http://dl.acm.org/citation.cfm?id=1756006.1953039",
  issn     = "1532-4435",
  keywords = {DL}
}

@ARTICLE{Bengio2008-th,
  title    = "Neural net language models",
  author   = "Bengio, Yoshua",
  journal  = "Scholarpedia J.",
  volume   =  3,
  issue    =  1,
  year     =  2008,
  url      = "http://dx.doi.org/10.4249/scholarpedia.3881",
  keywords = "NLP",
  issn     = "1941-6016",
  doi      = "10.4249/scholarpedia.3881",
  keywords = {DL}
}

@BOOK{Boureau_undated-bz,
  title    = "Sparse Feature Learning for Deep Belief Networks",
  author   = "Boureau, Y.-Lan and Lecun, Yann",
  abstract = "Unsupervised learning algorithms aim to discover the structure
              hidden in the data, and to learn representations that are more
              suitable as input to a supervised machine than the raw input.
              Many unsupervised methods are based on reconstructing the input
              from the representation, while constraining the representation to
              have certain desirable properties (e.g. low dimension, sparsity,
              etc). Others are based on approximating density by stochastically
              reconstructing the input from the representation. We describe a
              novel and efficient algorithm to learn sparse representations,
              and compare it theoretically and experimentally with a similar
              machine trained probabilistically, namely a Restricted Boltzmann
              Machine. We propose a simple criterion to compare and select
              different unsupervised machines based on the trade-off between
              the reconstruction error and the information content of the
              representation. We demonstrate this method by extracting features
              from a dataset of handwritten numerals, and from a dataset of
              natural image patches. We show that by stacking multiple levels
              of such machines and by training sequentially, high-order
              dependencies between the input observed variables can be
              captured. 1",
  keywords = {DL}
}

@INPROCEEDINGS{Lee2008-of,
  title     = "Sparse deep belief net model for visual area {V2}",
  author    = "Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew Y.",
  abstract  = "Motivated in part by the hierarchical organization of the
               cortex, a number of algorithms have recently been proposed that
               try to learn hierarchical, or “deep,” structure from unlabeled
               data. While several authors have formally or informally compared
               their algorithms to computations performed in visual area V1
               (and the cochlea), little attempt has been made thus far to
               evaluate these algorithms in terms of their fidelity for
               mimicking computations at deeper levels in the cortical
               hierarchy. This paper presents an unsupervised learning model
               that faithfully mimics certain properties of visual area V2.
               Specifically, we develop a sparse variant of the deep belief
               networks of Hinton et al. (2006). We learn two layers of nodes
               in the network, and demonstrate that the first layer, similar to
               prior work on sparse coding and ICA, results in localized,
               oriented, edge filters, similar to the Gabor functions known to
               model V1 cell receptive fields. Further, the second layer in our
               model encodes correlations of the first layer responses in the
               data. Specifically, it picks up both colinear (“contour”)
               features as well as corners and junctions. More interestingly,
               in a quantitative comparison, the encoding of these more complex
               “corner ” features matches well with the results from the Ito \&
               Komatsu’s study of biological V2 responses. This suggests that
               our sparse variant of deep belief networks holds promise for
               modeling more higher-order features. 1",
  publisher = "MIT Press",
  year      =  2008,
  keywords = {DL}
}

@BOOK{Schwenk_undated-oc,
  title    = "Continuous Space Language Models for Statistical Machine
              Translation",
  author   = "Schwenk, Holger and Dchelotte, Daniel and Gauvain, Jean-Luc",
  abstract = "Statistical machine translation systems are based on one or more
              translation models and a language model of the target language.
              While many different translation models and phrase extraction
              algorithms have been proposed, a standard word n-gram back-off
              language model is used in most systems. In this work, we propose
              to use a new statistical language model that is based on a
              continuous representation of the words in the vocabulary. A
              neural network is used to perform the projection and the
              probability estimation. We consider the translation of European
              Parliament Speeches. This task is part of an international
              evaluation organized by the TC-STAR project in 2006. The proposed
              method achieves consistent improvements in the BLEU score on the
              development and test data. We also present algorithms to improve
              the estimation of the language model probabilities when splitting
              long sentences into shorter chunks. 1",
  keywords = "NLP",
  keywords = {DL}
}

@ARTICLE{Rumelhart1986-jb,
  title   = "Learning representations by back-propagating errors",
  author  = "Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J",
  journal = "Nature",
  volume  =  323,
  issue   =  6088,
  pages   = "533--536",
  year    =  1986,
  issn    = "0028-0836",
  keywords = {DL}
}

@INPROCEEDINGS{Huang2012-pd,
  title    = "Improving word representations via global context and multiple
              word prototypes",
  author   = "Huang, Eric H. and Socher, Richard and Manning, Christopher D.
              and Ng, Andrew Y.",
  abstract = "Unsupervised word representations are very useful in NLP tasks
              both as inputs to learning algorithms and as extra word features
              in NLP systems. However, most of these models are built with only
              local context and one representation per word. This is
              problematic because words are often polysemous and global context
              can also provide useful information for learning word meanings.
              We present a new neural network architecture which 1) learns word
              embeddings that better capture the semantics of words by
              incorporating both local and global document context, and 2)
              accounts for homonymy and polysemy by learning multiple
              embeddings per word. We introduce a new dataset with human
              judgments on pairs of words in sentential context, and evaluate
              our model on it, showing that our model outperforms competitive
              baselines and other neural language models. 1 1",
  year     =  2012,
  keywords = "NLP",
  keywords = {DL}
}

@TECHREPORT{Erhan2010-er,
  title  = "Understanding representations learned in deep architectures",
  author = "Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua",
  year   =  2010,
  keywords = {DL}
}

@ARTICLE{Klobucar2013-na,
  title    = "Automated scoring in context: Rapid assessment for placed
              students",
  author   = "Klobucar, Andrew and Elliot, Norbert and Deess, Perry and Rudniy,
              Oleksandr and Joshi, Kamal",
  abstract = "This study investigated the use of automated essay scoring (AES)
              to identify at-risk students enrolled in a first-year university
              writing course. An application of AES, the Criterion® Online
              Writing Evaluation Service was evaluated through a methodology
              focusing on construct modelling, response processes,
              disaggregation, extrapolation, generalization, and consequence.
              Based on the results of our two-year study with students (N =
              1,482) at a public technological research university in the
              United States, we found that Criterion offered a defined writing
              construct congruent with established models, achieved acceptance
              among students and instructors, showed no statistically
              significant differences between ethnicity groups of sufficient
              sample size, correlated at acceptable levels with other writing
              measures, performed in a stable fashion, and enabled instructors
              to identify at-risk students to increase their course success.",
  journal  = "Assessing Writing",
  volume   =  18,
  issue    =  1,
  pages    = "62--84",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1016/j.asw.2012.10.001",
  keywords = "Automated essay scoring (AES); Validation methods; Writing
              assessment; Writing placement",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2012.10.001",
  keywords = {AWA}
}

@INPROCEEDINGS{Glorot2010-fk,
  title    = "Understanding the difficulty of training deep feedforward neural
              networks",
  author   = "Glorot, Xavier and Bengio, Yoshua",
  abstract = "Whereas before 2006 it appears that deep multilayer neural
              networks were not successfully trained, since then several
              algorithms have been shown to successfully train them, with
              experimental results showing the superiority of deeper vs less
              deep architectures. All these experimental results were obtained
              with new initialization or training mechanisms. Our objective
              here is to understand better why standard gradient descent from
              random initialization is doing so poorly with deep neural
              networks, to better understand these recent relative successes
              and help design better algorithms in the future. We first observe
              the influence of the non-linear activations functions. We find
              that the logistic sigmoid activation is unsuited for deep
              networks with random initialization because of its mean value,
              which can drive especially the top hidden layer into saturation.
              Surprisingly, we find that saturated units can move out of
              saturation by themselves, albeit slowly, and explaining the
              plateaus sometimes seen when training neural networks. We find
              that a new non-linearity that saturates less can often be
              beneficial. Finally, we study how activations and gradients vary
              across layers and during training, with the idea that training
              may be more difficult when the singular values of the Jacobian
              associated with each layer are far from 1. Based on these
              considerations, we propose a new initialization scheme that
              brings substantially faster convergence. 1 Deep Neural Networks
              Deep learning methods aim at learning feature hierarchies with
              features from higher levels of the hierarchy formed by the
              composition of lower level features. They include",
  year     =  2010,
  keywords = {DL}
}

%@ARTICLE{Pearlmutter1994-lc,
%  title    = "Fast Exact Multiplication by the Hessian",
%  author   = "Pearlmutter, Barak A.",
%  abstract = "Just storing the Hessian H (the matrix of second derivatives d^2
%              E/dw\_i dw\_j of the error E with respect to each pair of
%              weights) of a large neural network is difficult. Since a common
%              use of a large matrix like H is to compute its product with
%              various vectors, we derive a technique that directly calculates
%              Hv, where v is an arbitrary vector. This allows H to be treated
%              as a generalized sparse matrix. To calculate Hv, we first define
%              a differential operator Rf(w) = (d/dr)f(w + rv)|\_\{r=0\}, note
%              that R\{grad\_w\} = Hv and R\{w\} = v, and then apply R\{\} to
%              the equations used to compute grad\_w. The result is an exact and
%              numerically stable procedure for computing Hv, which takes about
%              as much computation, and is about as local, as a gradient
%              evaluation. We then apply the technique to backpropagation
%              networks, recurrent backpropagation, and stochastic Boltzmann
%              Machines. Finally, we show that this technique can be used at the
%              heart of many iterative techniques for computing various
%              properties of H, obviating the need for direct methods.",
%  journal  = "Neural Comput.",
%  volume   =  6,
%  pages    = "147--160",
%  year     =  1994,
%  issn     = "0899-7667"
%}

@ARTICLE{Bottou2011-nh,
  title   = "From Machine Learning to Machine Reasoning",
  author  = "Bottou, L\'{e}on",
  journal = "CoRR",
  volume  = "abs/1102.1808",
  year    =  2011,
  url     = "http://arxiv.org/abs/1102.1808",
  keywords = {DL}
}

@BOOK{Salakhutdinov2010-qq,
  title    = "An Efficient Learning Procedure for Deep Boltzmann Machines",
  author   = "Salakhutdinov, Ruslan and Hinton, Geoffrey",
  abstract = "We present a new learning algorithm for Boltzmann Machines that
              contain many layers of hidden variables. Data-dependent
              statistics are estimated using a variational approximation that
              tends to focus on a single mode, and data-independent statistics
              are estimated using persistent Markov chains. The use of two
              quite different techniques for estimating the two types of
              statistic that enter into the gradient of the log likelihood
              makes it practical to learn Boltzmann Machines with multiple
              hidden layers and millions of parameters. The learning can be
              made more efficient by using a layerby-layer “pre-training” phase
              that initializes the weights sensibly. The pre-training also
              allows the variational inference to be initialized sensibly with
              a single bottom-up pass. We present results on the MNIST and NORB
              datasets showing that Deep Boltzmann Machines learn very good
              generative models of hand-written digits and 3-D objects. We also
              show that the features discovered by Deep Boltzmann Machines are
              a very effective way to initialize the hidden layers of
              feed-forward neural nets which are then discriminatively
              fine-tuned.",
  year     =  2010,
  keywords = {DL}
}

@INPROCEEDINGS{Foltz1999-wj,
  title  = "Automated essay scoring: Applications to educational technology",
  author = "Foltz, Peter W and Laham, Darrell and Landauer, Thomas K",
  volume =  1999,
  pages  = "939--944",
  year   =  1999,
  keywords = {AWA}
}

@INPROCEEDINGS{Jurgens2012-ig,
  title     = "{SemEval-2012} task 2: measuring degrees of relational
               similarity",
  author    = "Jurgens, David A. and Turney, Peter D. and Mohammad, Saif M. and
               Holyoak, Keith J.",
  abstract  = "Up to now, work on semantic relations has focused on relation
               classification: recognizing whether a given instance (a word
               pair such as virus:flu) belongs to a specific relation class
               (such as CAUSE:EFFECT). However, instances of a single relation
               class may still have significant variability in how
               characteristic they are of that class. We present a new SemEval
               task based on identifying the degree of prototypicality for
               instances within a given class. As a part of the task, we have
               assembled the first dataset of graded relational similarity
               ratings across 79 relation categories. Three teams submitted six
               systems, which were evaluated using two methods.",
  publisher = "Association for Computational Linguistics",
  pages     = "356--364",
  series    = "SemEval '12",
  year      =  2012,
  url       = "http://dl.acm.org/citation.cfm?id=2387636.2387693",
  keywords  = "NLP",
  keywords = {DL}
}

@ARTICLE{Wang2007-yi,
  title   = "Automated essay scoring versus human scoring: A comparative study",
  author  = "Wang, Jinhao and Brown, Michelle Stallone",
  journal = "The Journal of Technology, Learning and Assessment",
  volume  =  6,
  issue   =  2,
  year    =  2007,
  keywords = {AWA}
}

@BOOK{Nair_undated-mg,
  title    = "Rectified Linear Units Improve Restricted Boltzmann Machines",
  author   = "Nair, Vinod and Hinton, Geoffrey E.",
  abstract = "Restricted Boltzmann machines were developed using binary
              stochastic hidden units. These can be generalized by replacing
              each binary unit by an infinite number of copies that all have
              the same weights but have progressively more negative biases. The
              learning and inference rules for these “Stepped Sigmoid Units ”
              are unchanged. They can be approximated efficiently by noisy,
              rectified linear units. Compared with binary units, these units
              learn features that are better for object recognition on the NORB
              dataset and face verification on the Labeled Faces in the Wild
              dataset. Unlike binary units, rectified linear units preserve
              information about relative intensities as information travels
              through multiple layers of feature detectors. 1.",
  keywords = {DL}
}

@INCOLLECTION{Bengio2011-gs,
  title     = "On the Expressive Power of Deep Architectures",
  booktitle = "Discovery Science",
  author    = "Bengio, Yoshua and Delalleau, Olivier",
  editor    = "Elomaa, Tapio and Hollm\'{e}n, Jaakko and Mannila, Heikki",
  abstract  = "Deep architectures are families of functions corresponding to
               deep circuits. Deep Learning algorithms are based on
               parametrizing such circuits and tuning their parameters so as to
               approximately optimize some training objective. Whereas it was
               thought too difficult to train deep architectures, several
               successful algorithms have been proposed in recent years. We
               review some of the theoretical motivations for deep
               architectures, as well as some of their practical successes, and
               propose directions of investigations to address some of the
               remaining challenges.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "1--1",
  series    = "Lecture Notes in Computer Science",
  month     =  jan,
  year      =  2011,
  url       = "http://link.springer.com/chapter/10.1007/978-3-642-24477-3_1",
  keywords  = "Algorithm Analysis and Problem Complexity; Artificial
               Intelligence (incl. Robotics); Data Mining and Knowledge
               Discovery; Database Management; Information Storage and
               Retrieval; Information Systems Applications (incl. Internet)",
  keywords = {DL}
}

@INPROCEEDINGS{Mikolov2011-yg,
  title     = "Strategies for training large scale neural network language
               models",
  booktitle = "2011 {IEEE} Workshop on Automatic Speech Recognition and
               Understanding ({ASRU})",
  author    = "Mikolov, T and Deoras, A and Povey, D and Burget, L and
               Cernocky, J",
  abstract  = "We describe how to effectively train neural network based
               language models on large data sets. Fast convergence during
               training and better overall performance is observed when the
               training data are sorted by their relevance. We introduce
               hash-based implementation of a maximum entropy model, that can
               be trained as a part of the neural network model. This leads to
               significant reduction of computational complexity. We achieved
               around 10\% relative reduction of word error rate on English
               Broadcast News speech recognition task, against large 4-gram
               model trained on 400M tokens.",
  pages     = "196--201",
  year      =  2011,
  url       = "http://dx.doi.org/10.1109/ASRU.2011.6163930",
  keywords  = "NLP",
  doi       = "10.1109/ASRU.2011.6163930",
  keywords = {DL}
}

@article{Hinton2006-mm,
    title={Reducing the dimensionality of data with neural networks},
    author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
    journal={Science},
    volume={313},
    number={5786},
    pages={504--507},
    year={2006},
    publisher={American Association for the Advancement of Science}
}

@ARTICLE{Byrne2010-gq,
  title    = "eGrader, a software application that automatically scores student
              essays: with a postscript on ethical complexities",
  author   = "Byrne, Roxanne and Tang, Michael and Tranduc, John and Tang,
              Matthew",
  abstract = "Online and traditional teachers face severalinstructional
              challenges with regard toassessing student learning. This
              paperfocuses on a software application thatautomatically scores
              student essay. The firstpart gives a brief overview of
              threecommercial automated essay scoringsystems. Then it describes
              the technicalaspects of the machine grader developed bythe
              authors, including an assessment of itsperformance. Although the
              statistical resultswere significant in finding a
              strongcorrelation between human and machinescorers and the other
              measures, follow-upnon-quantitative evaluations led
              theresearchers to discontinue using the eGrader.They concluded
              that while the eGrader’sability to measure objective
              evaluationcriteria was successful, measuring subjectiveideas
              proved to more complex andproblematic.",
  journal  = "Systemics, Cybernetics, and Informatics",
  volume   =  8,
  issue    =  6,
  year     =  2010,
  issn     = "1690-4524",
  keywords = {AWA}
}

@ARTICLE{Vincent2011-no,
  title    = "A connection between score matching and denoising autoencoders",
  author   = "Vincent, Pascal",
  abstract = "Denoising autoencoders have been previously shown to be
              competitive alternatives to restricted Boltzmann machines for
              unsupervised pretraining of each layer of a deep architecture. We
              show that a simple denoising autoencoder training criterion is
              equivalent to matching the score (with respect to the data) of a
              specific energy-based model to that of a nonparametric Parzen
              density estimator of the data. This yields several useful
              insights. It defines a proper probabilistic model for the
              denoising autoencoder technique, which makes it in principle
              possible to sample from them or rank examples by their energy. It
              suggests a different way to apply score matching that is related
              to learning to denoise and does not require computing second
              derivatives. It justifies the use of tied weights between the
              encoder and decoder and suggests ways to extend the success of
              denoising autoencoders to a larger family of energy-based models.",
  journal  = "Neural Comput.",
  volume   =  23,
  issue    =  7,
  pages    = "1661--1674",
  month    =  jul,
  year     =  2011,
  url      = "http://dx.doi.org/10.1162/NECO_a_00142",
  issn     = "0899-7667",
  doi      = "10.1162/NECO\_a\_00142",
  keywords = {DL}
}

%@TECHREPORT{HInton_undated-zi,
%  title  = "The development of the time-delay neural network architecture for
%            speech recognition",
%  author = "HInton, GE"
%}

@ARTICLE{Mesnil2012-ke,
  title   = "Unsupervised and Transfer Learning Challenge: a Deep Learning
             Approach",
  author  = "Mesnil, Gr\'{e}goire and Dauphin, Yann and Glorot, Xavier and
             Rifai, Salah and Bengio, Yoshua and Goodfellow, Ian J. and Lavoie,
             Erick and Muller, Xavier and Desjardins, Guillaume and
             Warde-Farley, David and Vincent, Pascal and Courville, Aaron C.
             and Bergstra, James",
  journal = "Journal of Machine Learning Research - Proceedings Track",
  pages   = "97--110",
  year    =  2012,
  keywords = {DL}
}

@BOOK{Hinton2010-rn,
  title    = "Discovering Binary Codes for Documents by Learning Deep
              Generative Models",
  author   = "Hinton, Geoffrey and Salakhutdinov, Ruslan",
  abstract = "We describe a deep generative model in which the lowest layer
              represents the word-count vector of a document and the top layer
              represents a learned binary code for that document. The top two
              layers of the generative model form an undirected associative
              memory and the remaining layers form a belief net with directed,
              top-down connections. We present efficient learning and inference
              procedures for this type of generative model and show that it
              allows more accurate and much faster retrieval than latent
              semantic analysis. By using our method as a filter for a much
              slower method called TF-IDF we achieve higher accuracy than
              TF-IDF alone and save several orders of magnitude in retrieval
              time. By using short binary codes as addresses, we can perform
              retrieval on very large document sets in a time that is
              independent of the size of the document set using only one word
              of memory to describe each document.",
  year     =  2010,
  keywords = "NLP",
  keywords = {DL}
}

@ARTICLE{Pollack1990-uy,
  title    = "Recursive Distributed Representations",
  author   = "Pollack, Jordan B.",
  abstract = "A long-standing difficulty for connectionist modeling has been
              how to represent variable-sized recursive data structures, such
              as trees and lists, in fixed-width patterns. This paper presents
              a connectionist architecture which automatically develops compact
              distributed representations for such compositional structures, as
              well as efficient accessing mechanisms for them. Patterns which
              stand for the internal nodes of fixed-valence trees are devised
              through the recursive use of back-propagation on three-layer
              autoassociative encoder networks. The resulting representations
              are novel, in that they combine apparently immiscible aspects of
              features, pointers, and symbol structures. They form a bridge
              between the data structures necessary for high-level cognitive
              tasks and the associative, pattern recognition machinery provided
              by neural networks. 2 J. B. Pollack 1. Introduction One of the
              major stumbling blocks in the application of Connectionism to
              higherlevel cognitive tasks, such as Na...",
  journal  = "Artif. Intell.",
  volume   =  46,
  pages    = "77--105",
  year     =  1990,
  keywords = "NLP",
  issn     = "0004-3702",
  keywords = {DL}
}

@INPROCEEDINGS{Burstein2000-al,
  title  = "Benefits of modularity in an automated essay scoring system",
  author = "Burstein, Jill and Marcu, Daniel",
  pages  = "44--50",
  year   =  2000,
  keywords = {AWA}
}

@MISC{ASAPSAS,
  title        = "Description - The Hewlett Foundation: Short Answer Scoring |
                  Kaggle",
  abstract     = "Kaggle is a platform for data prediction competitions.
                  Companies, organizations and researchers post their data and
                  have it scrutinized by the world's best statisticians.",
  url          = "http://www.kaggle.com/c/asap-sas",
  howpublished = "\url{http://www.kaggle.com/c/asap-sas}",
  note         = "Accessed: 2013-10-7",
  keywords = {AWA}
}

@INPROCEEDINGS{Hstad1989-ua,
  title     = "Almost Optimal Lower Bounds for Small Depth Circuits",
  author    = "H\aa{}stad, Johan",
  abstract  = "We give improved lower bounds for the size of small depth
               circuits computing several functions. In particular we prove
               almost optimal lower bounds for the size of parity circuits.
               Fur-ther we show that there are functions computable in
               polynomial size and depth k but requires ex-ponential size when
               the depth is restricted to k-1. Our main lemma which is of
               independent interest states that by using a random restriction
               we can convert an AND of small ORs to an OR of small ANDs and
               conversely.",
  publisher = "JAI Press",
  pages     = "6--20",
  year      =  1989,
  keywords = {DL}
}

%@INPROCEEDINGS{A2000-xj,
%  title     = "Anisotropic Noise Injection for Input Variables Relevance
%               Determination",
%  author    = "A, Tnn and Grandvalet, Yves",
%  abstract  = "There are two archetypal ways to control the complexity of a
%               exible regressor: subset selection and ridge regression. In
%               neural networks jargon, they are respectively known as pruning
%               and weight decay. These techniques may also be adapted to
%               estimate which features of the input space are relevant for
%               predicting the output variables. Relevance is given by a binary
%               indicator for subset selection, and by a continuous rating for
%               ridge regression.",
%  publisher = "Springer",
%  pages     = "463--468",
%  year      =  2000,
%}

@INPROCEEDINGS{Sutskever2013-dz,
  title     = "On the importance of initialization and momentum in deep
               learning",
  booktitle = "Proceedings of The 30th International Conference on Machine
               Learning",
  author    = "Sutskever, Ilya and Martens, James and Dahl, George and Hinton,
               Geoffrey",
  pages     = "1139--1147",
  year      =  2013,
  url       = "http://jmlr.org/proceedings/papers/v28/sutskever13.html",
  keywords = {DL}
}

@INPROCEEDINGS{Elman1991-es,
  title    = "Distributed representations, simple recurrent networks, and
              grammatical structure",
  author   = "Elman, Jeffrey L.",
  abstract = "Abstract. In this paper three problems for a connectionist
              account of language are considered: 1. What is the nature of
              linguistic representations? 2. How can complex structural
              relationships such as constituent structure be represented? 3.
              How can the apparently open-ended nature of language be
              accommodated by a fixed-resource system? Using a prediction task,
              a simple recurrent network (SRN) is trained on multiclausal
              sentences which contain multiply-embedded relative clauses.
              Principal component analysis of the hidden unit activation
              patterns reveals that the network solves the task by developing
              complex distributed representations which encode the relevant
              grammatical relations and hierarchical constituent structure.
              Differences between the SRN state representations and the more
              traditional pushdown store are discussed in the final section.",
  pages    = "195--225",
  year     =  1991,
  keywords = "NLP",
  keywords = {DL}
}

@ARTICLE{Socher2013-tx,
  title    = "Parsing with compositional vector grammars",
  author   = "Socher, R and Bauer, J and Manning, CD and Ng, AY",
  journal  = "Proceedings of the ACL conference (to",
  year     =  2013,
  keywords = "NLP",
  keywords = {DL}
}

@MISC{Elliott_undated-kb,
  title        = "Computer-graded essays full of flaws",
  author       = "Elliott, Scott",
  abstract     = "The computer grader gave a top score to my banana-eating
                  purple imaginary friend, even if the description made no
                  sense in a middle school-level essay.< But according to the
                  computer, the gibberish-filled essay displayed better writing
                  than an on-topic",
  url          = "http://mo.daytondailynews.com/project/content/project/tests/0524testautoscore.html",
  howpublished = "\url{http://mo.daytondailynews.com/project/content/project/tests/0524testautoscore.html}",
  note         = "Accessed: 2013-10-8",
  keywords = {AWA}
}

%@INCOLLECTION{Srinivasan2010-lc,
%  title     = "Parameter Screening and Optimisation for {ILP} Using Designed
%               Experiments",
%  booktitle = "Inductive Logic Programming",
%  author    = "Srinivasan, Ashwin and Ramakrishnan, Ganesh",
%  editor    = "Raedt, Luc De",
%  abstract  = "Reports of experiments conducted with an Inductive Logic
%               Programming system rarely describe how specific values of
%               parameters of the system are arrived at when constructing
%               models. Usually, no attempt is made to identify sensitive
%               parameters, and those that are used are often given
%               “factory-supplied” default values, or values obtained from some
%               non-systematic exploratory analysis. The immediate consequence
%               of this is, of course, that it is not clear if better models
%               could have been obtained if some form of parameter selection and
%               optimisation had been performed. Questions follow inevitably on
%               the experiments themselves: specifically, are all algorithms
%               being treated fairly, and is the exploratory phase sufficiently
%               well-defined to allow the experiments to be replicated? In this
%               paper, we investigate the use of parameter selection and
%               optimisation techniques grouped under the study of experimental
%               design. Screening and “response surface” methods determine, in
%               turn, sensitive parameters and good values for these parameters.
%               This combined use of parameter selection and response
%               surface-driven optimisation has a long history of application in
%               industrial engineering, and its role in ILP is investigated
%               using two well-known benchmarks. The results suggest that
%               computational overheads from this preliminary phase are not
%               substantial, and that much can be gained, both on improving
%               system performance and on enabling controlled experimentation,
%               by adopting well-established procedures such as the ones
%               proposed here.",
%  publisher = "Springer Berlin Heidelberg",
%  pages     = "217--225",
%  series    = "Lecture Notes in Computer Science",
%  month     =  jan,
%  year      =  2010,
%  url       = "http://link.springer.com/chapter/10.1007/978-3-642-13840-9_21",
%  keywords  = "Algorithm Analysis and Problem Complexity; Data Mining and
%               Knowledge Discovery; Database Management; Information Storage
%               and Retrieval; Logics and Meanings of Programs; Mathematical
%               Logic and Formal Languages",
%}

@ARTICLE{James2008-vb,
  title    = "Electronic scoring of essays: Does topic matter?",
  author   = "James, Cindy L.",
  abstract = "The scoring of student essays by computer has generated much
              debate and subsequent research. The majority of the research thus
              far has focused on validating the automated scoring tools by
              comparing the electronic scores to human scores of writing or
              other measures of writing skills, and exploring the predictive
              validity of the automated scores. However, very little research
              has investigated possible effects of the essay prompts. This
              study endeavoured to do so by exploring test scores for three
              different prompts for the ACCUPLACER® WritePlacer® Plus test
              which is scored by the IntelliMetric® automated scoring system.
              The results indicated that there was no significant difference
              among the prompts overall; among males, between males and
              females, by native language or in comparison to scores generated
              by human raters. However, there was a significant difference in
              mean scores by topic for females.",
  journal  = "Assessing Writing",
  volume   =  13,
  issue    =  2,
  pages    = "80--92",
  year     =  2008,
  url      = "http://dx.doi.org/10.1016/j.asw.2008.05.001",
  keywords = "ACCUPLACER®; Electronic scoring; Essays; Topic effect; Writing
              assessment",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2008.05.001",
  keywords = {AWA}
}

%@ARTICLE{Hua2006-pp,
%  title    = "Noise-injected neural networks show promise for use on
%              small-sample expression data",
%  author   = "Hua, Jianping and Lowey, James and Xiong, Zixiang and Dougherty,
%              Edward R.",
%  abstract = "Overfitting the data is a salient issue for classifier design in
%              small-sample settings. This is why selecting a classifier from a
%              constrained family of classifiers, ones that do not possess the
%              potential to too finely partition the feature space, is typically
%              preferable. But overfitting is not merely a consequence of the
%              classifier family; it is highly dependent on the classification
%              rule used to design a classifier from the sample data. Thus, it
%              is possible to consider families that are rather complex but for
%              which there are classification rules that perform well for small
%              samples. Such classification rules can be advantageous because
%              they facilitate satisfactory classification when the
%              class-conditional distributions are not easily separated and the
%              sample is not large. Here we consider neural networks, from the
%              perspectives of classical design based solely on the sample data
%              and from noise-injection-based design.",
%  journal  = "BMC Bioinformatics",
%  volume   =  7,
%  issue    =  1,
%  month    =  may,
%  year     =  2006,
%  url      = "http://dx.doi.org/10.1186/1471-2105-7-274",
%  issn     = "1471-2105",
%  doi      = "10.1186/1471-2105-7-274"
%}

@article{Hinton2006-rt,
    title={A fast learning algorithm for deep belief nets},
    author={Hinton, Geoffrey and Osindero, Simon and Teh, Yee-Whye},
    journal={Neural computation},
    volume={18},
    number={7},
    pages={1527--1554},
    year={2006},
    publisher={MIT Press}
}

@INPROCEEDINGS{Bengio2007-ni,
  title     = "Topmoumoute Online Natural Gradient Algorithm",
  booktitle = "Advances in Neural Information Processing Systems 20",
  author    = "Bengio, Yoshua",
  year      =  2007,
  url       = "http://nips.cc/Conferences/2007/Program/event.php?ID=869",
  keywords = {DL}
}

%@ARTICLE{Tchistiakov2000-sm,
%  title    = "Neural network modelling for very small spectral data sets:
%              reduction of the spectra and hierarchical approach",
%  author   = "Tchistiakov, Valeri and Ruckebusch, Cyril and Duponchel, Ludovic
%              and Huvenne, Jean-Pierre and Legrand, Pierre",
%  abstract = "For studies on industrial materials, scarcity of samples and
%              incomplete information are everyday situations. Furthermore, the
%              number of points per sample typically reaches several hundreds.
%              Consequently, the sample-to-data ratio does not satisfy the
%              requirements of most of the mathematical treatments. We thus
%              discuss the use of different approaches in order to reduce the
%              number of parameters of the networks in case of data sets with
%              extremely small number of samples. Therefore, more or less new
%              approaches using wavelet or Fourier-transform coefficients for
%              the reduction of spectra have been offered for a few years.
%              Moreover, the necessity emerges to associate these various
%              pre-processing methods with the construction of input–output
%              relationships models. Combinations of different artificial neural
%              networks (ANNs) for non-linear hierarchical modelling are thus
%              examined.In practice, we apply these methods to infrared spectra
%              in three different situations:• qualitative analysis of complex
%              mixtures (identification)•semi-quantitative analysis of a major
%              compound•quantitative and precise analysis of minor
%              compounds.This study demonstrates that, when real data are
%              investigated, a combination of compression methods and multilevel
%              modelling offers accuracy advantages compared with more classical
%              architecture networks.",
%  journal  = "Chemometrics Intellig. Lab. Syst.",
%  volume   =  54,
%  issue    =  2,
%  pages    = "93--106",
%  month    =  dec,
%  year     =  2000,
%  url      = "http://dx.doi.org/10.1016/S0169-7439(00)00108-8",
%  keywords = "Artificial neural networks; FTIR; Infrared spectroscopy;
%              Multilevel modelling; Transformations; Wavelet",
%  issn     = "0169-7439",
%  doi      = "10.1016/S0169-7439(00)00108-8"
%}

@ARTICLE{Erhan2010-mu,
  title   = "Why does unsupervised pre-training help deep learning?",
  author  = "Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and
             Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy",
  journal = "Journal of Machine Learning Research",
  volume  =  11,
  pages   = "625--660",
  year    =  2010,
  issn    = "1532-4435",
  keywords = {DL}
}

@INPROCEEDINGS{Wild2005-gv,
  title    = "Parameters Driving Effectiveness of Automated Essay Scoring with
              {LSA}",
  author   = "Wild, Fridolin and Stahl, Christina and Stermsek, Gerald and
              Neumann, Gustaf",
  abstract = "Automated essay scoring with latent semantic analysis (LSA) has
              recently been subject to increasing interest. Although previous
              authors have achieved grade ranges similar to those awarded by
              humans, it is still not clear which and how parameters improve or
              decrease the effectiveness of LSA. This paper presents an
              analysis of the effects of these parameters, such as text
              preprocessing, weighting, singular value dimensionality and type
              of similarity measure, and benchmarks this effectiveness by
              comparing machine-assigned with human-assigned scores in a
              real-world case. We show that each of the identified factors
              significantly influences the quality of automated essay scoring
              and that the factors are not independent of each other.",
  pages    = "485--494",
  month    =  jul,
  year     =  2005,
  url      = "http://nm.wu-wien.ac.at/research/publications/b497.pdf",
  keywords = "prolearn",
  keywords = {AWA}
}

%@ARTICLE{Richardson2006-hy,
%  title   = "Markov logic networks",
%  author  = "Richardson, Matthew and Domingos, Pedro",
%  journal = "Mach. Learn.",
%  volume  =  62,
%  issue   = "1-2",
%  pages   = "107--136",
%  year    =  2006,
%  issn    = "0885-6125"
%}

@ARTICLE{Quinlan2009-ba,
  title    = "Evaluating the {Construct-Coverage} of the e-rater[R] Scoring
              Engine. Research Report. {ETS} {RR-09-01}",
  author   = "Quinlan, Thomas and Higgins, Derrick and Wolff, Susanne",
  abstract = "This report evaluates the construct coverage of the e-rater[R[
              scoring engine. The matter of construct coverage depends on
              whether one defines writing skill, in terms of process or
              product. Originally, the e-rater engine consisted of a large set
              of components with a proven ability to predict human holistic
              scores. By organizing these capabilities into features, e-rater
              researchers organized the e-rater engine along the lines of trait
              scoring, which recognizes that essay quality has several
              dimensions. Some traits of essay quality cut across different
              methods for scoring essay quality, such as the rubrics employed
              by the GRE[R] and TOEFL[R] assessments, as well as the 6-trait
              scoring model. Factor analyses conducted by Attali and Powers
              (2008) suggest that e-rater features capture low-level aspects of
              essay quality, such as sentence complexity, vocabulary, and
              conventions. Future e-rater development should focus on (a)
              deepening and expanding coverage of the construct, such as by
              developing measures of essay content and organization, as well as
              on (b) addressing accuracy issues in existing features. (Two
              appendices are included: (1) Scoring Guides for the Graduate
              Record Examinations[R] (GRE[R]), Test of English as a Foreign
              Language[TM] (TOEFL[R]), SAT[R]; and National Assessment of
              Educational Progress (NAEP); and (2) Glossary of e-rater
              Microfeatures. Contains 2 figures and 1 tables.)",
  journal  = "Educational Testing Service",
  month    =  jan,
  year     =  2009,
  url      = "http://eric.ed.gov/?id=ED505571",
  keywords = "Automation; Computer Assisted Testing; Construct Validity;
              Educational Assessment; Educational Testing; Essay Tests; Essays;
              Evaluation Research; Factor Analysis; Guides; Measurement;
              Measurement Techniques; Scoring; Standardized Tests; Writing
              Skills; Writing Tests",
  keywords = {AWA}
}

@INPROCEEDINGS{Bottou2008-hn,
  title    = "The tradeoffs of large scale learning",
  author   = "Bottou, L\'{e}on and Bousquet, Olivier",
  abstract = "This contribution develops a theoretical framework that takes
              into account the effect of approximate optimization on learning
              algorithms. The analysis shows distinct tradeoffs for the case of
              small-scale and large-scale learning problems. Small-scale
              learning problems are subject to the usual
              approximation–estimation tradeoff. Large-scale learning problems
              are subject to a qualitatively different tradeoff involving the
              computational complexity of the underlying optimization
              algorithms in non-trivial ways.",
  pages    = "161--168",
  year     =  2008,
  keywords = {DL}
}

@ARTICLE{Bengio2013-gx,
  title   = "Generalized Denoising {Auto-Encoders} as Generative Models",
  author  = "Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent,
             Pascal",
  journal = "CoRR",
  volume  = "abs/1305.6663",
  year    =  2013,
  url     = "http://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1305-6663",
  keywords = {DL}
}

@ARTICLE{Burstein2001-ji,
  title     = "Automated evaluation of essays and short answers",
  author    = "Burstein, Jill and Leacock, Claudia and Swartz, Richard",
  abstract  = "Essay questions designed to measure writing ability, along with
               open-ended questions requiring short answers, are highly-valued
               components of effective assessment programs, but the expense and
               logistics of scoring them reliably often present a barrier to
               their use. Extensive research and development efforts at
               Educational Testing Service (ETS) over the past several years
               (see http://www.ets.org/research/erater.html) in natural
               language processing have produced two applications with the
               potential to dramatically reduce the difficulties associated
               with scoring these types of assessments.",
  publisher = "© Loughborough University",
  year      =  2001,
  url       = "http://dx.doi.org/https://dspace.lboro.ac.uk/2134/1790",
  doi       = "https://dspace.lboro.ac.uk/2134/1790",
  keywords = {AWA}
}

@INCOLLECTION{Perleman_undated-bd,
  title     = "Construct validity, length, score, and time in holistically
               graded writing assessments: The case against automated essay
               scoring ({AES})",
  booktitle = "International advances in writing research: Cultures, places,
               measures",
  author    = "Perleman, Les",
  publisher = "Parlor Press",
  pages     = "121--132",
  keywords = {AWA}
}

@ARTICLE{LeCun1989-eq,
  title     = "Generalization and network design strategies",
  author    = "LeCun, Y",
  abstract  = "Abstract An interestmg property of connectiomst systems is their
               ability to learn from examples. Although most recent work in the
               field concentrates on reducing learning times, the most
               important feature of a learning machine is its generalization
               performance. It is ...",
  journal   = "Connections in Perspective. North-Holland,",
  publisher = "masters.donntu.edu.ua",
  year      =  1989,
  keywords = {DL}
}

%@ARTICLE{Bordes2009-fg,
%  title   = "{SGD-QN}: Careful {Quasi-Newton} Stochastic Gradient Descent",
%  author  = "Bordes, A and Bottou, L and Gallinari, P",
%  journal = "J. Mach. Learn. Res.",
%  volume  =  10,
%  pages   = "1737--1754",
%  year    =  2009,
%  issn    = "1532-4435"
%}

@BOOK{Martens_undated-ch,
  title    = "Deep learning via Hessian-free optimization",
  author   = "Martens, James",
  abstract = "We develop a 2nd-order optimization method based on the
              “Hessian-free” approach, and apply it to training deep
              auto-encoders. Without using pre-training, we obtain results
              superior to those reported by Hinton \& Salakhutdinov (2006) on
              the same tasks they considered. Our method is practical, easy to
              use, scales nicely to very large datasets, and isn’t limited in
              applicability to autoencoders, or any specific model class. We
              also discuss the issue of “pathological curvature ” as a possible
              explanation for the difficulty of deeplearning and how 2 nd-order
              optimization, and our method in particular, effectively deals
              with it. 1.",
  keywords = {DL}
}

%@ARTICLE{Le_Roux2012-wa,
%  title    = "A Stochastic Gradient Method with an Exponential Convergence Rate
%              for Finite Training Sets",
%  author   = "Le Roux, Nicolas and Schmidt, Mark and Bach, Francis",
%  abstract = "We propose a new stochastic gradient method for optimizing the
%              sum of a finite set of smooth functions, where the sum is
%              strongly convex. While standard stochastic gradient methods
%              converge at sublinear rates for this problem, the proposed method
%              incorporates a memory of previous gradient values in order to
%              achieve a linear convergence rate. In a machine learning context,
%              numerical experiments indicate that the new algorithm can
%              dramatically outperform standard algorithms, both in terms of
%              optimizing the training error and reducing the test error
%              quickly.",
%  journal  = "ArXiv e-prints",
%  volume   =  1202,
%  month    =  feb,
%  year     =  2012,
%  url      = "http://adsabs.harvard.edu/abs/2012arXiv1202.6258L",
%  keywords = "Computer Science - Learning; Mathematics - Optimization and
%              Control"
%}

@ARTICLE{Breuleux2011-xe,
  title    = "Quickly generating representative samples from an rbm-derived
              process",
  author   = "Breuleux, Olivier and Bengio, Yoshua and Vincent, Pascal",
  abstract = "Two recently proposed learning algorithms, herding and fast
              persistent contrastive divergence (FPCD), share the following
              interesting characteristic: they exploit changes in the model
              parameters while sampling in order to escape modes and mix better
              during the sampling process that is part of the learning
              algorithm. We justify such approaches as ways to escape modes
              while keeping approximately the same asymptotic distribution of
              the Markov chain. In that spirit, we extend FPCD using an idea
              borrowed from Herding in order to obtain a pure sampling
              algorithm, which we call the rates-FPCD sampler. Interestingly,
              this sampler can improve the model as we collect more samples,
              since it optimizes a lower bound on the log likelihood of the
              training data. We provide empirical evidence that this new
              algorithm displays substantially better and more robust mixing
              than Gibbs sampling.",
  journal  = "Neural Comput.",
  volume   =  23,
  issue    =  8,
  pages    = "2058--2073",
  month    =  aug,
  year     =  2011,
  url      = "http://dx.doi.org/10.1162/NECO_a_00158",
  issn     = "0899-7667",
  doi      = "10.1162/NECO\_a\_00158",
  keywords = {DL}
}

@ARTICLE{Wiskott2002-yh,
  title    = "Slow feature analysis: unsupervised learning of invariances",
  author   = "Wiskott, Laurenz and Sejnowski, Terrence J",
  abstract = "Invariant features of temporally varying signals are useful for
              analysis and classification. Slow feature analysis (SFA) is a new
              method for learning invariant or slowly varying features from a
              vectorial input signal. It is based on a nonlinear expansion of
              the input signal and application of principal component analysis
              to this expanded signal and its time derivative. It is guaranteed
              to find the optimal solution within a family of functions
              directly and can learn to extract a large number of decorrelated
              features, which are ordered by their degree of invariance. SFA
              can be applied hierarchically to process high-dimensional input
              signals and extract complex features. SFA is applied first to
              complex cell tuning properties based on simple cell output,
              including disparity and motion. Then more complicated
              input-output functions are learned by repeated application of
              SFA. Finally, a hierarchical network of SFA modules is presented
              as a simple model of the visual system. The same unstructured
              network can learn translation, size, rotation, contrast, or, to a
              lesser degree, illumination invariance for one-dimensional
              objects, depending on only the training stimulus. Surprisingly,
              only a few training objects suffice to achieve good
              generalization to new objects. The generated representation is
              suitable for object recognition. Performance degrades if the
              network is trained to learn multiple invariances simultaneously.",
  journal  = "Neural Comput.",
  volume   =  14,
  issue    =  4,
  pages    = "715--770",
  month    =  apr,
  year     =  2002,
  url      = "http://dx.doi.org/10.1162/089976602317318938",
  keywords = "Algorithms; Artificial Intelligence; Computer Simulation; Neural
              Networks (Computer); Nonlinear Dynamics; Pattern Recognition,
              Automated; Photoreceptor Cells; Visual Perception",
  issn     = "0899-7667",
  pmid     = "11936959",
  doi      = "10.1162/089976602317318938",
  keywords = {DL}
}

@ARTICLE{Frank1992-mn,
  title    = "Writing to Be Read: Young Writers' Ability to Demonstrate
              Audience Awareness When Evaluated by Their Readers",
  author   = "Frank, Laura A.",
  abstract = "This study explores the audience awareness of 30 fifth-grade
              students when they compose and revise an original text for two
              audiences--a good third-grade reader and an experienced adult
              reader--in a realistic transactional writing task. The writing
              task was designed to reflect the natural writer-audience
              relationship by allowing the two audiences addressed by the
              writers (rather than an independent rater or teacher) to evaluate
              the writers success in revising their compositions to meet the
              audiences' needs and expectations. Third-grade and adult readers
              attempted to identify the audience addressed in two revisions
              produced by each writer (60 total texts). A test of significance
              for proportional differences revealed that the fifth graders
              successfully revised their texts to address the expectations of
              both audiences. Results also suggest that the fifth graders were
              more successful addressing the third-grade than the adult
              audience. Samples of the students' revisions indicate how these
              young writers perceived and attempted to address the expectations
              of their readers.",
  journal  = "Research in the Teaching of English",
  volume   =  26,
  issue    =  3,
  pages    = "277--298",
  month    =  oct,
  year     =  1992,
  url      = "http://dx.doi.org/10.2307/40171309",
  issn     = "0034-527X",
  doi      = "10.2307/40171309",
  keywords = {AWA}
}

@ARTICLE{Ciresan2010-js,
  title   = "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition",
  author  = "Ciresan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria
             and Schmidhuber, J{\"{u}}rgen",
  journal = "CoRR",
  volume  = "abs/1003.0358",
  year    =  2010,
  url     = "http://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1003-0358",
  keywords = {DL}
}

@inproceedings{Collobert2008-su,
    title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
    author={Collobert, Ronan and Weston, Jason},
    booktitle={Proceedings of the 25th International Conference on Machine learning},
    pages={160--167},
    year={2008},
    organization={ACM}
}

@BOOK{Sra2012-uq,
  title     = "Optimization for Machine Learning",
  author    = "Sra, S and Nowozin, S and Wright, SJ",
  publisher = "MIT Press",
  series    = "Neural information processing series",
  year      =  2012,
  url       = "http://books.google.com/books?id=JPQx7s2L1A8C",
  keywords = {DL}
}

@ARTICLE{James2006-db,
  title    = "Validating a computerized scoring system for assessing writing
              and placing students in composition courses",
  author   = "James, Cindy L.",
  abstract = "How do scores from writing samples generated by computerized
              essay scorers compare to those generated by “untrained” human
              scorers and what combination of scores, if any, is more accurate
              at placing students in composition courses? This study endeavored
              to answer this two-part question by evaluating the correspondence
              between writing sample scores generated by the IntelliMetric™
              automated scoring system and scores generated by University
              Preparation English faculty, as well as examining the predictive
              validity of both the automated and human scores. The results
              revealed significant correlations between the faculty scores and
              the IntelliMetric™ scores of the ACCUPLACER™ OnLine WritePlacer
              Plus test. Moreover, logistic regression models that utilized the
              IntelliMetric™ scores and average faculty scores were more
              accurate at placing students (77\% overall correct placement
              rate) than were models incorporating only the average faculty
              score or the IntelliMetric™ scores.",
  journal  = "Assessing Writing",
  volume   =  11,
  issue    =  3,
  pages    = "167--178",
  year     =  2006,
  url      = "http://dx.doi.org/10.1016/j.asw.2007.01.002",
  keywords = "ACCUPLACER™; Automated scoring; IntelliMetric™; Placement;
              Validity; Writing assessment",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2007.01.002",
  keywords = {AWA}
}

%@ARTICLE{Bartlett1998-cc,
%  title    = "The sample complexity of pattern classification with neural
%              networks: the size of the weights is more important than the size
%              of the network",
%  author   = "Bartlett, PL",
%  abstract = "Sample complexity results from computational learning theory,
%              when applied to neural network learning for pattern
%              classification problems, suggest that for good generalization
%              performance the number of training examples should grow at least
%              linearly with the number of adjustable parameters in the network.
%              Results in this paper show that if a large neural network is used
%              for a pattern classification problem and the learning algorithm
%              finds a network with small weights that has small squared error
%              on the training patterns, then the generalization performance
%              depends on the size of the weights rather than the number of
%              weights. For example, consider a two-layer feedforward network of
%              sigmoid units, in which the sum of the magnitudes of the weights
%              associated with each unit is bounded by A and the input dimension
%              is n. We show that the misclassification probability is no more
%              than a certain error estimate (that is related to squared error
%              on the training set) plus A3 √((log n)/m) (ignoring log A and log
%              m factors), where m is the number of training patterns. This may
%              explain the generalization performance of neural networks,
%              particularly when the number of training examples is considerably
%              smaller than the number of weights. It also supports heuristics
%              (such as weight decay and early stopping) that attempt to keep
%              the weights small during training. The proof techniques appear to
%              be useful for the analysis of other pattern classifiers: when the
%              input domain is a totally bounded metric space, we use the same
%              approach to give upper bounds on misclassification probability
%              for classifiers with decision boundaries that are far from the
%              training examples",
%  journal  = "IEEE Trans. Inf. Theory",
%  volume   =  44,
%  issue    =  2,
%  pages    = "525--536",
%  year     =  1998,
%  url      = "http://dx.doi.org/10.1109/18.661502",
%  keywords = "bounded metric space; computational learning theory; Computer
%              networks; decision boundaries; early stopping; Error analysis;
%              error estimate; feedforward neural nets; generalization
%              performance; input dimension; input domain; learning (artificial
%              intelligence); misclassification probability; network parameters;
%              network size; neural network learning; Neural networks; Pattern
%              analysis; pattern classification; Pattern Recognition;
%              probability; Probability distribution; sample complexity; sigmoid
%              units; signal sampling; small squared error; squared error;
%              Statistical learning; training data; training patterns; two-layer
%              feedforward network; Upper bound; upper bounds; Virtual
%              colonoscopy; weight decay; weights size",
%  issn     = "0018-9448",
%  doi      = "10.1109/18.661502"
%}

%@INPROCEEDINGS{Krogh1995-xw,
%  title     = "Neural Network Ensembles, Cross Validation, and Active Learning",
%  author    = "Krogh, Anders and Vedelsby, Jesper",
%  abstract  = "Learning of continuous valued functions using neural network
%               ensembles (committees) can give improved accuracy, reliable
%               estimation of the generalization error, and active learning. The
%               ambiguity is defined as the variation of the output of ensemble
%               members averaged over unlabeled data, so it quantifies the
%               disagreement among the networks. It is discussed how to use the
%               ambiguity in combination with cross-validation to give a
%               reliable estimate of the ensemble generalization error, and how
%               this type of ensemble cross-validation can sometimes improve
%               performance. It is shown how to estimate the optimal weights of
%               the ensemble members using unlabeled data. By a generalization
%               of query by committee, it is finally shown how the ambiguity can
%               be used to select new training data to be labeled in an active
%               learning scheme. 1 INTRODUCTION It is well known that a
%               combination of many different predictors can improve
%               predictions. In the neural networks community {"}ensembles{"} of
%               neural networks h...",
%  publisher = "MIT Press",
%  pages     = "231--238",
%  year      =  1995,
%}

%@INPROCEEDINGS{Schraudolph1997-ya,
%  title     = "Centering Neural Network Gradient Factors",
%  author    = "Schraudolph, Nicol N.",
%  abstract  = "It has long been known that neural networks can learn faster
%               when their input and hidden unit activities are centered about
%               zero; recently we have extended this approach to also encompass
%               the centering of error signals [2]. Here we generalize this
%               notion to all factors involved in the network's gradient,
%               leading us to propose centering the slope of hidden unit
%               activation functions as well. Slope centering removes the linear
%               component of backpropagated error; this improves credit
%               assignment in networks with shortcut connections. Benchmark
%               results show that this can speed up learning significantly
%               without adversely affecting the trained network's generalization
%               ability.",
%  publisher = "Springer Verlag",
%  pages     = "207--226",
%  year      =  1997,
%}

@ARTICLE{Deane2013-pv,
  title    = "On the relation between automated essay scoring and modern views
              of the writing construct",
  author   = "Deane, Paul",
  abstract = "This paper examines the construct measured by automated essay
              scoring (AES) systems. AES systems measure features of the text
              structure, linguistic structure, and conventional print form of
              essays; as such, the systems primarily measure text production
              skills. In the current state-of-the-art, AES provide little
              direct evidence about such matters as strength of argumentation
              or rhetorical effectiveness. However, since there is a
              relationship between ease of text production and ability to
              mobilize cognitive resources to address rhetorical and conceptual
              problems, AES systems have strong correlations with overall
              performance and can effectively distinguish students in a
              position to apply a broader writing construct from those for whom
              text production constitutes a significant barrier to achievement.
              The papers begins by defining writing as a construct and then
              turns to the e-rater scoring engine as an example of AES
              state-of-the-art construct measurement. Common criticisms of AES
              are defined and explicated—fundamental objections to the
              construct measured, methods used to measure the construct, and
              technical inadequacies—and a direction for future research is
              identified through a socio-cognitive approach to AES.",
  journal  = "Assessing Writing",
  volume   =  18,
  issue    =  1,
  pages    = "7--24",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1016/j.asw.2012.10.002",
  keywords = "Assessment; Automated essay scoring (AES); Cognitively Based
              Assessments of, for, and as Learning (CBAL); Writing; Writing
              construct",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2012.10.002",
  keywords = {AWA}
}

@INPROCEEDINGS{Wang2013-kf,
  title     = "Fast dropout training",
  booktitle = "Proceedings of The 30th International Conference on Machine
               Learning",
  author    = "Wang, Sida and Manning, Christopher",
  pages     = "118--126",
  year      =  2013,
  url       = "http://jmlr.org/proceedings/papers/v28/wang13a.html",
  keywords = {DL}
}

@ARTICLE{McCurry2010-ut,
  title    = "Can machine scoring deal with broad and open writing tests as
              well as human readers?",
  author   = "McCurry, Doug",
  abstract = "This article considers the claim that machine scoring of writing
              test responses agrees with human readers as much as humans agree
              with other humans. These claims about the reliability of machine
              scoring of writing are usually based on specific and constrained
              writing tasks, and there is reason for asking whether machine
              scoring of writing requires specific and constrained tasks to
              produce results that mimic human judgements. The conclusion of a
              National Assessment of Educational Progress (NAEP) report on the
              online assessment of writing that ‘the automated scoring of essay
              responses did not agree with the scores awarded by human readers’
              is discussed. The article presents the results of a trial in
              which two software programmes for scoring writing test responses
              were compared with the results of the human scoring of a broad
              and open writing test. The trial showed that ‘automated essay
              scoring’ (AES) did not grade the broad and open writing task
              responses as reliably as human markers.",
  journal  = "Assessing Writing",
  volume   =  15,
  issue    =  2,
  pages    = "118--129",
  year     =  2010,
  url      = "http://dx.doi.org/10.1016/j.asw.2010.04.002",
  keywords = "Automated Essay Scoring; Computer scoring of writing; Machine
              scoring of writing; Validity of writing tests; Writing test
              design",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2010.04.002",
  keywords = {AWA}
}

@INCOLLECTION{noauthor_2012-dm,
  title     = "Mass-market writing assessments as bullshit",
  booktitle = "Writing assessment in the 21st century: Essays in honor of
               Edward M. White",
  publisher = "Hampton Press",
  pages     = "425--438",
  year      =  2012,
  keywords = {AWA}
}

@ARTICLE{Elliot2013-bd,
  title   = "Assessing Writing special issue: Assessing writing with automated
             scoring systems",
  author  = "Elliot, Norbert and Williamson, David M.",
  journal = "Assessing Writing",
  volume  =  18,
  issue   =  1,
  pages   = "1--6",
  month   =  jan,
  year    =  2013,
  url     = "http://dx.doi.org/10.1016/j.asw.2012.11.002",
  issn    = "1075-2935",
  doi     = "10.1016/j.asw.2012.11.002",
  keywords = {AWA}
}

@INPROCEEDINGS{Schwenk2012-du,
  title     = "Large, pruned or continuous space language models on a {GPU} for
               statistical machine translation",
  author    = "Schwenk, Holger and Rousseau, Anthony and Attik, Mohammed",
  abstract  = "Language models play an important role in large vocabulary
               speech recognition and statistical machine translation systems.
               The dominant approach since several decades are back-off
               language models. Some years ago, there was a clear tendency to
               build huge language models trained on hundreds of billions of
               words. Lately, this tendency has changed and recent works
               concentrate on data selection. Continuous space methods are a
               very competitive approach, but they have a high computational
               complexity and are not yet in widespread use. This paper
               presents an experimental comparison of all these approaches on a
               large statistical machine translation task. We also describe an
               open-source implementation to train and use continuous space
               language models (CSLM) for such large tasks. We describe an
               efficient implementation of the CSLM using graphical processing
               units from Nvidia. By these means, we are able to train an CSLM
               on more than 500 million words in 20 hours. This CSLM provides
               an improvement of up to 1.8 BLEU points with respect to the best
               back-off language model that we were able to build.",
  publisher = "Association for Computational Linguistics",
  pages     = "11--19",
  series    = "WLM '12",
  year      =  2012,
  url       = "http://dl.acm.org/citation.cfm?id=2390940.2390942",
  keywords  = "NLP",
  keywords = {DL}
}

@ARTICLE{Luong2013-js,
  title    = "Better word representations with recursive neural networks for
              morphology",
  author   = "Luong, MT and Socher, R and Manning, CD",
  journal  = "CoNLL-2013",
  year     =  2013,
  keywords = "NLP",
  keywords = {DL}
}

%@ARTICLE{Polyak1992-if,
%  title   = "Acceleration of Stochastic Approximation by Averaging",
%  author  = "Polyak, B T and Juditsky, A B",
%  journal = "SIAM J. Control Optim.",
%  volume  =  30,
%  issue   =  4,
%  pages   = "838--855",
%  month   =  jul,
%  year    =  1992,
%  url     = "http://dx.doi.org/10.1137/0330046",
%  issn    = "1095-7138",
%  doi     = "10.1137/0330046"
%}

@ARTICLE{Chen2013-su,
  title    = "The Expressive Power of Word Embeddings",
  author   = "Chen, Yanqing and Perozzi, Bryan and Al-Rfou', Rami and Skiena,
              Steven",
  journal  = "CoRR",
  volume   = "abs/1301.3226",
  year     =  2013,
  url      = "http://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1301-3226",
  keywords = "NLP",
  keywords = {DL}
}

@INPROCEEDINGS{Streeter2002-td,
  title  = "The credible grading machine: Automated essay scoring in the {DoD}",
  author = "Streeter, Lynn and Psotka, Joseph and Laham, Darrell and MacCuish,
            Don",
  volume =  2002,
  year   =  2002,
  keywords = {AWA}
}

@MISC{ASAPAES,
  title        = "Description - The Hewlett Foundation: Automated Essay Scoring
                  | Kaggle",
  abstract     = "Kaggle is a platform for data prediction competitions.
                  Companies, organizations and researchers post their data and
                  have it scrutinized by the world's best statisticians.",
  url          = "http://www.kaggle.com/c/ASAP-AES",
  howpublished = "\url{http://www.kaggle.com/c/ASAP-AES}",
  note         = "Accessed: 2013-10-7",
  keywords = {AWA}
}

@BOOK{Socher_undated-qz,
  title    = "Learning Continuous Phrase Representations and Syntactic Parsing
              with Recursive Neural Networks",
  author   = "Socher, Richard and Manning, Christopher D. and Ng, Andrew Y.",
  abstract = "Natural language parsing has typically been done with small sets
              of discrete categories such as NP and VP, but this representation
              does not capture the full syntactic nor semantic richness of
              linguistic phrases, and attempts to improve on this by
              lexicalizing phrases only partly address the problem at the cost
              of huge feature spaces and sparseness. To address this, we
              introduce a recursive neural network architecture for jointly
              parsing natural language and learning vector space
              representations for variable-sized inputs. At the core of our
              architecture are context-sensitive recursive neural networks
              (CRNN). These networks can induce distributed feature
              representations for unseen phrases and provide syntactic
              information to accurately predict phrase structure trees. Most
              excitingly, the representation of each phrase also captures
              semantic information: For instance, the phrases “decline to
              comment” and “would not disclose the terms ” are close by in the
              induced embedding space. Our current system achieves an unlabeled
              bracketing F-measure of 92.1\% on the Wall Street Journal dataset
              for sentences up to length 15. 1",
  keywords = "NLP",
  keywords = {DL}
}

%@ARTICLE{Lamblin2010-rk,
%  title   = "Important gains from supervised fine-tuning of deep architectures
%             on large labeled sets",
%  author  = "Lamblin, P and Bengio, Y",
%  journal = "NIPS* 2010 Deep Learning and Unsupervised Feature",
%  year    =  2010
%}

@ARTICLE{Johnson1996-hs,
  title    = "On Bayesian Analysis Of {Multi-Rater} Ordinal Data: An
              Application To Automated Essay Grading",
  author   = "Johnson, Valen E.",
  abstract = "A framework is proposed for the analysis of ordinal categorical
              data when ratings from several judges are available. Emphasis
              focuses on the tasks of estimating quantiles of individual items,
              regressing item quantiles on observed covariates, and comparing
              the performance of raters. The model is illustrated in the design
              and evaluation of an automated essay grader. This grader is based
              on a regression of variables obtained from a grammer checker on
              essay scores estimated from a panel of experts. The performance
              of the grader is evaluated relative to the human graders, and
              implications on the reliability and repeatability of both
              automated and human raters is investigated.",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  91,
  pages    = "42--51",
  year     =  1996,
  issn     = "0162-1459",
  keywords = {AWA}
}

%@ARTICLE{Nesterov2009-gh,
%  title    = "Primal-dual subgradient methods for convex problems",
%  author   = "Nesterov, Yurii",
%  abstract = "In this paper we present a new approach for constructing
%              subgradient schemes for different types of nonsmooth problems
%              with convex structure. Our methods are primal-dual since they are
%              always able to generate a feasible approximation to the optimum
%              of an appropriately formulated dual problem. Besides other
%              advantages, this useful feature provides the methods with a
%              reliable stopping criterion. The proposed schemes differ from the
%              classical approaches (divergent series methods, mirror descent
%              methods) by presence of two control sequences. The first sequence
%              is responsible for aggregating the support functions in the dual
%              space, and the second one establishes a dynamically updated scale
%              between the primal and dual spaces. This additional flexibility
%              allows to guarantee a boundedness of the sequence of primal test
%              points even in the case of unbounded feasible set (however, we
%              always assume the uniform boundedness of subgradients). We
%              present the variants of subgradient schemes for nonsmooth convex
%              minimization, minimax problems, saddle point problems,
%              variational inequalities, and stochastic optimization. In all
%              situations our methods are proved to be optimal from the view
%              point of worst-case black-box lower complexity bounds.",
%  journal  = "Math. Program.",
%  volume   =  120,
%  issue    =  1,
%  pages    = "221--259",
%  month    =  aug,
%  year     =  2009,
%  url      = "http://dx.doi.org/10.1007/s10107-007-0149-x",
%  keywords = "68Q25; 90C25; 90C47; Black-box methods; Calculus of Variations
%              and Optimal Control; Optimization; Combinatorics; Convex
%              optimization; Lower complexity bounds; Mathematical and
%              Computational Physics; Mathematical Methods in Physics;
%              Mathematics of Computing; Minimax problems; Non-smooth
%              optimization; Numerical Analysis; Saddle points; Stochastic
%              optimization; Subgradient methods; Variational inequalities",
%  issn     = "0025-5610",
%  doi      = "10.1007/s10107-007-0149-x"
%}

@ARTICLE{Goodfellow2012-eu,
  title    = "{Spike-and-Slab} Sparse Coding for Unsupervised Feature Discovery",
  author   = "Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua",
  abstract = "We consider the problem of using a factor model we call
              \textbackslashem spike-and-slab sparse coding (S3C) to learn
              features for a classification task. The S3C model resembles both
              the spike-and-slab RBM and sparse coding. Since exact inference
              in this model is intractable, we derive a structured variational
              inference procedure and employ a variational EM training
              algorithm. Prior work on approximate inference for this model has
              not prioritized the ability to exploit parallel architectures and
              scale to enormous problem sizes. We present an inference
              procedure appropriate for use with GPUs which allows us to
              dramatically increase both the training set size and the amount
              of latent factors. We demonstrate that this approach improves
              upon the supervised learning capabilities of both sparse coding
              and the ssRBM on the CIFAR-10 dataset. We evaluate our approach's
              potential for semi-supervised learning on subsets of CIFAR-10. We
              demonstrate state-of-the art self-taught learning performance on
              the STL-10 dataset and use our method to win the NIPS 2011
              Workshop on Challenges In Learning Hierarchical Models' Transfer
              Learning Challenge.",
  journal  = "ArXiv e-prints",
  volume   =  1201,
  month    =  jan,
  year     =  2012,
  url      = "http://adsabs.harvard.edu/abs/2012arXiv1201.3382G",
  keywords = "Computer Science - Learning; Statistics - Machine Learning",
  keywords = {DL}
}

%@INPROCEEDINGS{Kavukcuoglu2009-cs,
%  title     = "Learning invariant features through topographic filter maps",
%  booktitle = "{IEEE} Conference on Computer Vision and Pattern Recognition,
%               2009. {CVPR} 2009",
%  author    = "Kavukcuoglu, K and Ranzato, M and Fergus, R and LeCun, Y",
%  abstract  = "Several recently-proposed architectures for high-performance
%               object recognition are composed of two main stages: a feature
%               extraction stage that extracts locally-invariant feature vectors
%               from regularly spaced image patches, and a somewhat generic
%               supervised classifier. The first stage is often composed of
%               three main modules: (1) a bank of filters (often oriented edge
%               detectors); (2) a non-linear transform, such as a point-wise
%               squashing functions, quantization, or normalization; (3) a
%               spatial pooling operation which combines the outputs of similar
%               filters over neighboring regions. We propose a method that
%               automatically learns such feature extractors in an unsupervised
%               fashion by simultaneously learning the filters and the pooling
%               units that combine multiple filter outputs together. The method
%               automatically generates topographic maps of similar filters that
%               extract features of orientations, scales, and positions. These
%               similar filters are pooled together, producing locally-invariant
%               outputs. The learned feature descriptors give comparable results
%               as SIFT on image recognition tasks for which SIFT is well
%               suited, and better results than SIFT on tasks for which SIFT is
%               less well suited.",
%  pages     = "1605--1612",
%  year      =  2009,
%  url       = "http://dx.doi.org/10.1109/CVPR.2009.5206545",
%  keywords  = "Brain modeling; Detectors; feature extraction; Filter bank;
%               generic supervised classifier; image classification; Image edge
%               detection; image recognition; invariant feature vectors; learned
%               feature descriptors; learning (artificial intelligence); object
%               recognition; Proposals; quantization; quashing function;
%               Robustness; spaced image patches; spatial pooling operation;
%               topographic filter maps",
%  doi       = "10.1109/CVPR.2009.5206545"
%}

@ARTICLE{Srivastava2013-ae,
  title    = "Modeling Documents with Deep Boltzmann Machines",
  author   = "Srivastava, N and Salakhutdinov, R R and Hinton, G E",
  journal  = "ArXiv e-prints",
  month    =  sep,
  year     =  2013,
  keywords = "Computer Science - Artificial Intelligence; Computer Science -
              Learning; Statistics - Machine Learning",
  keywords = {DL}
}

@INCOLLECTION{Bengio2007-gr,
  title     = "Scaling Learning Algorithms Towards {AI}",
  booktitle = "Large Scale Kernel Machines",
  author    = "Bengio, Yoshua and LeCun, Yann",
  editor    = "Bottou, L\'{e}on and Chapelle, Olivier and DeCoste, D and
               Weston, J",
  abstract  = "One long-term goal of machine learning research is to produce
               methods that are applicable to highly complex tasks, such as
               perception (vision, audition), reasoning, intelligent control,
               and other artificially intelligent behaviors. We argue that in
               order to progress toward this goal, the Machine Learning
               community must endeavor to discover algorithms that can learn
               highly complex functions, with minimal need for prior knowledge,
               and with minimal human intervention. We present mathematical and
               empirical evidence suggesting that many popular approaches to
               non-parametric learning, particularly kernel methods, are
               fundamentally limited in their ability to learn complex
               high-dimensional functions. Our analysis focuses on two
               problems. First, kernel machines are shallow architectures, in
               which one large layer of simple template matchers is followed by
               a single layer of trainable coefficients. We argue that shallow
               architectures can be very inefficient in terms of required
               number of computational elements and examples. Second, we
               analyze a limitation of kernel machines with a local kernel,
               linked to the curse of dimensionality, that applies to
               supervised, unsupervised (manifold learning) and semi-supervised
               kernel machines. Using empirical results on invariant image
               recognition tasks, kernel methods are compared with deep
               architectures, in which lower-level features or concepts are
               progressively combined into more abstract and higher-level
               representations. We argue that deep architectures have the
               potential to generalize in non-local ways, i.e., beyond
               immediate neighbors, and that this is crucial in order to make
               progress on the kind of complex tasks required for artificial
               intelligence.",
  publisher = "MIT Press",
  year      =  2007,
  url       = "http://www.iro.umontreal.ca/
               lisa/pointeurs/bengio+lecun_chapter2007.pdf",
  keywords  = "2007; deep; svm",
  keywords = {DL}
}

@inproceedings{Mikolov2011-pe,
    title={Extensions of recurrent neural network language model},
    author={Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan H and Khudanpur, Sanjeev},
    booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
    pages={5528--5531},
    year={2011},
    organization={IEEE}
}

@INPROCEEDINGS{Glorot2011-sk,
  title     = "Domain Adaptation for {Large-Scale} Sentiment Classification: A
               Deep Learning Approach",
  booktitle = "{ICML}",
  author    = "Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua",
  year      =  2011,
  url       = "http://www.icml-2011.org/papers/342_icmlpaper.pdf",
  keywords  = "deep\_belief\_networks",
  keywords = {DL}
}

@INPROCEEDINGS{Bordes2011-cq,
  title     = "Learning Structured Embeddings of Knowledge Bases",
  author    = "Bordes, Antoine and Weston, Jason and Collobert, Ronan and
               Bengio, Yoshua",
  editor    = "Burgard, Wolfram and Roth, Dan",
  publisher = "AAAI Press",
  year      =  2011,
  url       = "http://dblp.uni-trier.de/db/conf/aaai/aaai2011.html#BordesWCB11",
  keywords  = "dblp",
  keywords = {DL}
}

@INPROCEEDINGS{Glorot2011-so,
  title  = "Deep Sparse Rectifier Networks",
  author = "Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua",
  volume =  15,
  pages  = "315--323",
  year   =  2011,
  keywords = {DL}
}

@INPROCEEDINGS{Rifai2012-jk,
  title     = "A Generative Process for sampling Contractive {Auto-Encoders}",
  booktitle = "Proceedings of the 29th International Conference on Machine
               Learning ({ICML-12})",
  author    = "Rifai, Salah and Bengio, Yoshua and Vincent, Pascal and Dauphin,
               Yann N.",
  pages     = "1855--1862",
  year      =  2012,
  url       = "http://machinelearning.wustl.edu/mlpapers/papers/ICML2012Rifai_910",
  keywords = {DL}
}

@ARTICLE{Hinton1989-nv,
  title    = "Connectionist learning procedures",
  author   = "Hinton, Geoffrey E.",
  abstract = "A major goal of research on networks of neuron-like processing
              units is to discover efficient learning procedures that allow
              these networks to construct complex internal representations of
              their environment. The learning procedures must be capable of
              modifying the connection strengths in such a way that internal
              units which are not part of the input or output come to represent
              important features of the task domain. Several interesting
              gradient-descent procedures have recently been discovered. Each
              connection computes the derivative, with respect to the
              connection strength, of a global measure of the error in the
              performance of the network. The strength is then adjusted in the
              direction that decreases the error. These relatively simple,
              gradient-descent learning procedures work well for small tasks
              and the new challenge is to find ways of improving their
              convergence rate and their generalization abilities so that they
              can be applied to larger, more realistic tasks.",
  journal  = "Artif. Intell.",
  volume   =  40,
  issue    = "1–3",
  pages    = "185--234",
  month    =  sep,
  year     =  1989,
  url      = "http://dx.doi.org/10.1016/0004-3702(89)90049-0",
  issn     = "0004-3702",
  doi      = "10.1016/0004-3702(89)90049-0",
  keywords = {DL}
}

@ARTICLE{Foltz2000-dy,
  title   = "Supporting content-based feedback in on-line writing evaluation
             with {LSA}",
  author  = "Foltz, Peter W and Gilliam, Sara and Kendall, Scott",
  journal = "Interactive Learning Environments",
  volume  =  8,
  issue   =  2,
  pages   = "111--127",
  year    =  2000,
  keywords = {AWA}
}

%@ARTICLE{Olshausen1997-wt,
%  title    = "Sparse coding with an overcomplete basis set: A strategy employed
%              by V1?",
%  author   = "Olshausen, Bruno A. and Field, David J.",
%  abstract = "The spatial receptive fields of simple cells in mammalian striate
%              cortex have been reasonably well described physiologically and
%              can be characterized as being localized, oriented, and bandpass,
%              comparable with the basis functions of wavelet transforms.
%              Previously, we have shown that these receptive field properties
%              may be accounted for in terms of a strategy for producing a
%              sparse distribution of output activity in response to natural
%              images. Here, in addition to describing this work in a more
%              expansive fashion, we examine the neurobiological implications of
%              sparse coding. Of particular interest is the case when the code
%              is overcomplete—i.e., when the number of code elements is greater
%              than the effective dimensionality of the input space. Because the
%              basis functions are non-orthogonal and not linearly independent
%              of each other, sparsifying the code will recruit only those basis
%              functions necessary for representing a given input, and so the
%              input-output function will deviate from being purely linear.
%              These deviations from linearity provide a potential explanation
%              for the weak forms of non-linearity observed in the response
%              properties of cortical simple cells, and they further make
%              predictions about the expected interactions among units in
%              response to naturalistic stimuli.",
%  journal  = "Vision Res.",
%  volume   =  37,
%  issue    =  23,
%  pages    = "3311--3325",
%  month    =  dec,
%  year     =  1997,
%  url      = "http://dx.doi.org/10.1016/S0042-6989(97)00169-7",
%  keywords = "Coding; Gabor-wavelet; Natural images; V1",
%  issn     = "0042-6989",
%  doi      = "10.1016/S0042-6989(97)00169-7"
%}

@ARTICLE{Klinger2008-nk,
  title    = "The Evolving Culture of {Large-Scale} Assessments in Canadian
              Education",
  author   = "Klinger, Don A. and DeLuca, Christopher and Miller, Tess",
  abstract = "In recent years, an increase in the number of large-scale
              assessment programs in Canada has been observed. However, due to
              the provincial/territorial control of education throughout
              Canada, the format and purposes of these assessment programs
              vary. The central purpose of this study was to document the
              format and explicit purposes of the current large-scale
              assessment programs in each of Canada's ten provinces and three
              territories. Through document analysis of publicly accessible
              policy documents, examination of Ministry websites, and telephone
              interviews with Ministry of Education officials, specific and
              general characteristics and purposes of large-scale assessment
              practices in Canada were obtained and analyzed. This analysis
              provided an opportunity to examine the commonalities and
              variations in the frameworks guiding the current large-scale
              assessment culture that is evolving in Canada. Assessment
              programs were categorized by their explicit purposes as they
              related to the functions of accountability, gatekeeping,
              instructional diagnosis, and monitoring student achievement over
              time. (Contains 3 tables.)",
  journal  = "Canadian Journal of Educational Administration and Policy",
  month    =  jul,
  year     =  2008,
  url      = "http://eric.ed.gov/?id=EJ807064",
  keywords = {AWA}
}

@INPROCEEDINGS{Salakhutdinov2009-oh,
  title  = "Deep boltzmann machines",
  author = "Salakhutdinov, Ruslan and Hinton, Geoffrey E",
  pages  = "448--455",
  year   =  2009,
  keywords = {DL}
}

@INPROCEEDINGS{Maas2010-km,
  title     = "A Probabilistic Model for Semantic Word Vectors",
  booktitle = "Deep Learning and Unsupervised Feature Learning Workshop —
               {NIPS} 2010",
  author    = "Maas, Andrew and Ng, Andrew",
  year      =  2010,
  url       = "http://deeplearningworkshopnips2010.files.wordpress.com/2010/11/amaas_dlufl2010.pdf",
  keywords  = "NLP; sentiment",
  keywords = {DL}
}

@ARTICLE{Luduea2013-cv,
  title    = "A {Self-Organized} Neural Comparator",
  author   = "Ludue\~{n}a, Guillermo A. and Gros, Claudius",
  abstract = "Learning algorithms need generally the ability to compare several
              streams of information. Neural learning architectures hence need
              a unit, a comparator, able to compare several inputs encoding
              either internal or external information, for instance,
              predictions and sensory readings. Without the possibility of
              comparing the values of predictions to actual sensory inputs,
              reward evaluation and supervised learning would not be possible.",
  journal  = "Neural Comput.",
  volume   =  25,
  issue    =  4,
  pages    = "1006--1028",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1162/NECO_a_00424",
  issn     = "0899-7667",
  doi      = "10.1162/NECO\_a\_00424",
  keywords = {DL}
}

@ARTICLE{noauthor_undated-tu,
  title = "Word Association Profiles and their Use for Automated Scoring of
           Essays",
  url   = "http://aclweb.org/anthology/P/P13/P13-1113.pdf",
  keywords = {DL}
}

@ARTICLE{Ramineni2013-jv,
  title    = "Validating automated essay scoring for online writing placement",
  author   = "Ramineni, Chaitanya",
  abstract = "In this paper, I describe the design and evaluation of automated
              essay scoring (AES) models for an institution's writing placement
              program. Information was gathered on admitted student writing
              performance at a science and technology research university in
              the northeastern United States. Under timed conditions,
              first-year students (N = 879) were assigned to write essays on
              two persuasive prompts within the Criterion® Online Writing
              Evaluation Service at the beginning of the semester. AES models
              were built and evaluated for a total of four prompts. AES models
              meeting recommended performance criteria were then compared to
              standardized admissions measures and locally developed writing
              measures. Results suggest that there is evidence to support the
              use of Criterion as part of the placement process at the
              institution.",
  journal  = "Assessing Writing",
  volume   =  18,
  issue    =  1,
  pages    = "40--61",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1016/j.asw.2012.10.005",
  keywords = "Automated Essay Scoring; Online placement; Writing assessment",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2012.10.005",
  keywords = {AWA}
}

@article{Matsuoka1992-ee,
    title={Noise injection into inputs in back-propagation learning},
    author={Matsuoka, Kiyotoshi},
    journal={Systems, Man and Cybernetics, IEEE Transactions on},
    volume={22},
    number={3},
    pages={436--440},
    year={1992},
    publisher={IEEE}
}

@ARTICLE{Bergstra2012-os,
  title    = "Random search for hyper-parameter optimization",
  author   = "Bergstra, James and Bengio, Yoshua",
  abstract = "Grid search and manual search are the most widely used strategies
              for hyper-parameter optimization. This paper shows empirically
              and theoretically that randomly chosen trials are more efficient
              for hyper-parameter optimization than trials on a grid. Empirical
              evidence comes from a comparison with a large previous study that
              used grid search and manual search to configure neural networks
              and deep belief networks. Compared with neural networks
              configured by a pure grid search, we find that random search over
              the same domain is able to find models that are as good or better
              within a small fraction of the computation time. Granting random
              search the same computational budget, random search finds better
              models by effectively searching a larger, less promising
              configuration space. Compared with deep belief networks
              configured by a thoughtful combination of manual search and grid
              search, purely random search over the same 32-dimensional
              configuration space found statistically equal performance on four
              of seven data sets, and superior performance on one of seven. A
              Gaussian process analysis of the function from hyper-parameters
              to validation set performance reveals that for most data sets
              only a few of the hyper-parameters really matter, but that
              different hyper-parameters are important on different data sets.
              This phenomenon makes grid search a poor choice for configuring
              algorithms for new data sets. Our analysis casts some light on
              why recent {"}High Throughput{"} methods achieve surprising
              success--they appear to search through a large number of
              hyper-parameters because most hyper-parameters do not matter
              much. We anticipate that growing interest in large hierarchical
              models will place an increasing burden on techniques for
              hyper-parameter optimization; this work shows that random search
              is a natural baseline against which to judge progress in the
              development of adaptive (sequential) hyper-parameter optimization
              algorithms.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  13,
  issue    =  1,
  pages    = "281--305",
  month    =  feb,
  year     =  2012,
  url      = "http://dl.acm.org/citation.cfm?id=2503308.2188395",
  keywords = "deep learning; global optimization; model selection; Neural
              networks; response surface modeling",
  issn     = "1532-4435",
  keywords = {DL}
}

@INPROCEEDINGS{Ngiam2011-bg,
  title  = "On optimization methods for deep learning",
  author = "Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and Prochnow,
            Bobby and Ng, Andrew and Le, Quoc",
  pages  = "265--272",
  year   =  2011,
  keywords = {DL}
}

@ARTICLE{Cope2011-vp,
  title    = "{Technology-Mediated} Writing Assessments: Principles and
              Processes",
  author   = "Cope, Bill and Kalantzis, Mary and McCarthey, Sarah and Vojak,
              Colleen and Kline, Sonia",
  abstract = "This paper explores developments in technology-mediated writing
              environments that may support new forms of formative assessment
              and the closer relation of formative to summative assessment. Not
              only might these provide more learner-responsive and effective
              assessment of writing, but they may also support the assessment
              of disciplinary knowledge embedded in written and multimodal
              texts. After an overview of current debates on contemporary
              assessment practice, the paper goes on to develop six principles
              for effective assessment of writing. On this basis, the paper
              identifies potentially promising aspects of emerging processes of
              technology-mediated writing assessment.",
  journal  = "Computers and Composition",
  volume   =  28,
  issue    =  2,
  pages    = "79--96",
  month    =  jun,
  year     =  2011,
  url      = "http://dx.doi.org/10.1016/j.compcom.2011.04.007",
  keywords = "Assessment; Internet; Multimodality; technology; Writing",
  issn     = "8755-4615",
  doi      = "10.1016/j.compcom.2011.04.007",
  keywords = {AWA}
}

@ARTICLE{Schwenk2007-sx,
  title    = "Continuous space language models",
  author   = "Schwenk, Holger",
  abstract = "This paper describes the use of a neural network language model
              for large vocabulary continuous speech recognition. The
              underlying idea of this approach is to attack the data sparseness
              problem by performing the language model probability estimation
              in a continuous space. Highly efficient learning algorithms are
              described that enable the use of training corpora of several
              hundred million words. It is also shown that this approach can be
              incorporated into a large vocabulary continuous speech recognizer
              using a lattice rescoring framework at a very low additional
              processing time. The neural network language model was thoroughly
              evaluated in a state-of-the-art large vocabulary continuous
              speech recognizer for several international benchmark tasks, in
              particular the Nist evaluations on broadcast news and
              conversational speech recognition. The new approach is compared
              to four-gram back-off language models trained with modified
              Kneser–Ney smoothing which has often been reported to be the best
              known smoothing method. Usually the neural network language model
              is interpolated with the back-off language model. In that way,
              consistent word error rate reductions for all considered tasks
              and languages were achieved, ranging from 0.4\% to almost 1\%
              absolute.",
  journal  = "Comput. Speech Lang.",
  volume   =  21,
  issue    =  3,
  pages    = "492--518",
  month    =  jul,
  year     =  2007,
  url      = "http://dx.doi.org/10.1016/j.csl.2006.09.003",
  keywords = "NLP",
  issn     = "0885-2308",
  doi      = "10.1016/j.csl.2006.09.003",
  keywords = {DL}
}

@ARTICLE{Riedel2006-uy,
  title   = "Experimental evidence on the effectiveness of automated essay
             scoring in teacher education cases",
  author  = "Riedel, Eric and Dexter, Sara L and Scharber, Cassandra and
             Doering, Aaron",
  journal = "Journal of Educational Computing Research",
  volume  =  35,
  issue   =  3,
  pages   = "267--287",
  year    =  2006,
  keywords = {AWA}
}

@ARTICLE{Wang2008-dn,
  title    = "Assessing creative problem-solving with automated text grading",
  author   = "Wang, Hao-Chuan and Chang, Chun-Yen and Li, Tsai-Yen",
  abstract = "The work aims to improve the assessment of creative
              problem-solving in science education by employing language
              technologies and computational–statistical machine learning
              methods to grade students’ natural language responses
              automatically. To evaluate constructs like creative
              problem-solving with validity, open-ended questions that elicit
              students’ constructed responses are beneficial. But the high cost
              required in manually grading constructed responses could become
              an obstacle in applying open-ended questions. In this study,
              automated grading schemes have been developed and evaluated in
              the context of secondary Earth science education. Empirical
              evaluations revealed that the automated grading schemes may
              reliably identify domain concepts embedded in students’ natural
              language responses with satisfactory inter-coder agreement
              against human coding in two sub-tasks of the test (Cohen’s Kappa
              = .65–.72). And when a single holistic score was computed for
              each student, machine-generated scores achieved high inter-rater
              reliability against human grading (Pearson’s r = .92). The
              reliable performance in automatic concept identification and
              numeric grading demonstrates the potential of using automated
              grading to support the use of open-ended questions in science
              assessments and enable new technologies for science learning.",
  journal  = "Comput. Educ.",
  volume   =  51,
  issue    =  4,
  pages    = "1450--1466",
  month    =  dec,
  year     =  2008,
  url      = "http://dx.doi.org/10.1016/j.compedu.2008.01.006",
  keywords = "Automated grading; Computer-aided assessment; Creative
              problem-solving; Machine learning application; Science learning
              assessment",
  issn     = "0360-1315",
  doi      = "10.1016/j.compedu.2008.01.006",
  keywords = {AWA}
}

@ARTICLE{Geman1992-ue,
  title   = "Neural networks and the bias/variance dilemma",
  author  = "Geman, Stuart and Bienenstock, Elie and Doursat, Ren\'{e}",
  journal = "Neural Comput.",
  volume  =  4,
  issue   =  1,
  pages   = "1--58",
  month   =  jan,
  year    =  1992,
  url     = "http://dx.doi.org/10.1162/neco.1992.4.1.1",
  issn    = "0899-7667",
  doi     = "10.1162/neco.1992.4.1.1",
  keywords = {DL}
}

@article{Larochelle2009-ml,
    title={Exploring strategies for training deep neural networks},
    author={Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Lamblin, Pascal},
    journal={The Journal of Machine Learning Research},
    volume={10},
    pages={1--40},
    year={2009},
    publisher={JMLR.org}
}

@ARTICLE{Krueger2009-cu,
  title    = "Flexible shaping: How learning in small steps helps",
  author   = "Krueger, Kai A. and Dayan, Peter",
  abstract = "Humans and animals can perform much more complex tasks than they
              can acquire using pure trial and error learning. This gap is
              filled by teaching. One important method of instruction is
              shaping, in which a teacher decomposes a complete task into
              sub-components, thereby providing an easier path to learning.
              Despite its importance, shaping has not been substantially
              studied in the context of computational modeling of cognitive
              learning. Here we study the shaping of a hierarchical working
              memory task using an abstract neural network model as the target
              learner. Shaping significantly boosts the speed of acquisition of
              the task compared with conventional training, to a degree that
              increases with the temporal complexity of the task. Further, it
              leads to internal representations that are more robust to task
              manipulations such as reversals. We use the model to investigate
              some of the elements of successful shaping.",
  journal  = "Cognition",
  volume   =  110,
  issue    =  3,
  pages    = "380--394",
  month    =  mar,
  year     =  2009,
  url      = "http://dx.doi.org/10.1016/j.cognition.2008.11.014",
  keywords = "Computational modeling; Gating; PFC; Sequence learning; Shaping",
  issn     = "0010-0277",
  doi      = "10.1016/j.cognition.2008.11.014",
  keywords = {DL}
}

@ARTICLE{Hinton2012-xw,
  title    = "Improving neural networks by preventing co-adaptation of feature
              detectors",
  author   = "Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex
              and Sutskever, Ilya and Salakhutdinov, Ruslan",
  journal  = "CoRR",
  volume   = "abs/1207.0580",
  year     =  2012,
  url      = "http://dblp.uni-trier.de/db/journals/corr/corr1207.html#abs-1207-0580",
  keywords = "dblp",
  keywords = {DL}
}

@BOOK{Collobert2011-rm,
  title    = "Natural language processing (almost) from scratch.
              arXiv:1103.0398v1",
  author   = "Collobert, Ronan and Weston, Jason and Bottou, L\'{e}on and
              Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel and
              Collins, Michael",
  abstract = "We propose a unified neural network architecture and learning
              algorithm that can be applied to various natural language
              processing tasks including part-of-speech tagging, chunking,
              named entity recognition, and semantic role labeling. This
              versatility is achieved by trying to avoid task-specific
              engineering and therefore disregarding a lot of prior knowledge.
              Instead of exploiting man-made input features carefully optimized
              for each task, our system learns internal representations on the
              basis of vast amounts of mostly unlabeled training data. This
              work is then used as a basis for building a freely available
              tagging system with good performance and minimal computational
              requirements.",
  year     =  2011,
  url      = "http://arxiv.org/abs/1103.0398",
  keywords = "NLP",
  keywords = {DL}
}

@ARTICLE{Shermis2002-mq,
  title   = "Trait ratings for automated essay grading",
  author  = "Shermis, Mark D and Koch, Chantal Mees and Page, Ellis B and
             Keith, Timothy Z and Harrington, Susanmarie",
  journal = "Educ. Psychol. Meas.",
  volume  =  62,
  issue   =  1,
  pages   = "5--18",
  year    =  2002,
  issn    = "0013-1644",
  keywords = {AWA}
}

@MISC{noauthor_undated-ja,
  title        = "{NCTE} Position Statement on Machine Scoring",
  abstract     = "Machine Scoring Fails the Test, from the NCTE Task Force on
                  Writing Assessment, aproved by the NCTE Executive Committee,
                  April 15, 2013",
  url          = "http://www.ncte.org/positions/statements/machine_scoring",
  howpublished = "\url{http://www.ncte.org/positions/statements/machine_scoring}",
  note         = "Accessed: 2013-10-7",
  keywords = {AWA}
}

@ARTICLE{Elman1993-sw,
  title    = "Learning and development in neural networks: The importance of
              starting small",
  author   = "Elman, Jeffrey L.",
  abstract = "It is a striking fact that in humans the greatest learnmg occurs
              precisely at that point in time- childhood- when the most
              dramatic maturational changes also occur. This report describes
              possible synergistic interactions between maturational change and
              the ability to learn a complex domain (language), as investigated
              in con-nectionist networks. The networks are trained to process
              complex sentences involving relative clauses, number agreement,
              and several types of verb argument structure. Training fails in
              the case of networks which are fully formed and ‘adultlike ’ in
              their capacity. Training succeeds only when networks begin with
              limited working memory and gradually ‘mature ’ to the adult
              state. This result suggests that rather than being a limitation,
              developmental restrictions on resources may constitute a
              necessary prerequisite for mastering certain complex domains.
              Specifically, successful learning may depend on starting small.",
  journal  = "Cognition",
  volume   =  48,
  pages    = "71--99",
  year     =  1993,
  issn     = "0010-0277",
  keywords = {DL}
}

@ARTICLE{KyungHyun_Cho2011-zp,
  title  = "Enhanced Gradient and Adaptive Learning Rate for Training
            Restricted Boltzmann Machines",
  author = "KyungHyun Cho, Tapani Raiko",
  pages  = "105--112",
  year   =  2011,
  keywords = {DL}
}

@ARTICLE{Valenti2003-uk,
  title   = "An overview of current research on automated essay grading",
  author  = "Valenti, Salvatore and Neri, Francesca and Cucchiarelli,
             Alessandro",
  journal = "Journal of Information Technology Education",
  volume  =  2,
  pages   = "319--330",
  year    =  2003,
  keywords = {AWA}
}

@ARTICLE{Van_der_Maaten2008-wv,
  title   = "Visualizing data using {t-SNE}",
  author  = "Van der Maaten, Laurens and Hinton, Geoffrey",
  journal = "J. Mach. Learn. Res.",
  volume  =  9,
  issue   = "2579-2605",
  year    =  2008,
  issn    = "1532-4435",
  keywords = {DL}
}

@ARTICLE{Warschauer2008-rs,
  title   = "Automated writing assessment in the classroom",
  author  = "Warschauer, Mark and Grimes, Douglas",
  journal = "Pedagogies: An International Journal",
  volume  =  3,
  issue   =  1,
  pages   = "22--36",
  year    =  2008,
  keywords = {AWA}
}

@ARTICLE{Herrington2001-rf,
  title   = "What Happens When Machines Read Our Students' Writing?",
  author  = "Herrington, Anne and Moran, Charles",
  journal = "College English",
  volume  =  63,
  issue   =  4,
  pages   = "480--499",
  month   =  mar,
  year    =  2001,
  url     = "http://dx.doi.org/10.2307/378891",
  issn    = "0010-0994",
  doi     = "10.2307/378891",
  keywords = {AWA}
}

@ARTICLE{Murray_undated-lg,
  title    = "Automatic Essay Scoring",
  author   = "Murray, KW and Orii, N",
  abstract = "Abstract Standardized tests are hampered by the manual effort
              required to score student- written essays . In this paper, we
              show how linear regression can be used to automatically grade
              essays on standardized tests. We combine simple, shallow features
              of the essays , ...",
  journal  = "cs.cmu.edu",
  url      = "http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/norii/pub/aes.pdf",
  keywords = {AWA}
}

@INPROCEEDINGS{Turian2010-vh,
  title     = "Word Representations: A Simple and General Method for
               {Semi-Supervised} Learning",
  booktitle = "Proceedings of the 48th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "Turian, Joseph and Ratinov, Lev-Arie and Bengio, Yoshua",
  publisher = "Association for Computational Linguistics",
  pages     = "384--394",
  year      =  2010,
  url       = "http://www.aclweb.org/anthology/P10-1040",
  keywords  = "NLP",
  keywords = {DL}
}

@ARTICLE{Hstad1991-pl,
  title    = "On the power of small-depth threshold circuits",
  author   = "H\aa{}stad, Johan and Goldmann, Mikael",
  abstract = "We investigate the power of threshold circuits of small depth. In
              particular, we give functions that require exponential size
              unweighted threshold circuits of depth 3 when we restrict the
              bottom fanin. We also prove that there are monotone functionsf k
              that can be computed in depthk and linear size ⋎, ⋏-circuits but
              require exponential size to compute by a depthk−1 monotone
              weighted threshold circuit.",
  journal  = "Comput. Complexity",
  volume   =  1,
  issue    =  2,
  pages    = "113--129",
  month    =  jun,
  year     =  1991,
  url      = "http://dx.doi.org/10.1007/BF01272517",
  keywords = "68Q15; 68Q99; Algorithm Analysis and Problem Complexity; Circuit
              complexity; Computational Mathematics and Numerical Analysis;
              lower bounds; monotone circuits; threshold circuits",
  issn     = "1016-3328",
  doi      = "10.1007/BF01272517",
  keywords = {DL}
}

@ARTICLE{Dikli2006-ag,
  title   = "An overview of automated scoring of essays",
  author  = "Dikli, Semire",
  journal = "The Journal of Technology, Learning and Assessment",
  volume  =  5,
  issue   =  1,
  year    =  2006,
  keywords = {AWA}
}

@INCOLLECTION{Bengio2006-bm,
  title     = "Neural Probabilistic Language Models",
  booktitle = "Innovations in Machine Learning",
  author    = "Bengio, Yoshua and Schwenk, Holger and Sen\'{e}cal,
               Jean-S\'{e}bastien and Morin, Fr\'{e}deric and Gauvain, Jean-Luc",
  editor    = "Holmes, Professor Dawn E. and Jain, Professor Lakhmi C.",
  abstract  = "A central goal of statistical language modeling is to learn the
               joint probability function of sequences of words in a language.
               This is intrinsically difficult because of the curse of
               dimensionality: a word sequence on which the model will be
               tested is likely to be different from all the word sequences
               seen during training. Traditional but very successful approaches
               based on n-grams obtain generalization by concatenating very
               short overlapping sequences seen in the training set. We propose
               to fight the curse of dimensionality by learning a distributed
               representation for words which allows each training sentence to
               inform the model about an exponential number of semantically
               neighboring sentences. Generalization is obtained because a
               sequence of words that has never been seen before gets high
               probability if it is made of words that are similar (in the
               sense of having a nearby representation) to words forming an
               already seen sentence. Training such large models (with millions
               of parameters) within a reasonable time is itself a significant
               challenge. We report on several methods to speed-up both
               training and probability computation, as well as comparative
               experiments to evaluate the improvements brought by these
               techniques. We finally describe the incorporation of this new
               language model into a state-of-the-art speech recognizer of
               conversational speech.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "137--186",
  series    = "Studies in Fuzziness and Soft Computing",
  month     =  jan,
  year      =  2006,
  url       = "http://link.springer.com/chapter/10.1007/3-540-33486-6_6",
  keywords  = "NLP",
  keywords = {DL}
}

@INPROCEEDINGS{Le2011-pr,
  title     = "Structured Output Layer neural network language model",
  booktitle = "2011 {IEEE} International Conference on Acoustics, Speech and
               Signal Processing ({ICASSP})",
  author    = "Le, Hai-Son and Oparin, I and Allauzen, A and Gauvain, J and
               Yvon, F",
  abstract  = "This paper introduces a new neural network language model (NNLM)
               based on word clustering to structure the output vocabulary:
               Structured Output Layer NNLM. This model is able to handle
               vocabularies of arbitrary size, hence dispensing with the design
               of short-lists that are commonly used in NNLMs. Several softmax
               layers replace the standard output layer in this model. The
               output structure depends on the word clustering which uses the
               continuous word representation induced by a NNLM. The GALE
               Mandarin data was used to carry out the speech-to-text
               experiments and evaluate the NNLMs. On this data the well tuned
               baseline system has a character error rate under 10\%. Our model
               achieves consistent improvements over the combination of an
               n-gram model and classical short-list NNLMs both in terms of
               perplexity and recognition accuracy.",
  pages     = "5524--5527",
  year      =  2011,
  url       = "http://dx.doi.org/10.1109/ICASSP.2011.5947610",
  keywords  = "NLP",
  doi       = "10.1109/ICASSP.2011.5947610",
  keywords = {DL}
}

@INPROCEEDINGS{Muller2011-ue,
  title     = "The Manifold Tangent Classifier",
  booktitle = "Advances in Neural Information Processing Systems 24",
  author    = "Muller, Xavier",
  year      =  2011,
  url       = "http://nips.cc/Conferences/2011/Program/event.php?ID=2837",
  keywords = {DL}
}

@ARTICLE{Burstein2003-ox,
  title   = "The e-rater scoring engine: Automated essay scoring with natural
             language processing",
  author  = "Burstein, Jill",
  journal = "Automated essay scoring: A cross-disciplinary perspective",
  pages   = "113--121",
  year    =  2003,
  keywords = {AWA}
}

@inproceedings{Rifai2011-ng,
    title={Contractive auto-encoders: Explicit invariance during feature extraction},
    author={Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
    booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
    pages={833--840},
    year={2011}
}

@INPROCEEDINGS{Montavon2012-ra,
  title  = "Deep Boltzmann machines as feed-forward hierarchies",
  author = "Montavon, Gr\'{e}goire and Braun, Mikio L and M{\"{u}}ller,
            Klaus-Robert",
  pages  = "798--804",
  year   =  2012,
  keywords = {DL}
}

@INPROCEEDINGS{Burstein2003-bw,
  title  = "{CriterionSM} Online Essay Evaluation: An Application for Automated
            Evaluation of Student Essays",
  author = "Burstein, Jill and Chodorow, Martin and Leacock, Claudia",
  pages  = "3--10",
  year   =  2003,
  keywords = {AWA}
}

@INPROCEEDINGS{Tieleman2008-da,
  title     = "Training restricted Boltzmann machines using approximations to
               the likelihood gradient",
  booktitle = "Proceedings of the 25th international conference on Machine
               learning",
  author    = "Tieleman, Tijmen",
  abstract  = "A new algorithm for training Restricted Boltzmann Machines is
               introduced. The algorithm, named Persistent Contrastive
               Divergence, is different from the standard Contrastive
               Divergence algorithms in that it aims to draw samples from
               almost exactly the model distribution. It is compared to some
               standard Contrastive Divergence and Pseudo-Likelihood algorithms
               on the tasks of modeling and classifying various types of data.
               The Persistent Contrastive Divergence algorithm outperforms the
               other algorithms, and is equally fast and simple.",
  publisher = "ACM",
  pages     = "1064--1071",
  year      =  2008,
  url       = "http://dx.doi.org/10.1145/1390156.1390290",
  keywords  = "boltzmann-machines; graphical-models; markov-chains;
               maximum-likelihood",
  doi       = "10.1145/1390156.1390290",
  keywords = {DL}
}

@ARTICLE{Shermis2010-ap,
  title     = "Automated essay scoring: Writing assessment and instruction",
  author    = "Shermis, MD and Burstein, J and Higgins, D and others",
  abstract  = "In 1973, the late Ellis Page and colleagues at the University of
               Connecticut programmed the first successful automated essay
               scoring engine,“Project Essay Grade (PEG)”(1973). The technology
               was foretold some six years earlier in a landmark Phi Delta
               Kappan article ...",
  journal   = "International",
  publisher = "mkzechner.net",
  year      =  2010,
  url       = "http://mkzechner.net/AES_IEE09.pdf",
  keywords = {AWA}
}

%@ARTICLE{Robbins1951-iy,
%  title    = "A Stochastic Approximation Method",
%  author   = "Robbins, Herbert and Monro, Sutton",
%  journal  = "Ann. Math. Stat.",
%  volume   =  22,
%  issue    =  3,
%  pages    = "400--407",
%  month    =  sep,
%  year     =  1951,
%  url      = "http://dx.doi.org/10.1214/aoms/1177729586",
%  keywords = "kindle; stochastic-algorithm",
%  issn     = "0003-4851",
%  doi      = "10.1214/aoms/1177729586"
%}

@INPROCEEDINGS{Pascanu2013-ji,
  title     = "On the difficulty of training recurrent neural networks",
  booktitle = "Proceedings of The 30th International Conference on Machine
               Learning",
  author    = "Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua",
  pages     = "1310--1318",
  year      =  2013,
  url       = "http://jmlr.org/proceedings/papers/v28/pascanu13.html",
  keywords = {DL}
}

@BOOK{Burstein2001-sh,
  title     = "Enriching automated essay scoring using discourse marking",
  author    = "Burstein, Jill and Kukich, Karen and Wolff, Susanne and Lu, Ji
               and Chodorow, Martin",
  publisher = "ERIC Clearinghouse",
  year      =  2001,
  keywords = {AWA}
}

@ARTICLE{Bridgeman2012-yu,
  title    = "Comparison of Human and Machine Scoring of Essays: Differences by
              Gender, Ethnicity, and Country",
  author   = "Bridgeman, Brent and Trapani, Catherine and Attali, Yigal",
  abstract = "Essay scores generated by machine and by human raters are
              generally comparable; that is, they can produce scores with
              similar means and standard deviations, and machine scores
              generally correlate as highly with human scores as scores from
              one human correlate with scores from another human. Although
              human and machine essay scores are highly related on average,
              this does not eliminate the possibility that machine and human
              scores may differ significantly for certain gender, ethnic, or
              country groups. Such differences were explored with essay data
              from two large-scale high-stakes testing programs: the Test of
              English as a Foreign Language and the Graduate Record
              Examination. Human and machine scores were very similar across
              most subgroups, but there were some notable exceptions. Policies
              were developed so that any differences between humans and
              machines would have a minimal impact on final reported scores.",
  journal  = "Applied Measurement in Education",
  volume   =  25,
  issue    =  1,
  pages    = "27--40",
  year     =  2012,
  url      = "http://dx.doi.org/10.1080/08957347.2012.635502",
  issn     = "0895-7347",
  doi      = "10.1080/08957347.2012.635502",
  keywords = {AWA}
}

@INPROCEEDINGS{Larochelle2008-tj,
  title    = "Classification using discriminative restricted boltzmann machines",
  author   = "Larochelle, Hugo and Bengio, Yoshua",
  abstract = "Recently, many applications for Restricted Boltzmann Machines
              (RBMs) have been developed for a large variety of learning
              problems. However, RBMs are usually used as feature extractors
              for another learning algorithm or to provide a good
              initialization for deep feed-forward neural network classifiers,
              and are not considered as a standalone solution to classification
              problems. In this paper, we argue that RBMs provide a
              self-contained framework for deriving competitive non-linear
              classifiers. We present an evaluation of different learning
              algorithms for RBMs which aim at introducing a discriminative
              component to RBM training and improve their performance as
              classifiers. This approach is simple in that RBMs are used
              directly to build a classifier, rather than as a stepping stone.
              Finally, we demonstrate how discriminative RBMs can also be
              successfully employed in a semi-supervised setting.",
  year     =  2008,
  keywords = {DL}
}

@article{Bengio2007-jn,
    title={Greedy layer-wise training of deep networks},
    author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo and others},
    journal={Advances in Neural Information Processing Systems},
    volume={19},
    pages={153},
    year={2007},
    publisher={MIT; 1998}
}

@ARTICLE{Condon2013-wd,
  title    = "Large-scale assessment, locally-developed measures, and automated
              scoring of essays: Fishing for red herrings?",
  author   = "Condon, William",
  abstract = "Automated Essay Scoring (AES) has garnered a great deal of
              attention from the rhetoric and composition/writing studies
              community since the Educational Testing Service began using
              e-rater® and the Criterion® Online Writing Evaluation Service as
              products in scoring writing tests, and most of the responses have
              been negative. While the criticisms leveled at AES are
              reasonable, the more important, underlying issues relate to the
              aspects of the writing construct of the tests AES can rate.
              Because these tests underrepresent the construct as it is
              understood by the writing community, such tests should not be
              used in writing assessment, whether for admissions, placement,
              formative, or achievement testing. Instead of continuing the
              traditional, large-scale, commercial testing enterprise
              associated with AES, we should look to well-established,
              institutionally contextualized forms of assessment as models that
              yield fuller, richer information about the student's control of
              the writing construct. Such tests would be more valid, as
              reliable, and far fairer to the test-takers, whose stakes are
              often quite high.",
  journal  = "Assessing Writing",
  volume   =  18,
  issue    =  1,
  pages    = "100--108",
  month    =  jan,
  year     =  2013,
  url      = "http://dx.doi.org/10.1016/j.asw.2012.11.001",
  keywords = "Automated Essay Scoring; Construct representation; Large-scale
              testing; Locally-developed assessment; Validation",
  issn     = "1075-2935",
  doi      = "10.1016/j.asw.2012.11.001",
  keywords = {AWA}
}

@MISC{noauthor_undated-ll,
  title        = "Winners - The Hewlett Foundation: Short Answer Scoring |
                  Kaggle",
  abstract     = "Kaggle is a platform for data prediction competitions.
                  Companies, organizations and researchers post their data and
                  have it scrutinized by the world's best statisticians.",
  url          = "http://www.kaggle.com/c/asap-sas/details/winners",
  howpublished = "\url{http://www.kaggle.com/c/asap-sas/details/winners}",
  note         = "Accessed: 2013-10-7",
  keywords = {AWA}
}

@ARTICLE{Burstein2004-ki,
  title   = "Automated essay evaluation: the criterion online writing service",
  author  = "Burstein, Jill and Chodorow, Martin and Leacock, Claudia",
  journal = "AI Magazine",
  volume  =  25,
  issue   =  3,
  year    =  2004,
  keywords = {AWA}
}

@MISC{noauthor_undated-za,
  title        = "Framework for Success in Postsecondary Writing | Council of
                  Writing Program Administrators",
  url          = "http://wpacouncil.org/framework",
  howpublished = "\url{http://wpacouncil.org/framework}",
  note         = "Accessed: 2013-10-7",
  keywords = {AWA}
}

@INPROCEEDINGS{Ng2009-tu,
  title     = "Measuring Invariances in Deep Networks",
  booktitle = "Advances in Neural Information Processing Systems 22",
  author    = "Ng, Andrew",
  year      =  2009,
  url       = "http://nips.cc/Conferences/2009/Program/event.php?ID=1633",
  keywords = {DL}
}

@ARTICLE{Duchi2011-xb,
  title    = "Adaptive Subgradient Methods for Online Learning and Stochastic
              Optimization",
  author   = "Duchi, John and Hazan, Elad and Singer, Yoram",
  abstract = "We present a new family of subgradient methods that dynamically
              incorporate knowledge of the geometry of the data observed in
              earlier iterations to perform more informative gradient-based
              learning. Metaphorically, the adaptation allows us to find
              needles in haystacks in the form of very predictive but rarely
              seen features. Our paradigm stems from recent advances in
              stochastic optimization and online learning which employ proximal
              functions to control the gradient steps of the algorithm. We
              describe and analyze an apparatus for adaptively modifying the
              proximal function, which significantly simplifies setting a
              learning rate and results in regret guarantees that are provably
              as good as the best proximal function that can be chosen in
              hindsight. We give several efficient algorithms for empirical
              risk minimization problems with common and important
              regularization functions and domain constraints. We
              experimentally study our theoretical analysis and show that
              adaptive subgradient methods outperform state-of-the-art, yet
              non-adaptive, subgradient algorithms.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  12,
  pages    = "2121--2159",
  month    =  jul,
  year     =  2011,
  url      = "http://dl.acm.org/citation.cfm?id=1953048.2021068",
  issn     = "1532-4435",
  keywords = {DL}
}

@BOOK{Cohen2003-rd,
  title     = "The effect of specific language features on the complexity of
               systems for automated essay scoring",
  author    = "Cohen, Yoav and Ben-Simon, Anat and Hovav, Myra",
  publisher = "National Institute for Testing \& Evaluation",
  year      =  2003,
  keywords = {AWA}
}

@ARTICLE{Bengio2012-vp,
  title    = "Implicit Density Estimation by Local Moment Matching to Sample
              from {Auto-Encoders}",
  author   = "Bengio, Yoshua and Alain, Guillaume and Rifai, Salah",
  journal  = "CoRR",
  volume   = "abs/1207.0057",
  year     =  2012,
  url      = "http://dblp.uni-trier.de/db/journals/corr/corr1207.html#abs-1207-0057",
  keywords = "dblp",
  keywords = {DL}
}

@INCOLLECTION{Bengio2012-ve,
  title     = "Practical Recommendations for {Gradient-Based} Training of Deep
               Architectures",
  booktitle = "Neural Networks: Tricks of the Trade",
  author    = "Bengio, Yoshua",
  editor    = "Montavon, Gr\'{e}goire and Orr, Genevi\`{e}ve B. and
               M{\"{u}}ller, Klaus-Robert",
  abstract  = "Learning algorithms related to artificial neural networks and in
               particular for Deep Learning may seem to involve many bells and
               whistles, called hyper-parameters. This chapter is meant as a
               practical guide with recommendations for some of the most
               commonly used hyperparameters, in particular in the context of
               learning algorithms based on back-propagated gradient and
               gradient-based optimization. It also discusses how to deal with
               the fact that more interesting results can be obtained when
               allowing one to adjust many hyper-parameters. Overall, it
               describes elements of the practice used to successfully and
               efficiently train and debug large-scale and often deep
               multi-layer neural networks. It closes with open questions about
               the training difficulties observed with deeper architectures.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "437--478",
  series    = "Lecture Notes in Computer Science",
  month     =  jan,
  year      =  2012,
  url       = "http://link.springer.com/chapter/10.1007/978-3-642-35289-8_26",
  keywords  = "Algorithm Analysis and Problem Complexity; Artificial
               Intelligence (incl. Robotics); Complexity; Computation by
               Abstract Devices; Information Systems Applications (incl.
               Internet); Pattern Recognition",
  keywords = {DL}
}

@MISC{noauthor_undated-av,
  title        = "Can You Trust Automated Grading? | Inside Higher Ed",
  url          = "http://www.insidehighered.com/news/2011/02/21/debate_over_reliability_of_automated_essay_grading",
  howpublished = "\url{http://www.insidehighered.com/news/2011/02/21/debate_over_reliability_of_automated_essay_grading}",
  note         = "Accessed: 2013-10-7",
  keywords = {AWA}
}

@ARTICLE{Bengio2012-nf,
  title   = "Better Mixing via Deep Representations",
  author  = "Bengio, Yoshua and Mesnil, Gr\'{e}goire and Dauphin, Yann and
             Rifai, Salah",
  journal = "CoRR",
  volume  = "abs/1207.4404",
  year    =  2012,
  url     = "http://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1207-4404",
  keywords = {DL}
}

@INPROCEEDINGS{Hinton1986-tt,
  title  = "Learning distributed representations of concepts",
  author = "Hinton, Geoffrey E",
  pages  = "1--12",
  year   =  1986,
  keywords = {DL}
}

@ARTICLE{Cheville2004-yg,
  title   = "Automated Scoring Technologies and the Rising Influence of Error",
  author  = "Cheville, Julie",
  journal = "The English Journal",
  volume  =  93,
  issue   =  4,
  pages   = "47--52",
  month   =  mar,
  year    =  2004,
  url     = "http://dx.doi.org/10.2307/4128980",
  issn    = "0013-8274",
  doi     = "10.2307/4128980",
  keywords = {AWA}
}

@INPROCEEDINGS{Weston2008-xv,
  title     = "Deep learning via semi-supervised embedding",
  author    = "Weston, Jason and Ratle, Fr\'{e}d\'{e}ric and Collobert, Ronan",
  abstract  = "We show how nonlinear embedding algorithms popular for use with
               shallow semi-supervised learning techniques such as kernel
               methods can be applied to deep multilayer architectures, either
               as a regularizer at the output layer, or on each layer of the
               architecture. This provides a simple alternative to existing
               approaches to deep learning whilst yielding competitive error
               rates compared to those methods, and existing shallow
               semi-supervised techniques.",
  publisher = "ACM",
  pages     = "1168--1175",
  series    = "ICML '08",
  year      =  2008,
  url       = "http://dx.doi.org/10.1145/1390156.1390303",
  doi       = "10.1145/1390156.1390303",
  keywords = {DL}
}

@INPROCEEDINGS{Williams2001-ld,
  title  = "Automated essay grading: An evaluation of four conceptual models",
  author = "Williams, Robert",
  pages  = "7--9",
  year   =  2001,
  keywords = {AWA}
}

@INPROCEEDINGS{Kgl2011-uh,
  title     = "Algorithms for {Hyper-Parameter} Optimization",
  booktitle = "Advances in Neural Information Processing Systems 24",
  author    = "K\'{e}gl, Bal\'{a}zs",
  year      =  2011,
  url       = "http://nips.cc/Conferences/2011/Program/event.php?ID=2579",
  keywords = {DL}
}

@inproceedings{Mikolov2013-hm,
    title={Linguistic Regularities in Continuous Space Word Representations.},
    author={Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
    booktitle={HLT-NAACL},
    pages={746--751},
    year={2013}
}

@ARTICLE{Foltz1999-mn,
  title    = "The Intelligent Essay Assessor: Applications to Educational
              Technology",
  author   = "Foltz, P W and Laham, D and Landauer, T K",
  journal  = "Interactive Multimedia Education Journal of Computer enhanced
              learning On-line journal",
  volume   = "1(2)",
  year     =  1999,
  keywords = {AWA}
}

@INCOLLECTION{Mikolov2013-us,
  title     = "Distributed Representations of Words and Phrases and their
               Compositionality",
  booktitle = "Advances in Neural Information Processing Systems 26",
  author    = "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
               Greg S and Dean, Jeff",
  editor    = "Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and
               Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  pages     = "3111--3119",
  year      =  2013,
  url       = "http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality",
  keywords = {DL}
}

@UNPUBLISHED{Bengio2014-rh,
  title         = "How {Auto-Encoders} Could Provide Credit Assignment in Deep
                   Networks via Target Propagation",
  author        = "Bengio, Yoshua",
  abstract      = "In this paper we propose to exploit \{\textbackslash{}em
                   reconstruction\} as a layer-local training signal for deep
                   learning, be it generative or discriminant, single or
                   multi-modal, supervised, semi-supervised or unsupervised,
                   feedforward or recurrent. Reconstructions can be propagated
                   in a form of target propagation playing a role similar to
                   back-propagation but helping to reduce the reliance on
                   back-propagation in order to perform credit assignment
                   across many levels of possibly strong non-linearities (which
                   is difficult for back-propagation). A regularized
                   auto-encoder tends produce a reconstruction that is a more
                   likely version of its input, i.e., a small move in the
                   direction of higher likelihood. By generalizing gradients,
                   target propagation may also allow to train deep networks
                   with discrete hidden units. If the auto-encoder takes both a
                   representation of input and target (or of any side
                   information) in input, then its reconstruction of input
                   representation provides a target towards a representation
                   that is more likely, conditioned on all the side
                   information. A deep auto-encoder decoding path generalizes
                   gradient propagation in a learned way that can thus handle
                   not just infinitesimal changes but larger, discrete changes,
                   hopefully allowing credit assignment through a long chain of
                   non-linear operations. For this to work, each layer must be
                   a good denoising or regularized auto-encoder itself. In
                   addition to each layer being a good auto-encoder, the
                   encoder also learns to please the upper layers by
                   transforming the data into a space where it is easier to
                   model by them, flattening manifolds and disentangling
                   factors. The motivations and theoretical justifications for
                   this approach are laid down in this paper, along with
                   conjectures that will have to be verified either
                   mathematically or experimentally.",
  journal       = "arXiv [cs.LG]",
  month         =  "29~" # jul,
  year          =  2014,
  url           = "http://arxiv.org/abs/1407.7906",
  archivePrefix = "arXiv",
  eprint        = "1407.7906",
  primaryClass  = "cs.LG",
  arxivid       = "1407.7906",
  keywords = {DL}
}

@ARTICLE{Salakhutdinov2009-ik,
  title    = "Semantic hashing - Special Section on Graphical Models and
              Information Retrieval",
  author   = "Salakhutdinov, Ruslan and Hinton, Geoffrey",
  abstract = "We show how to learn a deep graphical model of the word-count
              vectors obtained from a large set of documents. The values of the
              latent variables in the deepest layer are easy to infer and give
              a much better representation of each document than Latent
              Semantic Analysis. When the deepest layer is forced to use a
              small number of binary variables (e.g. 32), the graphical model
              performs “semantic hashing”: Documents are mapped to memory
              addresses in such a way that semantically similar documents are
              located at nearby addresses. Documents similar to a query
              document can then be found by simply accessing all the addresses
              that differ by only a few bits from the address of the query
              document. This way of extending the efficiency of hash-coding to
              approximate matching is much faster than locality sensitive
              hashing, which is the fastest current method. By using semantic
              hashing to filter the documents given to TF-IDF, we achieve
              higher accuracy than applying TF-IDF to the entire document set.",
  journal  = "Int. J. Approx. Reason.",
  volume   =  50,
  number   =  7,
  pages    = "969--978",
  month    =  jul,
  year     =  2009,
  url      = "http://dx.doi.org/10.1016/j.ijar.2008.11.006",
  issn     = "0888-613X",
  doi      = "10.1016/j.ijar.2008.11.006",
  keywords = {DL}
}

@ARTICLE{Mikolov2010-sa,
  title    = "Recurrent neural network based language model",
  author   = "Mikolov, Tomas and Karafi\'{a}t, Martin and Burget, Luk\'{a}s and
              Cernocky, Jan and Khudanpur, Sanjeev",
  journal  = "Proceedings of Interspeech",
  year     =  2010,
  keywords = "NLP;Language Models;Distributional Semantics;Machine
              Learning/Neural Networks;Natural Language Processing",
  keywords = {DL}
}

@ARTICLE{Rumelhart1986-wl,
  title   = "Parallel distributed processing, volume 1: Foundations",
  author  = "Rumelhart, D E and McClelland, J L and Research, Pdp",
  journal = "MIT Press, Cambridge, MA",
  year    =  1986,
  keywords = {DL}
}

@ARTICLE{Leacock2003-qg,
  title    = "C-rater: Automated scoring of short-answer questions",
  author   = "Leacock, Claudia and Chodorow, Martin",
  journal  = "Comput. Hum.",
  volume   =  37,
  number   =  4,
  pages    = "389--405",
  year     =  2003,
  keywords = "Automated Essay Scoring",
  issn     = "0010-4817",
  keywords = {AWA}
}

@UNPUBLISHED{Le2014-pb,
  title         = "Distributed Representations of Sentences and Documents",
  author        = "Le, Quoc V and Mikolov, Tomas",
  abstract      = "Many machine learning algorithms require the input to be
                   represented as a fixed-length feature vector. When it comes
                   to texts, one of the most common fixed-length features is
                   bag-of-words. Despite their popularity, bag-of-words
                   features have two major weaknesses: they lose the ordering
                   of the words and they also ignore semantics of the words.
                   For example, {"}powerful,{"} {"}strong{"} and {"}Paris{"}
                   are equally distant. In this paper, we propose Paragraph
                   Vector, an unsupervised algorithm that learns fixed-length
                   feature representations from variable-length pieces of
                   texts, such as sentences, paragraphs, and documents. Our
                   algorithm represents each document by a dense vector which
                   is trained to predict words in the document. Its
                   construction gives our algorithm the potential to overcome
                   the weaknesses of bag-of-words models. Empirical results
                   show that Paragraph Vectors outperform bag-of-words models
                   as well as other techniques for text representations.
                   Finally, we achieve new state-of-the-art results on several
                   text classification and sentiment analysis tasks.",
  journal       = "arXiv [cs.CL]",
  month         =  "16~" # may,
  year          =  2014,
  url           = "http://arxiv.org/abs/1405.4053",
  keywords      = "Deep Learning;Information Retrieval;Machine Learning/Neural
                   Networks;Natural Language Processing",
  archivePrefix = "arXiv",
  eprint        = "1405.4053",
  primaryClass  = "cs.CL",
  arxivid       = "1405.4053",
  keywords = {DL}
}

@ARTICLE{Landauer2003-vm,
  title    = "Automatic essay assessment",
  author   = "Landauer, Thomas K and Laham, Darrell and Foltz, Peter",
  journal  = "Assessment in Education: Principles, Policy and Practice",
  volume   =  10,
  number   =  3,
  pages    = "295--308",
  year     =  2003,
  keywords = "Principal Components Analysis;Singular Value Decomposition;Active
              learning and automated essay scoring bibliography;Automated Essay
              Scoring",
  keywords = {AWA}
}

@INPROCEEDINGS{Socher2011-lm,
  title     = "Semi-supervised Recursive Autoencoders for Predicting Sentiment
               Distributions",
  booktitle = "Proceedings of the Conference on Empirical Methods in Natural
               Language Processing",
  author    = "Socher, Richard and Pennington, Jeffrey and Huang, Eric H and
               Ng, Andrew Y and Manning, Christopher D",
  abstract  = "Abstract We introduce a novel machine learning framework based
               on recursive autoencoders for sentence-level prediction of
               sentiment label distributions . Our method learns vector space
               representations for multi-word phrases. In sentiment prediction
               tasks ...",
  journal   = "Proceedings of the",
  publisher = "Association for Computational Linguistics",
  pages     = "151--161",
  series    = "EMNLP '11",
  year      =  2011,
  url       = "http://dl.acm.org/citation.cfm?id=2145432.2145450",
  address   = "Stroudsburg, PA, USA",
  keywords = {DL}
}

@ARTICLE{Page1968-pl,
  title     = "{PROJECT} {ESSAY} {GRADE-A} {FORTRAN} {PROGRAM} {FOR}
               {STATISTICAL} {ANALYSIS} {OF} {PROSE}",
  author    = "Page, E B and Fisher, G A and Fisher, M A",
  journal   = "STATISTICAL …",
  publisher = "BRITISH PSYCHOLOGICAL SOC ST …",
  year      =  1968,
  keywords = {AWA}
}

@INPROCEEDINGS{Morin2005-ho,
  title    = "Hierarchical probabilistic neural network language model",
  author   = "Morin, Frederic and Bengio, Yoshua",
  abstract = "In recent years, variants of a neural network architecture for
              statistical language modeling have been proposed and successfully
              applied, e.g. in the language modeling component of speech
              recognizers. The main advantage of these architectures is that
              they learn an embedding for words (or other symbols) in a
              continuous space that helps to smooth the language model and
              provide good generalization even when the number of training
              examples is insufficient. However, these models are extremely
              slow in comparison to the more commonly used n-gram models, both
              for training and recognition. As an alternative to an importance
              sampling method proposed to speed-up training, we introduce a
              hierarchical decomposition of the conditional probabilities that
              yields a speed-up of about 200 both during training and
              recognition. The hierarchical decomposition is a binary
              hierarchical clustering constrained by the prior knowledge
              extracted from the WordNet semantic hierarchy.",
  pages    = "246--252",
  year     =  2005,
  keywords = {DL}
}

@ARTICLE{Blei2003-md,
  title     = "Latent dirichlet allocation",
  author    = "Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  3,
  pages     = "993--1022",
  month     =  "1~" # mar,
  year      =  2003,
  url       = "http://dl.acm.org/citation.cfm?id=944937",
  issn      = "1532-4435",
  keywords = {DL}
}

@INPROCEEDINGS{Hofmann1999-qb,
  title     = "Probabilistic Latent Semantic Indexing",
  booktitle = "Proceedings of the 22Nd Annual International {ACM} {SIGIR}
               Conference on Research and Development in Information Retrieval",
  author    = "Hofmann, Thomas",
  abstract  = "Abstract Probabilistic Latent Semantic Indexing is a novel
               approach to automated document indexing which is based on a
               statistical latent class model for factor analysis of count
               data. Fitted from a training corpus of text documents by a
               generalization of the Expectation ...",
  journal   = "Proceedings of the 22nd annual international ACM",
  publisher = "ACM",
  pages     = "50--57",
  series    = "SIGIR '99",
  year      =  1999,
  url       = "http://dx.doi.org/10.1145/312624.312649",
  address   = "New York, NY, USA",
  doi       = "10.1145/312624.312649",
  keywords = {DL}
}

@ARTICLE{Attali2006-nz,
  title    = "Automated essay scoring with e-rater® V. 2",
  author   = "Attali, Yigal and Burstein, Jill",
  journal  = "The Journal of Technology, Learning and Assessment",
  volume   =  4,
  number   =  3,
  year     =  2006,
  keywords = {AWA}
}

@BOOK{Diamantaras1996-wy,
  title     = "Principal Component Neural Networks: Theory and Applications",
  author    = "Diamantaras, K I and Kung, S Y",
  publisher = "Wiley",
  series    = "A Wiley-Interscience publication",
  year      =  1996,
  url       = "http://books.google.com/books?id=mKZQAAAAMAAJ",
  keywords  = "Principal Components Analysis;Singular Value Decomposition",
  isbn      = "9780471054368",
  lccn      = "lc95000242",
  keywords = {DL}
}

@ARTICLE{Bengio2003-lk,
  title    = "A Neural Probabilistic Language Model",
  author   = "Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and
              Jauvin, Christian",
  abstract = "A goal of statistical language modeling is to learn the joint
              probability function of sequences of words in a language. This is
              intrinsically difficult because of the curse of dimensionality: a
              word sequence on which the model will be tested is likely to be
              different from all the word sequences seen during training.
              Traditional but very successful approaches based on n-grams
              obtain generalization by concatenating very short overlapping
              sequences seen in the training set. We propose to fight the curse
              of dimensionality by learning a distributed representation for
              words which allows each training sentence to inform the model
              about an exponential number of semantically neighboring
              sentences. The model learns simultaneously (1) a distributed
              representation for each word along with (2) the probability
              function for word sequences, expressed in terms of these
              representations. Generalization is obtained because a sequence of
              words that has never been seen before gets high probability if it
              is made of words that are similar (in the sense of having a
              nearby representation) to words forming an already seen sentence.
              Training such large models (with millions of parameters) within a
              reasonable time is itself a significant challenge. We report on
              experiments using neural networks for the probability function,
              showing on two text corpora that the proposed approach
              significantly improves on state-of-the-art n-gram models, and
              that the proposed approach allows to take advantage of longer
              contexts.",
  journal  = "Journal of Machine Learning Research.",
  volume   =  3,
  pages    = "1137--1155",
  year     =  2003,
  issn     = "1532-4435",
  keywords = {DL}
}

@ARTICLE{Mikolov2013-vo,
  title    = "Efficient Estimation of Word Representations in Vector Space",
  author   = "Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey",
  journal  = "CoRR",
  volume   = "abs/1301.3781",
  year     =  2013,
  url      = "http://arxiv.org/abs/1301.3781",
  keywords = "NLP;Language Models;Distributional Semantics;Machine
              Learning/Neural Networks",
  keywords = {DL}
}

@INPROCEEDINGS{Dauphin2011-rw,
  title     = "{Large-Scale} Learning of Embeddings with Reconstruction
               Sampling",
  booktitle = "{ICML}",
  author    = "Dauphin, Yann and Glorot, Xavier and Bengio, Yoshua",
  year      =  2011,
  url       = "http://www.icml-2011.org/papers/491_icmlpaper.pdf",
  keywords = {DL}
}

@article{JeffreyPennington2014-fc,
    title={GloVe: Global vectors for word representation},
    author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
    journal={Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014)},
    volume={12},
    year={2014}
}

@inproceedings{Mnih2008-nf,
    title={A scalable hierarchical distributed language model},
    author={Mnih, Andriy and Hinton, Geoffrey E},
    booktitle={Advances in neural information processing systems},
    pages={1081--1088},
    year={2009}
}

@article{Deerwester1990-yp,
    title={Indexing by latent semantic analysis},
    author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
    journal={Journal of the American Society for Information Science},
    year={1990}
}

@ARTICLE{McClelland1986-iz,
  title   = "Parallel distributed processing: explorations in the
             microstructures of cognition, volume 2: psychological and
             biological models",
  author  = "McClelland, J L and Rumelhart, D E and Research, Pdp",
  journal = "MIT Press",
  year    =  1986,
  keywords = {DL}
}

@UNPUBLISHED{Scheible2013-on,
  title         = "Cutting Recursive Autoencoder Trees",
  author        = "Scheible, Christian and Schuetze, Hinrich",
  abstract      = "Deep Learning models enjoy considerable success in Natural
  Language Processing. While deep architectures produce useful
  representations that lead to improvements in various tasks,
  they are often difficult to interpret. This makes the
  analysis of learned structures particularly difficult. In
  this paper, we rely on empirical tests to see whether a
  particular structure makes sense. We present an analysis of
  the Semi-Supervised Recursive Autoencoder, a well-known
  model that produces structural representations of text. We
  show that for certain tasks, the structure of the
  autoencoder can be significantly reduced without loss of
  classification accuracy and we evaluate the produced
  structures using human judgment.",
  journal       = "arXiv [cs.CL]",
  month         =  01,
  year          =  2013,
  url           = "http://arxiv.org/abs/1301.2811",
  archivePrefix = "arXiv",
  eprint        = "1301.2811",
  primaryClass  = "cs.CL",
  arxivid       = "1301.2811",
  keywords = {DL}
}



@INPROCEEDINGS{Iyyer2014-hi,
  title     = "A Neural Network for Factoid Question Answering over Paragraphs",
  booktitle = "Conference on Empirical Methods in Natural Language Processing",
  author    = "Iyyer, M and Boyd-Graber, J and Claudino, L and Socher, R and Iii, H D",
  abstract  = "Abstract Text classification methods for tasks like
              factoid question answering typically use manually defined string
              matching rules or bag of words representations. These methods are
              ineffective when question text contains very few individual words
              (eg, named entities) that ...",
  year      =  2014,
  url       = "http://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf",
  keywords = {DL}
}

@UNPUBLISHED{Zaremba2014-bs,
  title         = "Recurrent Neural Network Regularization",
  author        = "Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol",
  abstract      = "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, and machine translation.",
  journal       = "arXiv [cs.NE]",
  month         =  "8~" # sep,
  year          =  2014,
  url           = "http://arxiv.org/abs/1409.2329",
  archivePrefix = "arXiv",
  eprint        = "1409.2329",
  primaryClass  = "cs.NE",
  arxivid       = "1409.2329"
}

@INCOLLECTION{Martens2012-qy,
  title     = "Training Deep and Recurrent Networks with {Hessian-Free} Optimization",
  booktitle = "Neural Networks: Tricks of the Trade",
  author    = "Martens, James and Sutskever, Ilya",
  abstract  = "In this chapter we will first describe the basic HF approach,
  and then examine well-known performance-improving techniques
  such as preconditioning which we have found to be beneficial for
  neural network training, as well as others of a more heuristic
  nature which are harder to justify, but which we have found to
  work well in practice. We will also provide practical tips for
  creating efficient and bug-free implementations and discuss
  various pitfalls which may arise when designing and using an
  HF-type approach in a particular application.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "479--535",
  series    = "Lecture Notes in Computer Science",
  month     =  "1~" # jan,
  year      =  2012,
  url       = "http://dx.doi.org/10.1007/978-3-642-35289-8_27",
  isbn      = "9783642352881",
  isbn_alt  = "9783642352898",
  doi       = "10.1007/978-3-642-35289-8\_27"
}

@ARTICLE{Sutskever2011-kp,
  title     = "Generating text with recurrent neural networks",
  author    = "Sutskever, I and Martens, J and {others}",
  abstract  = "Abstract Recurrent Neural Networks (RNNs) are very powerful
  sequence models that do not enjoy widespread use because it is
  extremely difficult to train them properly. Fortunately, recent
  advances in Hessian-free optimization have been able to overcome
  the difficulties ...",
  journal   = "Proceedings of the",
  publisher = "machinelearning.wustl.edu",
  year      =  2011,
  url       = "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf"
}

@ARTICLE{Martens2011-nf,
  title     = "Learning recurrent neural networks with hessian-free
  optimization",
  author    = "Martens, J and Sutskever, I",
  abstract  = "Abstract In this work we resolve the long-outstanding problem of
  how to effectively train recurrent neural networks (RNNs) on
  complex and difficult sequence modeling problems which may
  contain long-term data dependencies. Utilizing recent advances
  in the Hessian ...",
  journal   = "on Machine Learning (ICML-11 …",
  publisher = "machinelearning.wustl.edu",
  year      =  2011,
  url       = "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf"
}

@UNPUBLISHED{Memisevic2014-so,
  title         = "Zero-bias autoencoders and the benefits of co-adapting features",
  author        = "Memisevic, Roland and Konda, Kishore and Krueger, David",
  abstract      = "We show that training common regularized autoencoders resembles clustering, because it amounts to fitting a density model whose mass is concentrated in the directions of the individual weight vectors. We then propose a new activation function based on thresholding a linear function with zero bias (so it is truly linear not affine), and argue that this allows hidden units to ``collaborate'' in order to define larger regions of uniform density. We show that the new activation function makes it possible to train autoencoders without an explicit regularization penalty, such as sparsification, contraction or denoising, by simply minimizing reconstruction error. Experiments in a variety of recognition tasks show that zero-bias autoencoders perform about on par with common regularized autoencoders on low dimensional data and outperform these by an increasing margin as the dimensionality of the data increases.",
  journal       = "arXiv [stat.ML]",
  month         =  02,
  year          =  2014,
  url           = "http://arxiv.org/abs/1402.3337",
  archivePrefix = "arXiv",
  eprint        = "1402.3337",
  primaryClass  = "stat.ML",
  arxivid       = "1402.3337"
}

@INCOLLECTION{Sutskever2014-gy,
    title     = "Sequence to Sequence Learning with Neural Networks",
    booktitle = "Advances in Neural Information Processing Systems 27",
    author    = "Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V V",
    editor    = "Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q",
    abstract  = "... 1). There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks . ... [2]. The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks ...",
    publisher = "Curran Associates, Inc.",
    pages     = "3104--3112",
    year      =  2014,
    url       = "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf",
    issn      = "1049-5258"
}

@article{LeCun1998-ku,
    title={Gradient-based learning applied to document recognition},
    author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
    journal={Proceedings of the IEEE},
    volume={86},
    number={11},
    pages={2278--2324},
    year={1998},
    publisher={IEEE}
}

@INPROCEEDINGS{Bengio2009-ua,
    title     = "Curriculum Learning",
    booktitle = "Proceedings of the 26th Annual International Conference on Machine Learning",
    author    = "Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason",
    abstract  = "Abstract Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the ...",
    publisher = "ACM",
    pages     = "41--48",
    series    = "ICML '09",
    year      =  2009,
    url       = "http://doi.acm.org/10.1145/1553374.1553380",
    address   = "New York, NY, USA",
    doi       = "10.1145/1553374.1553380"
}

@ARTICLE{An1996-ez,
title    = "The Effects of Adding Noise During Backpropagation Training on a
Generalization Performance",
author   = "An, Guozhong",
abstract = "We study the effects of adding noise to the inputs, outputs,
weight connections, and weight changes of multilayer feedforward
neural networks during backpropagation training. We rigorously
derive and analyze the objective functions that are minimized by
the noise-affected training processes. We show that input noise
and weight noise encourage the neural-network output to be a
smooth function of the input or its weights, respectively. In the
weak-noise limit, noise added to the output of the neural
networks only changes the objective function by a constant.
Hence, it cannot improve generalization. Input noise introduces
penalty terms in the objective function that are related to, but
distinct from, those found in the regularization approaches.
Simulations have been performed on a regression and a
classification problem to further substantiate our analysis.
Input noise is found to be effective in improving the
generalization performance for both problems. However, weight
noise is found to be effective in improving the generalization
performance only for the classification problem. Other forms of
noise have practically no effect on generalization.",
journal  = "Neural Comput.",
volume   =  8,
number   =  3,
pages    = "643--674",
month    =  apr,
year     =  1996,
url      = "http://dx.doi.org/10.1162/neco.1996.8.3.643",
issn     = "0899-7667",
doi      = "10.1162/neco.1996.8.3.643"
}

@article{Hammadi1998-kf,
    title={Improving the performance of feedforward neural networks by noise injection into hidden neurons},
    author={Hammadi, Nait Charif and Ito, Hideo},
    journal={Journal of Intelligent and Robotic Systems},
    volume={21},
    number={2},
    pages={103--115},
    year={1998},
    publisher={Springer}
}

@article{Grandvalet1997-qc,
    title={Noise injection: Theoretical prospects},
    author={Grandvalet, Yves and Canu, St{\'e}phane and Boucheron, St{\'e}phane},
    journal={Neural Computation},
    volume={9},
    number={5},
    pages={1093--1108},
    year={1997},
    publisher={MIT Press}
}

@article{Grandvalet1995-tu,
    title={Comments on ``Noise injection into inputs in back propagation learning''},
    author={Grandvalet, Yves and Canu, St{\'e}phane},
    journal={Systems, Man and Cybernetics, IEEE Transactions on},
    volume={25},
    number={4},
    pages={678--681},
    year={1995},
    publisher={IEEE}
}

@article{Reed1995-nh,
    title={Similarities of error regularization, sigmoid gain scaling, target smoothing, and training with jitter},
    author={Reed, Russell and Marks, RJ and Oh, Seho and others},
    journal={Neural Networks, IEEE Transactions on},
    volume={6},
    number={3},
    pages={529--538},
    year={1995},
    publisher={IEEE}
}

@article{Holmstrom1992-tm,
    title={Using additive noise in back-propagation training},
    author={HolmstrOm, Lasse and Koistinen, Petri},
    journal={Neural Networks, IEEE Transactions on},
    volume={3},
    number={1},
    pages={24--38},
    year={1992},
    publisher={IEEE}
}

@article{Sietsma1991-ks,
    title={Creating artificial neural networks that generalize},
    author={Sietsma, Jocelyn and Dow, Robert JF},
    journal={Neural networks},
    volume={4},
    number={1},
    pages={67--79},
    year={1991},
    publisher={Elsevier}
}

@incollection{Masci2011-hh,
    title={Stacked convolutional auto-encoders for hierarchical feature extraction},
    author={Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
    booktitle={Artificial Neural Networks and Machine Learning--ICANN 2011},
    pages={52--59},
    year={2011},
    publisher={Springer}
}

@inproceedings{Zeiler2010-gf,
    title={Deconvolutional networks},
    author={Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Robert},
    booktitle={Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
    pages={2528--2535},
    year={2010},
    organization={IEEE}
}

@article{Okanohara2007-zp,
    title={A Discriminative Language Model with Pseudo-Negative Samples},
    author={Okanohara, Daisuke and Tsujii, Jun’ichi},
    booktitle={Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics},
    pages={73},
    year={2007}
}

@ARTICLE{Kim2014-vy,
    title = {Convolutional Neural Networks for Sentence Classification},
    author = {Kim, Yoon},
    journal = {Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014)},
    volume={12},
    year          =  2014,
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dos2014-ti,
    title     = "Deep convolutional neural networks for sentiment analysis of short texts",
    author    = "Dos, C N and Gatti, M",
    abstract  = "Abstract Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain.  Effectively solving this task requires strategies that combine the small text content with ...",
    journal   = "of the 25th International Conference on …",
    publisher = "aclweb.org",
    year      =  2014,
    url       = "http://www.aclweb.org/anthology/C14-1008"
}

@inproceedings{Kalchbrenner2014-tl,
    title={A convolutional neural network for modelling sentences},
    author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
    booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
    year={2014},
    organization={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics}
}

@misc{Kalchbrenner2014-code,
    title={Code for ``A convolutional neural network for modelling sentences``},
    author={Kalchbrenner, Nal},
    url={http://nal.co/DCNN}
}

@article{Denil2014-dx,
    title={Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network},
    author={Denil, Misha and Demiraj, Alban and Kalchbrenner, Nal and Blunsom, Phil and de Freitas, Nando},
    journal={arXiv preprint arXiv:1406.3830},
    year={2014}
}

@article{Johnson2014-ol,
    title={Effective Use of Word Order for Text Categorization with Convolutional Neural Networks},
    author={Johnson, Rie and Zhang, Tong},
    journal={arXiv preprint arXiv:1412.1058},
    year={2014}
}

@article{Kalchbrenner2013-kz,
    title={Recurrent convolutional neural networks for discourse compositionality},
    author={Kalchbrenner, Nal and Blunsom, Phil},
    journal={arXiv preprint arXiv:1306.3584},
    year={2013}
}

@inproceedings{Xu2012-ci,
    title={Continuous space discriminative language modeling},
    author={Xu, Puyang and Khudanpur, Sanjeev and Lehr, Maider and Prud'hommeaux, E and Glenn, Nathan and Karakos, Damianos and Roark, Brian and Sagae, Kenji and Saraclar, Murat and Shafran, Izhak and others},
    booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on},
    pages={2129--2132},
    year={2012},
    organization={IEEE}
}

@INPROCEEDINGS{Sandbank2008-ow,
    title     = "Refining Generative Language Models Using Discriminative Learning",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    author    = "Sandbank, Ben",
    abstract  = "Abstract We propose a new approach to language modeling which utilizes discriminative learning methods. Our approach is an iterative one: starting with an initial language model, in each iteration we generate'false'sentences from the current model, and then train a ...",
    publisher = "Association for Computational Linguistics",
    pages     = "51--58",
    series    = "EMNLP '08",
    year      =  2008,
    url       = "http://dl.acm.org/citation.cfm?id=1613715.1613723",
    address   = "Stroudsburg, PA, USA"
}

@INCOLLECTION{Dosovitskiy2014-dx,
    title     = "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
    booktitle = "Advances in Neural Information Processing Systems 27",
    author    = "Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas",
    editor    = "Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q",
    abstract  = "Abstract Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the ...",
    publisher = "Curran Associates, Inc.",
    pages     = "766--774",
    year      =  2014,
    url       = "http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf",
    issn      = "1049-5258"
}

@ARTICLE{Madnani_undated-zx,
    title    = "Predicting Grammaticality on an Ordinal Scale",
    author   = "Madnani, Michael Heilman Aoife Cahill Nitin and Mulholland, Melissa Lopez Matthew and Tetreault, Joel", 
    journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
    year = {2014},
}

@ARTICLE{Cho2014-zn,
    title         = "Learning Phrase Representations using {RNN} {Encoder-Decoder} for Statistical Machine Translation",
    author        = "Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua",
    abstract      = "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols.  The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    month         =  "3~" # jun,
    year          =  2014,
    url           = "http://arxiv.org/abs/1406.1078",
    archivePrefix = "arXiv",
    eprint        = "1406.1078",
    primaryClass  = "cs.CL",
    arxivid       = "1406.1078"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{McNamara2012-am,
    title     = "The {Writing-Pal}: Natural language algorithms to support intelligent tutoring on writing strategies",
    author    = "McNamara, Danielle S and Raine, Roxanne and Roscoe, Rod and Crossley, Scott and Jackson, G Tanner and Dai, Jianmin and Cai, Zhiqiang and Renner, Adam and Brandon, Russell and Weston, Jennifer and {Others}",
    abstract  = "The Writing ‐ Pal (W‐Pal) is an intelligent tutoring system (ITS) that provides writing strategy instruction to high school students and entering college students. One unique quality of W‐ Pal is that it provides feedback to students' natural language input. Thus, much of our ...",
    journal   = "Applied natural language processing and content analysis: Identification, investigation, and resolution",
    publisher = "129.219.222.66",
    pages     = "298--311",
    year      =  2012,
    url       = "ftp://129.219.222.66/Publish/pdf/The_W-Pal_Natural_Language_Algorithms.pdf"
}

@INCOLLECTION{Connor2014-ls,
    title     = "Technologies That Support Students’ Literacy Development",
    booktitle = "Handbook of Research on Educational Communications and Technology",
    author    = "Connor, Carol Mcdonald and Goldman, Susan R and Fishman, Barry",
    abstract  = "Abstract This chapter reviews recent research on technology that supports students ' developing literacy skills from preschool through high school. We examine technologies for students across three developmental periods of reading: emergent literacy (preschool ...",
    publisher = "Springer",
    pages     = "591--604",
    year      =  2014,
    url       = "http://link.springer.com/chapter/10.1007/978-1-4614-3185-5_47"
}

@ARTICLE{Hastings2012-hp,
    title       = "Assessing the use of multiple sources in student essays",
    author      = "Hastings, Peter and Hughes, Simon and Magliano, Joseph P and Goldman, Susan R and Lawless, Kimberly",
    affiliation = "College of Computing and Digital Media, DePaul University, 243 South Wabash Avenue, Chicago, IL 60604, USA.  peterh@cdm.depaul.edu",
    abstract    = "The present study explored different approaches for automatically scoring student essays that were written on the basis of multiple texts. Specifically, these approaches were developed to classify whether or not important elements of the texts were present in the essays. The first was a simple pattern-matching approach called ``multi-word'' that allowed for flexible matching of words and phrases in the sentences.  The second technique was latent semantic analysis (LSA), which was used to compare student sentences to original source sentences using its high-dimensional vector-based representation. Finally, the third was a machine-learning technique, support vector machines, which learned a classification scheme from the corpus. The results of the study suggested that the LSA-based system was superior for detecting the presence of explicit content from the texts, but the multi-word pattern-matching approach was better for detecting inferences outside or across texts. These results suggest that the best approach for analyzing essays of this nature should draw upon multiple natural language processing approaches.",
    journal     = "Behav. Res. Methods",
    publisher   = "Springer",
    volume      =  44,
    number      =  3,
    pages       = "622--633",
    month       =  sep,
    year        =  2012,
    url         = "http://dx.doi.org/10.3758/s13428-012-0214-0",
    issn        = "1554-351X, 1554-3528",
    pmid        = "22653561",
    doi         = "10.3758/s13428-012-0214-0"
}

@ARTICLE{Dai2011-ix,
    title     = "The {Writing-Pal} tutoring system: Development and design",
    author    = "Dai, Jianmin and Raine, Roxanne B and Roscoe, Rod and Cai, Zhiqiang and McNamara, Danielle S",
    abstract  = "Writing - Pal is an intelligent tutoring system designed to offer high school students writing strategy instruction and guided practice to improve their essay-writing skills. Students are taught to use writing strategies via interactive lessons, games, and essay-writing practice. ...",
    journal   = "Computer",
    publisher = "129.219.222.66",
    volume    =  2,
    pages     = "1--11",
    year      =  2011,
    url       = "ftp://129.219.222.66/pdf/pdf/The_Writing_Pal_Tutoring_System.pdf",
    issn      = "0018-9162"
}

@ARTICLE{Wade-Stein2004-lj,
    title     = "Summary Street: Interactive Computer Support for Writing",
    author    = "Wade-Stein, David and Kintsch, Eileen",
    abstract  = "Summary Street is educational software based on latent semantic analysis (LSA), a computer method for representing the content of texts. The classroom trial described here demonstrates the power of LSA to support an educational goal by providing automatic feedback on the content of students' summaries.  Summary Street provides this feedback in an easy-to-grasp, graphic display that helps students to improve their writing across multiple cycles of writing and revision on their own before receiving a teacher's final evaluation. The software thus has the potential to provide students with extensive writing practice without increasing the teacher's workload. In classroom trials 6th-grade students not only wrote better summaries when receiving content-based feedback from Summary Street, but also spent more than twice as long engaged in the writing task.  Specifically, their summaries were characterized by a more balanced coverage of the content than summaries composed without this feedback. Greater improvement in content scores was observed with texts that were difficult to summarize. Classroom implementation of Summary Street is discussed, including suggestions for instructional activities beyond summary writing.",
    journal   = "Cogn. Instr.",
    publisher = "Taylor \& Francis",
    volume    =  22,
    number    =  3,
    pages     = "333--362",
    year      =  2004,
    url       = "http://dx.doi.org/10.1207/s1532690xci2203_3",
    eprint    = "http://dx.doi.org/10.1207/s1532690xci2203\_3",
    issn      = "0737-0008",
    doi       = "10.1207/s1532690xci2203\_3"
}


@ARTICLE{Franzke2006-ay,
    title     = "Building student summarization, writing and reading comprehension skills with guided practice and automated feedback",
    author    = "Franzke, Marita and Streeter, Lynn A",
    abstract  = "... 3 Note: Summary Street's core features are included in WriteToLearn ™, Pearson Knowledge Technologies' writing skills and reading comprehension development tool. ... Pearson Knowledge Technologies' writing and reading skills products, such as WriteToLearn ™ , ...",
    journal   = "Highlights From Research at the University of Colorado, A white paper from Pearson Knowledge Technologies",
    publisher = "assets.pearsonschool.com",
    year      =  2006,
    url       = "http://assets.pearsonschool.com/asset_mgr/legacy/200727/SummaryStreetWhitePaper-FINAL-1_262_1.pdf"
}

@INPROCEEDINGS{Sundermeyer2012-nf,
    title     = "{LSTM} Neural Networks for Language Modeling",
    booktitle = "{INTERSPEECH}",
    author    = "Sundermeyer, Martin and Schl{\"{u}}ter, Ralf and Ney, Hermann",
    abstract  = "Abstract Neural networks have become increasingly popular for the task of language modeling . Whereas feed-forward networks only exploit a fixed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into ...",
    publisher = "www-i6.informatik.rwth-aachen.de",
    year      =  2012,
    url       = "http://www-i6.informatik.rwth-aachen.de/publications/download/820/SundermeyerMartinSchl%7Bu%7DterRalfNeyHermann--LSTMNeuralNetworksforLanguageModeling--2012.pdf"
}

@BOOK{Graves2012-lb,
    title     = "Supervised Sequence Labelling with Recurrent Neural Networks:",
    author    = "Graves, Alex",
    publisher = "Springer Berlin Heidelberg",
    year      =  2012,
    url       = "http://link.springer.com/10.1007/978-3-642-24797-2",
    isbn      = "9783642247965, 9783642247972",
    doi       = "10.1007/978-3-642-24797-2"
}

@article{Russakovsky2014-nk,
    title={Imagenet large scale visual recognition challenge},
    author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
    journal={arXiv preprint arXiv:1409.0575},
    year={2014}
}

@article{Waibel1989-zo,
    title={Phoneme recognition using time-delay neural networks},
    author={Waibel, Alex and Hanazawa, Toshiyuki and Hinton, Geoffrey and Shikano, Kiyohiro and Lang, Kevin J},
    journal={Acoustics, Speech and Signal Processing, IEEE Transactions on},
    volume={37},
    number={3},
    pages={328--339},
    year={1989},
    publisher={IEEE}
}

@INCOLLECTION{Chandra2014-et,
    title     = "Adaptive Noise Schedule for Denoising Autoencoder",
    booktitle = "Neural Information Processing",
    author    = "Chandra, B and Sharma, Rajesh Kumar",
    abstract  = "The paper proposes an Adaptive Stacked Denoising Autoencoder (ASDA) to overcome the limitations of Stacked Denoising Autoencoder (SDA) [6] in which noise level is kept fixed during the training phase of the autoencoder. In ASDA, annealing schedule is applied on noise where the average noise level of input neurons is kept high during initial training phase and noise is slowly reduced as the training proceeds. The noise level of each input neuron is computed based on the weights connecting the input neuron to the hidden layer while keeping the average noise level of input layer to be same as that computed by annealing schedule. This enables the denoising autoencoder to learn the input manifold in greater details. As evident from results, ASDA gives better classification accuracy compared to SDA on variants of MNIST dataset [3].",
    publisher = "Springer International Publishing",
    pages     = "535--542",
    series    = "Lecture Notes in Computer Science",
    month     =  "3~" # nov,
    year      =  2014,
    url       = "http://link.springer.com/chapter/10.1007/978-3-319-12637-1_67",
    issn      = "1049-5258",
    isbn      = "9783319126364, 9783319126371",
    doi       = "10.1007/978-3-319-12637-1\_67"
}

@article{Geras2014-fp,
    title={Scheduled denoising autoencoders},
    author={Geras, Krzysztof J and Sutton, Charles},
    journal={arXiv preprint arXiv:1406.3269},
    year={2014}
}

@article{van2008visualizing,
    title={Visualizing data using t-SNE},
    author={Van der Maaten, Laurens and Hinton, Geoffrey},
    journal={Journal of Machine Learning Research},
    volume={9},
    number={2579-2605},
    pages={85},
    year={2008}
}

@incollection{zeiler2014visualizing,
    title={Visualizing and understanding convolutional networks},
    author={Zeiler, Matthew D and Fergus, Rob},
    booktitle={Computer Vision--ECCV 2014},
    pages={818--833},
    year={2014},
    publisher={Springer}
}

@inproceedings{li2002learning,
    title={Learning question classifiers},
    author={Li, Xin and Roth, Dan},
    booktitle={Proceedings of the 19th international conference on Computational linguistics-Volume 1},
    pages={1--7},
    year={2002},
    organization={Association for Computational Linguistics}
}

@inproceedings{Pang+Lee:05a,
    author = {Bo Pang and Lillian Lee},
    title = {Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales},
    year = {2005},
    pages = {115--124},
    booktitle = {Proceedings of ACL}
}

@article{le2014distributed,
    title={Distributed representations of sentences and documents},
    author={Le, Quoc V and Mikolov, Tomas},
    journal={arXiv preprint arXiv:1405.4053},
    year={2014}
}

@article{tsujiiythu2007discriminative,
  title={A Discriminative Language Model with Pseudo-Negative Samples},
  author={Okanohara, Daisuke and  Tsujii, Jun’ichi},
  journal={ACL 2007},
  pages={73},
  year={2007}
}

@inproceedings{collobert2008unified,
  title={A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th International Conference on Machine learning},
  pages={160--167},
  year={2008},
  organization={ACM}
}

@InProceedings{ICML2012Mnih_855,
  author =    {Andriy Mnih and Yee Whye Teh},
  title =     {A Fast and Simple Algorithm for Training Neural Probabilistic Language Models},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  series =    {ICML '12},
  year =      {2012},
  editor =    {John Langford and Joelle Pineau},
  location =  {Edinburgh, Scotland, GB},
  isbn =      {978-1-4503-1285-1},
  month =     {July},
  publisher = {Omnipress},
  address =   {New York, NY, USA},
  pages=      {1751--1758},
}

@inproceedings{gutmann2010noise,
  title={Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={297--304},
  year={2010}
}

@article{cherry2008discriminative,
  title={Discriminative, Syntactic Language Modeling through Latent SVMs},
  author={Cherry, Colin and Quirk, Chris},
  journal={Proceeding of Association for Machine Translation in the America (AMTA-2008)},
  year={2008}
}

@article{vinyals2014grammar,
  title={Grammar as a Foreign Language},
  author={Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1412.7449},
  year={2014}
}

@TECHREPORT{Bengio03,
         author = {Bengio, Yoshua and Sen{\'{e}}cal, Jean-S{\'{e}}bastien},
       projects = {Idiap},
          month = {0},
          title = {Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model},
           type = {Idiap-RR},
         number = {Idiap-RR-35-2003},
           year = {2003},
    institution = {IDIAP},
       abstract = {Previous work on statistical language modeling has shown that it is possible to train a feed-forward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models. However, in order to train the model on the maximum likelihood criterion, one has to make, for each example, as many network passes as there are words in the vocabulary. We introduce adaptive importance sampling as a way to accelerate training of the model. We show that a very significant speed-up can be obtained on standard problems.},
            pdf = {http://publications.idiap.ch/downloads/reports/2003/rr-03-35.pdf},
     postscript = {ftp://ftp.idiap.ch/pub/reports/2003/rr-03-35.ps.gz},
ipdmembership={learning},
}

@InProceedings{clifton-whitney-sarkar:2013:IJCNLP,
  author    = {Clifton, Ann  and  Whitney, Max  and  Sarkar, Anoop},
  title     = {An Online Algorithm for Learning over Constrained Latent Representations using Multiple Views},
  booktitle = {Proceedings of the Sixth International Joint Conference on Natural Language Processing},
  month     = {October},
  year      = {2013},
  address   = {Nagoya, Japan},
  publisher = {Asian Federation of Natural Language Processing},
  pages     = {1072--1076},
  url       = {http://www.aclweb.org/anthology/I13-1149}
}

@article{tsujiiythu2007discriminative,
  title={A Discriminative Language Model with Pseudo-Negative Samples},
  author={Okanohara, Daisuke and  Tsujii, Jun’ichi},
  journal={ACL 2007},
  pages={73},
  year={2007}
}

@inproceedings{collobert2008unified,
  title={A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th International Conference on Machine learning},
  pages={160--167},
  year={2008},
  organization={ACM}
}

@InProceedings{ICML2012Mnih_855,
  author =    {Andriy Mnih and Yee Whye Teh},
  title =     {A Fast and Simple Algorithm for Training Neural Probabilistic Language Models},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  series =    {ICML '12},
  year =      {2012},
  editor =    {John Langford and Joelle Pineau},
  location =  {Edinburgh, Scotland, GB},
  isbn =      {978-1-4503-1285-1},
  month =     {July},
  publisher = {Omnipress},
  address =   {New York, NY, USA},
  pages=      {1751--1758},
}

@inproceedings{gutmann2010noise,
  title={Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={297--304},
  year={2010}
}

@article{cherry2008discriminative,
  title={Discriminative, Syntactic Language Modeling through Latent SVMs},
  author={Cherry, Colin and Quirk, Chris},
  journal={Proceeding of Association for Machine Translation in the America (AMTA-2008)},
  year={2008}
}

@article{vinyals2014grammar,
  title={Grammar as a Foreign Language},
  author={Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1412.7449},
  year={2014}
}

@TECHREPORT{Bengio03,
         author = {Bengio, Yoshua and Sen{\'{e}}cal, Jean-S{\'{e}}bastien},
       projects = {Idiap},
          month = {0},
          title = {Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model},
           type = {Idiap-RR},
         number = {Idiap-RR-35-2003},
           year = {2003},
    institution = {IDIAP},
       abstract = {Previous work on statistical language modeling has shown that it is possible to train a feed-forward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models. However, in order to train the model on the maximum likelihood criterion, one has to make, for each example, as many network passes as there are words in the vocabulary. We introduce adaptive importance sampling as a way to accelerate training of the model. We show that a very significant speed-up can be obtained on standard problems.},
            pdf = {http://publications.idiap.ch/downloads/reports/2003/rr-03-35.pdf},
     postscript = {ftp://ftp.idiap.ch/pub/reports/2003/rr-03-35.ps.gz},
ipdmembership={learning},
}

@InProceedings{clifton-whitney-sarkar:2013:IJCNLP,
  author    = {Clifton, Ann  and  Whitney, Max  and  Sarkar, Anoop},
  title     = {An Online Algorithm for Learning over Constrained Latent Representations using Multiple Views},
  booktitle = {Proceedings of the Sixth International Joint Conference on Natural Language Processing},
  month     = {October},
  year      = {2013},
  address   = {Nagoya, Japan},
  publisher = {Asian Federation of Natural Language Processing},
  pages     = {1072--1076},
  url       = {http://www.aclweb.org/anthology/I13-1149}
}

@inproceedings{bergsma2008discriminative,
  title={Discriminative Learning of Selectional Preference from Unlabeled Text},
  author={Bergsma, Shane and Lin, Dekang and Goebel, Randy},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  pages={59--68},
  year={2008},
  organization={Association for Computational Linguistics}
}

@inproceedings{vincent2008extracting,
  title={Extracting and Composing Robust Features with Denoising Autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th International Conference on Machine Learning},
  pages={1096--1103},
  year={2008},
  organization={ACM}
}

@inproceedings{foster2009generrate,
  title={{GenERRate}: Generating Errors for Use in Grammatical Error Detection},
  author={Foster, Jennifer and Andersen, {\O}istein E},
  booktitle={Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={82--90},
  year={2009},
  organization={Association for Computational Linguistics}
}

@inproceedings{smith2005contrastive,
  title={Contrastive Estimation: Training Log-Linear Models on Unlabeled Data},
  author={Smith, Noah A and Eisner, Jason},
  booktitle={Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
  pages={354--362},
  year={2005},
  organization={Association for Computational Linguistics}
}

@inproceedings{dale2011helping,
  title={Helping Our Own: The {HOO} 2011 Pilot Shared Task},
  author={Dale, Robert and Kilgarriff, Adam},
  booktitle={Proceedings of the 13th European Workshop on Natural Language Generation},
  pages={242--249},
  year={2011},
  organization={Association for Computational Linguistics}
}

@inproceedings{dale2012hoo,
  title={{HOO} 2012: A Report on the Preposition and Determiner Error Correction Shared Task},
  author={Dale, Robert and Anisimoff, Ilya and Narroway, George},
  booktitle={Proceedings of the Seventh Workshop on Building Educational Applications Using NLP},
  pages={54--62},
  year={2012},
  organization={Association for Computational Linguistics}
}

@inproceedings{ng2013conll,
  title={The CoNLL-2013 Shared Task on Grammatical Error Correction},
  author={Ng, Hwee Tou and Wu, Siew Mei and Wu, Yuanbin and Hadiwinoto, Christian and Tetreault, Joel},
  booktitle={Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task},
  pages={1--12},
  year={2014}
}

@inproceedings{ng2014conll,
  title={The CoNLL-2014 Shared Task on Grammatical Error Correction},
  author={Ng, Hwee Tou and Wu, Siew Mei and Briscoe, Ted and Hadiwinoto, Christian and Susanto, Raymond Hendy and Bryant, Christopher},
  booktitle={Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task},
  pages={1--14},
  year={2014},
  organization={Association for Computational Linguistics}
}

@article{rozovskaya2014building,
  title={Building a State-of-the-Art Grammatical Error Correction System},
  author={Rozovskaya, Alla and Roth, Dan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={419--434},
  year={2014}
}

@article{klein2003maxent,
  title={Maxent Models, Conditional Estimation, and Optimization},
  author={Klein, Dan and Manning, Christopher},
  journal={NAACL-HLT 2003 Tutorial},
  year={2003}
}

@book{Hastad1987,
 author = {H{\aa}stad, Johan},
 title = {Computational Limitations of Small-Depth Circuits},
 year = {1987},
 isbn = {0262081679},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@incollection{Allender1996,
year={1996},
isbn={978-3-540-62034-1},
booktitle={Foundations of Software Technology and Theoretical Computer Science},
volume={1180},
series={Lecture Notes in Computer Science},
editor={Chandru, V. and Vinay, V.},
doi={10.1007/3-540-62034-6_33},
title={Circuit Complexity Before the Dawn of the New Millennium},
url={http://dx.doi.org/10.1007/3-540-62034-6_33},
publisher={Springer Berlin Heidelberg},
author={Allender, Eric},
pages={1-18},
language={English}
}

@incollection{bengio2007scaling,
  title={Scaling Learning Algorithms towards AI},
  author={Bengio, Yoshua and LeCun, Yann},
  booktitle={Large-scale Kernel Machines},
  editor={Bottou, León and Chapelle, Olivier and DeCoste, Dennis and Weston, Jason},
  publisher={MIT Press},
  volume={34},
  number={5},
  year={2007}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={The Journal of Machine Learning Research},
  volume={12},
  pages={2121--2159},
  year={2011},
  publisher={JMLR.org}
}

@inproceedings{han2004detecting,
  title={Detecting Errors in English Article Usage with a Maximum Entropy Classifier Trained on a Large, Diverse Corpus.},
  author={Han, Na-Rae and Chodorow, Martin and Leacock, Claudia},
  booktitle={LREC},
  year={2004},
  organization={Citeseer}
}

@inproceedings{han2010using,
  title={Using an Error-Annotated Learner Corpus to Develop an ESL/EFL Error Correction System.},
  author={Han, Na-Rae and Tetreault, Joel R and Lee, Soo-Hwa and Ha, Jin-Young},
  booktitle={LREC},
  year={2010}
}

@misc{WikiClean,
  title = {{WikiClean}: A Java Wikipedia markup to plain text converter},
  howpublished = {\url{https://github.com/lintool/wikiclean}},
  note = {Accessed: 2015-11-23},
  year={2015}
}

@article{andersen2007grammatical,
  title={Grammatical error detection using corpora and supervised learning},
  author={Andersen, {\O}istein E},
  journal={Proceedings of the Twelfth ESSLLI Sudent Session},
  pages={1--9},
  year={2007}
}


@inproceedings{chodorow2012problems,
  title={Problems in Evaluating Grammatical Error Detection Systems.},
  author={Chodorow, Martin and Dickinson, Markus and Israel, Ross and Tetreault, Joel R},
  booktitle={COLING},
  pages={611--628},
  year={2012}
}

@inproceedings{chodorow2000unsupervised,
  title={An unsupervised method for detecting grammatical errors},
  author={Chodorow, Martin and Leacock, Claudia},
  booktitle={Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference},
  pages={140--147},
  year={2000},
  organization={Association for Computational Linguistics}
}

@inproceedings{chodorow2007detection,
  title={Detection of grammatical errors involving prepositions},
  author={Chodorow, Martin and Tetreault, Joel R and Han, Na-Rae},
  booktitle={Proceedings of the fourth ACL-SIGSEM workshop on prepositions},
  pages={25--30},
  year={2007},
  organization={Association for Computational Linguistics}
}

@article{leacock2010automated,
  title={Automated grammatical error detection for language learners},
  author={Leacock, Claudia and Chodorow, Martin and Gamon, Michael and Tetreault, Joel},
  journal={Synthesis lectures on human language technologies},
  volume={3},
  number={1},
  pages={1--134},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{nagata2010evaluating,
  title={Evaluating performance of grammatical error detection to maximize learning effect},
  author={Nagata, Ryo and Nakatani, Kazuhide},
  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics: Posters},
  pages={894--900},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{tetreault2008ups,
  title={The ups and downs of preposition error detection in ESL writing},
  author={Tetreault, Joel R and Chodorow, Martin},
  booktitle={Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1},
  pages={865--872},
  year={2008},
  organization={Association for Computational Linguistics}
}

@inproceedings{tetreault2008native,
  title={Native judgments of non-native usage: Experiments in preposition error detection},
  author={Tetreault, Joel R and Chodorow, Martin},
  booktitle={Proceedings of the Workshop on Human Judgements in Computational Linguistics},
  pages={24--32},
  year={2008},
  organization={Association for Computational Linguistics}
}

@inproceedings{tetreault2010using,
  title={Using parse features for preposition selection and error detection},
  author={Tetreault, Joel and Foster, Jennifer and Chodorow, Martin},
  booktitle={Proceedings of the acl 2010 conference short papers},
  pages={353--358},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{tetreault2010rethinking,
  title={Rethinking grammatical error annotation and evaluation with the Amazon Mechanical Turk},
  author={Tetreault, Joel R and Filatova, Elena and Chodorow, Martin},
  booktitle={Proceedings of the NAACL-HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={45--48},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{wagner2007comparative,
  booktitle={Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  pages={112--121},
  title={A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors},
  author={Wagner, Joachim and Foster, Jennifer and van Genabith, Josef},
  year={2007},
  organization={Association for Computational Linguistics}
}

@incollection{NIPS2013_5021,
  title = {Distributed Representations of Words and Phrases and their Compositionality},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems 26},
  editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  pages = {3111--3119},
  year = {2013},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}


@inproceedings{mnih2013learning,
  title={Learning word embeddings efficiently with noise-contrastive estimation},
  author={Mnih, Andriy and Kavukcuoglu, Koray},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2265--2273},
  year={2013}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{rozovskaya2010generating,
  title={Generating confusion sets for context-sensitive error correction},
  author={Rozovskaya, Alla and Roth, Dan},
  booktitle={Proceedings of the 2010 conference on empirical methods in natural language processing},
  pages={961--970},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{chen2015semantic,
  title={Relation Extraction: Perspective from Convolutional Neural Networks},
  author={Nguyen, Thien Huu and Grishman, Ralph},
  booktitle={Proceedings of the NAACL-HLT 2015 Workshop on Vector Space Modeling for NLP},
  pages={168--175},
  year={2015}
}

@inproceedings{de2007automatically,
  title={Automatically acquiring models of preposition use},
  author={De Felice, Rachele and Pulman, Stephen G},
  booktitle={Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions},
  pages={45--50},
  year={2007},
  organization={Association for Computational Linguistics}
}

@inproceedings{gamon2008using,
  title={Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.},
  author={Gamon, Michael and Gao, Jianfeng and Brockett, Chris and Klementiev, Alexandre and Dolan, William B and Belenko, Dmitriy and Vanderwende, Lucy},
  booktitle={IJCNLP},
  volume={8},
  pages={449--456},
  year={2008}
}

@book{firth1957synopsis,
  title={A synopsis of linguistic theory, 1930-1955},
  author={Firth, John R},
  year={1957},
  publisher={Blackwell},
  url={https://books.google.com/books?id=T8LDtgAACAAJ}
}

@ARTICLE{LeCun98, 
author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
journal={Proceedings of the IEEE}, 
title={Gradient-based learning applied to document recognition}, 
year={1998}, 
volume={86}, 
number={11}, 
pages={2278-2324}, 
keywords={backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis}, 
doi={10.1109/5.726791}, 
ISSN={0018-9219}, 
month={Nov},}

@inproceedings{kim2014,
  title={Convolutional Neural Networks for Sentence Classification},
  author={Kim, Yoon},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1746--1751},  
  year={2014},
  organization={Association for Computational Linguistics}
}

@INPROCEEDINGS{Trask2015-ip,
  title = "Modeling Order in Neural Word Embeddings at Scale",
  booktitle = "Proceedings of The 32nd International Conference on Machine Learning",
  author = "Trask, Andrew and Gilmore, David and Russell, Matthew",
  pages = "2266--2275",
  year =  2015,
  url = {http://jmlr.org/proceedings/papers/v37/trask15.pdf}
}

@ARTICLE{Mikolov2013-vo,
  title    = "Efficient Estimation of Word Representations in Vector Space",
  author   = "Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey",
  journal  = "CoRR",
  volume   = "abs/1301.3781",
  year     =  2013,
  url      = "http://arxiv.org/abs/1301.3781",
  keywords = "NLP"
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010},
  pages={1045--1048},
  year={2010}
}}

@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@InProceedings{iyyer-EtAl:2015:ACL-IJCNLP,
  author    = {Iyyer, Mohit  and  Manjunatha, Varun  and  Boyd-Graber, Jordan  and  Daum\'{e} III, Hal},
  title     = {Deep Unordered Composition Rivals Syntactic Methods for Text Classification},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = {July},
  year      = {2015},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {1681--1691},
  url       = {http://www.aclweb.org/anthology/P15-1162}
}

@article{zhang2015universum,
  title={Universum Prescription: Regularization using Unlabeled Data},
  author={Zhang, Xiang and LeCun, Yann},
  journal={arXiv preprint arXiv:1511.03719},
  year={2015}
}

@article{konda2015dropout,
  title={Dropout as data augmentation},
  author={Konda, Kishore and Bouthillier, Xavier and Memisevic, Roland and Vincent, Pascal},
  journal={arXiv preprint arXiv:1506.08700},
  year={2015}
}   

@article{rittle20117,
  title={7 The Power of Comparison in Learning and Instruction: Learning Outcomes Supported by Different Types of Comparisons},
  author={Rittle-Johnson, Bethany and Star, Jon R},
  journal={Psychology of Learning and Motivation-Advances in Research and Theory},
  volume={55},
  pages={199},
  year={2011}
}

@article{schwartz2011practicing,
  title={Practicing versus inventing with contrasting cases: The effects of telling first on learning and transfer.},
  author={Schwartz, Daniel L and Chase, Catherine C and Oppezzo, Marily A and Chin, Doris B},
  journal={Journal of Educational Psychology},
  volume={103},
  number={4},
  pages={759},
  year={2011},
  publisher={American Psychological Association}
}

@article{kukich1992techniques,
  title={Techniques for automatically correcting words in text},
  author={Kukich, Karen},
  journal={ACM Computing Surveys (CSUR)},
  volume={24},
  number={4},
  pages={377--439},
  year={1992},
  publisher={ACM}
}

@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Geoffrey E. Hinton},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@inproceedings{graves2013speech,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alan and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  pages={6645--6649},
  year={2013},
  organization={IEEE}
}

@inproceedings{rasmus2015semi,
  title={Semi-Supervised Learning with Ladder Networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3532--3540},
  year={2015}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{kim2015character,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  journal={arXiv preprint arXiv:1508.06615},
  year={2015}
}

@book{JurafskyMartin2009,
 author = {Jurafsky, Daniel and Martin, James H.},
 title = {Speech and Language Processing (2Nd Edition)},
 year = {2009},
 isbn = {0131873210},
 publisher = {Prentice-Hall, Inc.},
 address = {Upper Saddle River, NJ, USA},
} 


@article{tran2016recurrent,
  title={Recurrent Memory Network for Language Modeling},
  author={Tran, Ke and Bisazza, Arianna and Monz, Christof},
  journal={arXiv preprint arXiv:1601.01272},
  year={2016}
}

@article{chen2015strategies,
  title={Strategies for Training Large Vocabulary Neural Language Models},
  author={Chen, Welin and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1512.04906},
  year={2015}
}

@article{baltescu2014pragmatic,
  title={Pragmatic Neural Language Modelling in Machine Translation},
  author={Baltescu, Paul and Blunsom, Phil},
  journal={arXiv preprint arXiv:1412.7119},
  year={2014}
}

@inproceedings{kernighan1990spelling,
  title={A spelling correction program based on a noisy channel model},
  author={Kernighan, Mark D and Church, Kenneth W and Gale, William A},
  booktitle={Proceedings of the 13th conference on Computational linguistics-Volume 2},
  pages={205--210},
  year={1990},
  organization={Association for Computational Linguistics}
}

@article{church1991probability,
  title={Probability scoring for spelling correction},
  author={Church, Kenneth W and Gale, William A},
  journal={Statistics and Computing},
  volume={1},
  number={2},
  pages={93--103},
  year={1991},
  publisher={Springer}
}